*[Word count](https://wordcounter.net/): 4,500*

## What is this?

This is the "technology and futurism" section of [this notebook](https://github.com/monastri/monastri.github.io/blob/master/quotes.md), last updated Apr 6, 2019, which got so big (1.2 million char) that GitHub refused to render the whole page anymore, ruining my original dream of having my entire notebook in one long page for two purposes: (1) zero-latency clickthrough to make up for my working memory's sand-thru-sieve transience; (2) lower activation energy for continuous document-wide refactoring, to aid recall and cross-domain interlinking.

That said, this is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

(There's also the parallel to [perpetual beta](https://breakingsmart.com/en/season-1/running-code-and-perpetual-beta/) for web-based software: scaffolding for extensive ongoing experimentation within the already-deployed app. Never start, as Gwern says.)

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping. (Or at least that was the original intent before I had to break up the notebook. Now I'm not so sure I can do this.)

I also really, *really* hate experiences of the [Jeremy Bentham type](https://github.com/monastri/monastri.github.io/blob/master/notes-amazing-people.md#Jeremy-bentham). This document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [The Singularity](#the-singularity)
	1. [The three main schools of thought](#Three-Singularity-schools)
	2. [Replacing humans at every step of the economic chain](#replacing-humans-at-every-step-of-the-economic-chain)
	2. [Disneyland with no children](#Disneyland-with-no-children)
	3. [The Singularity has already happened](#the-singularity-has-happened)
2. [The origins of information society](#Origins-of-information-society)
3. [Rambling and AI](#rambling-and-ai)
4. [The rhizomatic frankenstack of today's IT ecosystem](#rhizomatic-frankenstack)

---------------------------------------------------

<a name="#rhizomatic-frankenstack"></a>
## Rhizomatic frankenstack
([overview](#overview))

From Venkat's [Breaking Smart newsletter](https://mailchi.mp/ribbonfarm/frankenstacks-and-rhizomes). 

Introduction:

```markdown
The experience of suddenly being dumped into a messy, high-stakes emerging IT ecosystem
got me reflecting on a theme I keep returning to lately: the sheer complexity of the IT
stack we rely on today as individuals. 

Today, each of us lives within what I call a frankenstack. An assemblage of information 
technologies duct-taped together with a mess of protocols, and forming what philosophers
call a rhizomatic structure.

1/ Consider the difference between an onion and a piece of ginger. The ginger root is the
motif for what philosophers call a rhizome. The onion for what they call an arborescence.

2/ With an onion, you can always tell which way is up, and can distinguish horizontal 
sections apart from vertical sections easily by visual inspection.

2/ With a piece of ginger, there is no clear absolute orientation around an up, and no 
clear distinction between horizontal and vertical.

3/ According to the linked Wikipedia entry (worth reading), a rhizome "allows for multiple, 
non-hierarchical entry and exit points in data representation and interpretation."

4/ If you tend to use the cliched "hierarchies versus networks" metaphor for talking about
old versus new IT, you would do well to shift to the rhizomatic/arborescent distinction.

5/ Both onions and ginger roots show some signs of both hierarchical structure and network-
like internal connections.

6/ The difference is that one has no default orientation, direction of change, or defining
internal symmetries. Rhizomes are disorderly and messy in architectural terms.

7/ The diagram above shows a partial view of my personal frankenstack: a mess sprawling 
over wordpress, slack, mailchimp and dozens of other technology platforms.

8/ As a free agent solopreneur with a weird mix of activities, my frankenstack is probably
more complex than most, but not as complex as some power-users I know.

9/ If you work in a large organization defined by an enterprise IT system, your frankenstack
is likely more arborescent than mine. More onion-like.

10/ But this is not going to last much longer. Already, bleeding edge enterprise IT platform
architecture is acquiring the rhizomatic characteristics of the consumer web.
```

Why is the rhizome a better mental model for IT infrastructure than either hierarchies or networks? The answer has to do with the curse of dimensionality:

```markdown
12/ All of us today live informationally high-dimensional lives. We manage many complex 
information stocks and flows that merge and mix in a labyrinthine permissions/security
matrix.

13/ Hierarchies and networks are both clean, legible architectural patterns. Applying 
them to high dimensional situations is highly budrensome and largely useless.

14/ Consider an organization with a strictly hierarchical org chart. You could model it
with a single variable: who reports to whom. "Level" is a dependent variable.

15/ Or consider an organization that's a strict network topology. You could model it
with a graph: who is connected to whom. "Degrees of separation" is a dependent variable.

16/ Now start to throw in complicating factors. In orgs, you could have dotted-line 
relationships, floating assignments, full-time/part-time boundaries, staff vs. line etc.

17/ In a network, you could have different complicating factors: asymmetry vs symmetry in
follows, algorithmically maintained feeds that drive interactions, and so forth.

18/ With each complicating factor, more new variables enter the picture. The 
dimensionality increases. However, not all dimensions are equally important.

19/ So you end up with a mess of organizing constructs: hierarchies, networks, group
boundaries, permission levels, version histories, event histories, and so on.

20/ If you tried to extend a platonic concept like "hierarchy" or "network" you'd end up 
with impossibly high-dimensional structures that are empty for the most part.

21/ Instead, you switch to only modeling things that have information content, and doing 
so piecemeal. You end up with "crumpled" versions of high-dimensional structures in low-
dimensional spaces.

22/ In my rhizome picture above for example, there are a dozen dimensions (the words in
green). But I have a loose, gingery representation in 3d space.

23/ Things are related as they are pragmatically required to be, and interpenetrate and
relatively orient as is most convenient, rather than to conform to a platonic pattern.

24/ The archetypal action in a rhizomatic information architecture is cut-and-paste. The 
spreadsheet is the archetypal integration tool: a sort of generalized clipboard.

25/ There is a relationship here to the idea that the medium is the message, and Conway's
law (product structure mirrors org structure).

26/ Our information environments are becoming rhizomatic because our informational lives
are becoming rhizomatic, and vice versa, in a chicken-and-egg loop.

27/ Tools with rhizomatic dispositions include, besides cut-and-paste and spreadsheets, 
things like IFTTT, Zapier, and at the enterprise level, things like microservices.
```

Rhizomes aren't just about information:

```markdown
28/ Rhizomes aren't just about information. They are also about computational
capabilities, distribution capabilities, relationships, trust, and permission 
architectures.

29/ Two of the biggest technologies evolving today -- the blockchain and machine learning
-- are fundamentally rhizomatic in their DNA. You have to tread gingerly around them.

30/ Both have the predisposition to follow the contours of information content rather 
than idealized organization patterns. Neither is architecturally well-behaved and 
disciplined.

31/ Rhizomes are information dense, topologically complex (things connecting in weird ways),
highly heterogeneous and variegated, structurally compact, and with strong form-content 
coupling.

32/ Unlike pure-paradigm architectures, rhizomes mix and match multiple architectural 
paradigms along with emergent structures to create high-dissonance information environments.
```

What is it like to live in a rhizome?

```markdown
33/ What is it like to live in a rhizome? Well for starters, there are no default entry or
exit points, no "onboarding manual" that teaches you how to survive in one, and no "up."

34/ There is perhaps a distinction between a n00b and an expert, but it is highly localized 
around specific corners of the rhizome. You can go from n00b to expert and back to n00b in
2 steps.

35/ In a traditional org, you can count the floors between the executive suite and say 
the shop floor where blue-collar workers build products on assembly lines. Authority falls
as the elevator descends.

36/ n00b/expert relationships change slowly and predictably in space as you move. Expertise
and authority turfs are simply connected and simply bounded.

37/ In a rhizome, in a move from point A to point B, relative knowledge and expertise might
swing wildly. And the value of actions might swing wildly while you're moving.

38/ A rhizome is also a high-friction space. Movement through a rhizome involves an
unpredictable stream of transaction costs. Every journey is an obstacle course.

39/ Sometimes there's a good FAQ page, other times a tweet makes a difference between a 
minute and a week. Discovered structure, rather than inferences from maps, dictates the 
cost of action.

40/ Sometimes having a programmer friend whom you can quickly email can make the impossible
possible. Sometimes an online forum saves hours, sometimes it wastes hours.

41/ Sometimes a single click moves mountains. Other times, you need to move mountains to do
one tiny thing. Effort-outcome relationships get out of whack.

42/ Sometimes really important things are trivially easy if you happen to know somebody who
knows one weird trick. Other times, common-sense things turn out to be impossible.

43/ In a rhizomatic world, if your expectations and work habits are built around architectural 
cleanliness, you will get deeply frustrated and be perennially frozen.

44/ If you can only navigate well-paved paths and clean, well-lit spaces, you'll likely spend
a lot of time in low-value, or even futile, ritualized behaviors while getting nothing done.

45/ You must be willing to adopt an opportunistic approach to navigating complexity, and 
switch from ugly hack to elegant beauty, from amateurish fumble to expert flourish, in an 
instant.

46/ You've heard of analysis paralysis, right? I have a similar concept I call aesthetic 
paralysis: the desire for elegance in behavior limiting your agency. Superficial beauty is 
expensive in a rhizome.

47/ This is not to say rhizomes are ugly. Once you accept their inevitability, the act of 
navigating a rhizome can start to acquire its own beauty. 

48/ You start to be less attached to received ideas of importance, order, and logic, and 
learn to interact with the natural logic of the environment.

49/ You learn to adjust your aesthetic sensibilities to what you're experiencing, so you 
can actually see what's going on, rather than being bound by aesthetic expectations.

50/ You start to gain what a friend of mine just described as "infrastructure fluency." This
is in a way the opposite of architectural taste: an ability to experience the artificial
world in its natural language.

51/ Unlike in physical architecture, where there is such thing as an architect's view of 
reality, the only guide to a true rhizome is a burglar's guide. So stop worrying, and learn
to love your frankenstack.
```



```markdown

```

<a name="#rambling-and-ai"></a>
## Rambling and AI
([overview](#overview))

From Robin Hanson's post [Better Babblers](http://www.overcomingbias.com/2017/03/better-babblers.html), speculating on how the world might change in the proliferation of GPT-2s: 

```markdown
Let me call styles of talking (or music, etc.) that rely mostly on low order correlations 
“babbling”. Babbling isn’t meaningless, but to ignorant audiences it often appears to be 
based on a deeper understanding than is actually the case. When done well, babbling can be
entertaining, comforting, titillating, or exciting. It just isn’t usually a good place to
learn deep insight.

As we slowly get better at statistics and machine learning, our machines will slowly get 
better at babbling. The famous Eliza chatbot went surprisingly far using very low order
correlations, and today chatbots best fool us into thinking they are human when they stick
to babbling style conversations. So what does a world of better babblers look like?

First, machines will better mimic low quality student essays, so schools will have to try 
harder to keep such students from using artificial babblers.

Second, the better machines get at babbling, the more humans will try to distinguish
themselves from machines via non-babbling conversational styles. So expect less use of 
simple easy-to-understand-and-predict speech in casual polite conversation, inspirational 
speeches, and public intellectual talk.

One option is to put a higher premium on talk that actually makes deep sense, in terms of 
deep concepts that experts understand. That would be nice for those of us who have always 
emphasized such things. But alas there are other options.

A second option is to put a higher premium on developing very distinctive styles of talking.
This would be like how typical popular songs from two centuries ago could be sung and 
enjoyed by most anyone, compared to how popular music today is matched in great detail to 
the particular features of particular artists. Imagine most all future speakers having as 
distinct a personal talking style.

A third option is more indirect, ironic, and insider style talk, such as we tend to see on
Twitter today. People using words and phrases and cultural references in ways that only folks
very near in cultural space can clearly accept as within recent local fashion. Artificial 
babblers might not have enough data to track changing fashions in such narrow groups.

Bottom line: the more kinds of conversation styles that simple machines can manage, the more 
humans will try to avoid talking in those styles, a least when not talking to machines.
```

<a name="#the-singularity"></a>
## The Singularity
([overview](#overview))

<a name="#Three-Singularity-schools"></a>
### Three Singularity schools
([overview](#overview))

Eliezer Yudkowsky's [essay](http://yudkowsky.net/singularity/schools/) continues to be a good summary of how things stand. First a complaint:

```markdown
I’ve heard (many) other definitions of the Singularity attempted, but I usually find them 
to lack separate premises and conclusions. For example, the old Extropian FAQ used to define
the “Singularity” as the Inflection Point, “the time when technological development will be
at its fastest” and just before it starts slowing down. 

But what makes this an interesting point in history apart from its definition? What are the
consequences of this assumption? To qualify as a school of thought or even a thesis, one needs
an internal structure of argument, not just a definition.
```

On to the three schools: 

```markdown
**Accelerating Change:**

Core claim: Our intuitions about change are linear; we expect roughly as much change as has
occurred in the past over our own lifetimes. But technological change feeds on itself, and 
therefore accelerates. Change today is faster than it was 500 years ago, which in turn is 
faster than it was 5000 years ago. Our recent past is not a reliable guide to how much change
we should expect in the future.

Strong claim: Technological change follows smooth curves, typically exponential. Therefore we
can predict with fair precision when new technologies will arrive, and when they will cross key
thresholds, like the creation of Artificial Intelligence.

Advocates: Ray Kurzweil, Alvin Toffler(?), John Smart

**Event Horizon:**

Core claim: For the last hundred thousand years, humans have been the smartest intelligences 
on the planet. All our social and technological progress was produced by human brains. Shortly,
technology will advance to the point of improving on human intelligence (brain-computer 
interfaces, Artificial Intelligence). This will create a future that is weirder by far than most
science fiction, a difference-in-kind that goes beyond amazing shiny gadgets.

Strong claim: To know what a superhuman intelligence would do, you would have to be at least
that smart yourself. To know where Deep Blue would play in a chess game, you must play at Deep
Blue’s level. Thus the future after the creation of smarter-than-human intelligence is absolutely
unpredictable.

Advocates: Vernor Vinge

**Intelligence Explosion:**

Core claim: Intelligence has always been the source of technology. If technology can significantly 
improve on human intelligence – create minds smarter than the smartest existing humans – then this
closes the loop and creates a positive feedback cycle. What would humans with brain-computer
interfaces do with their augmented intelligence? One good bet is that they’d design the next 
generation of brain-computer interfaces. Intelligence enhancement is a classic tipping point; the
smarter you get, the more intelligence you can apply to making yourself even smarter.

Strong claim: This positive feedback cycle goes FOOM, like a chain of nuclear fissions gone 
critical – each intelligence improvement triggering an average of>1.000 further improvements of
similar magnitude – though not necessarily on a smooth exponential pathway. Technological progress 
drops into the characteristic timescale of transistors (or super-transistors) rather than human
neurons. The ascent rapidly surges upward and creates superintelligence (minds orders of magnitude
more powerful than human) before it hits physical limits.

Advocates: I. J. Good, Eliezer Yudkowsky
```

Note that while the core claims support each other, the strong claims are contradictory:

```markdown
If you extrapolate our existing version of Moore’s Law past the point of smarter-than-human AI to 
make predictions about 2099, then you are contradicting both the strong version of the Event 
Horizon (which says you can’t make predictions because you’re trying to outguess a transhuman mind)
and the strong version of the Intelligence Explosion (because progress will run faster once smarter-
than-human minds and nanotechnology drop it into the speed phase of transistors).
```

<a name="#the-singularity-has-happened"></a>
### The Singularity has happened
([overview](#overview))

Cosma Shalizi is usually hard to do justice to when quoting him, because he's so densely hyperlinked to so many cool further reads. That caveat out of the way, here's from [The Singularity in Our Past Light-Cone](http://bactra.org/weblog/699.html).

He's also great because in contrast to others of "his type", he's very well read in history, and so tends to provide appropriate historical contextualizing to recent events like general civilizational tech progress or whatever. This shows in the quote below.

```markdown
The Singularity has happened; we call it "the industrial revolution" or "the long nineteenth 
century". It was over by the close of 1918.

Exponential yet basically unpredictable growth of technology, rendering long-term extrapolation 
impossible (even when attempted by geniuses)? Check.

Massive, profoundly dis-orienting transformation in the life of humanity, extending to our 
ecology, mentality and social organization? Check.

Annihilation of the age-old constraints of space and time? Check.

Embrace of the fusion of humanity and machines? Check.

Creation of vast, inhuman distributed systems of information-processing, communication and
control, "the coldest of all cold monsters"? Check; we call them "the self-regulating market 
system" and "modern bureaucracies" (public or private), and they treat men and women, even those 
whose minds and bodies instantiate them, like straw dogs.

An implacable drive on the part of those networks to expand, to entrain more and more of the world
within their own sphere? Check. ("Drive" is the best I can do; words like "agenda" or "purpose" 
are too anthropomorphic, and fail to acknowledge the radical novely and strangeness of these 
assemblages, which are not even intelligent, as we experience intelligence, yet ceaselessly 
calculating.)

Why, then, since the Singularity is so plainly, even intrusively, visible in our past, does science
fiction persist in placing a pale mirage of it in our future? Perhaps: the owl of Minerva flies at 
dusk; and we are in the late afternoon, fitfully dreaming of the half-glimpsed events of the day, 
waiting for the stars to come out.
```

<a name="#Disneyland-with-no-children"></a>
### Disneyland with no children
([overview](#overview))

Finally dug up this quote! From Scott Alexander's legendary [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), quoting Nick Bostrom's *Superintelligence*:

```markdown
It is conceivable that optimal efficiency would be attained by grouping capabilities in
aggregates that roughly match the cognitive architecture of a human mind… But in the 
absence of any compelling reason for being confident that this so, we must countenance 
the possibility that human-like cognitive architectures are optimal only within the
constraints of human neurology (or not at all). When it becomes possible to build 
architectures that could not be implemented well on biological neural networks, new design
space opens up; and the global optima in this extended space need not resemble familiar 
types of mentality. Human-like cognitive organizations would then lack a niche in a
competitive post-transition economy or ecosystem.

We could thus imagine, as an extreme case, a technologically highly advanced society,
containing many complex structures, some of them far more intricate and intelligent than 
anything that exists on the planet today – a society which nevertheless lacks any type of
being that is conscious or whose welfare has moral significance. In a sense, this would be
an uninhabited society. It would be a society of economic miracles and technological 
awesomeness, with nobody there to benefit. A Disneyland with no children.
```

Scott then argues:

```markdown
The last value we have to sacrifice is being anything at all, having the lights on inside.
With sufficient technology we will be “able” to give up even the final spark.

*(Moloch whose eyes are a thousand blind windows!)*

Everything the human race has worked for – all of our technology, all of our civilization,
all the hopes we invested in our future – might be accidentally handed over to some kind of
unfathomable blind idiot alien god that discards all of them, and consciousness itself, in
order to participate in some weird fundamental-level mass-energy economy that leads to it 
disassembling Earth and everything on it for its component atoms.

*(Moloch whose fate is a cloud of sexless hydrogen!)*

Bostrom realizes that some people fetishize intelligence, that they are rooting for that 
blind alien god as some sort of higher form of life that ought to crush us for its own 
“higher good” the way we crush ants. He argues (Superintelligence, p. 219):

*The sacrifice looks even less appealing when we reflect that the superintelligence could 
realize a nearly-as-great good (in fractional terms) while sacrificing much less of our own
potential well-being. Suppose that we agreed to allow almost the entire accessible universe
to be converted into hedonium – everything except a small preserve, say the Milky Way, which
would be set aside to accommodate our own needs. Then there would still be a hundred billion
galaxies dedicated to the maximization of [the superintelligence’s own values]. But we would
have one galaxy within which to create wonderful civilizations that could last for billions 
of years and in which humans and nonhuman animals could survive and thrive, and have the 
opportunity to develop into beatific posthuman spirits.*

Remember: Moloch can’t agree even to this 99.99999% victory. Rats racing to populate an island
don’t leave a little aside as a preserve where the few rats who live there can live happy lives
producing artwork. Cancer cells don’t agree to leave the lungs alone because they realize it’s
important for the body to get oxygen. Competition and optimization are blind idiotic processes
and they fully intend to deny us even one lousy galaxy.

*They broke their backs lifting Moloch to Heaven! Pavements, trees, radios, tons! lifting the 
city to Heaven which exists and is everywhere about us!*

We will break our back lifting Moloch to Heaven, but unless something changes it will be his 
victory and not ours.
```

<a name="#Origins-of-information-society"></a>
## Origins of information society
([overview](#overview))

From Cosma Shalizi's [book review](http://bactra.org/reviews/beniger/) of James Beniger's *The Control Revolution: Technological and Economic Origins of the Information Society*:

```markdown
I was going to say that this is undoubtedly the best work ever done by a professor of 
communications, but that would be praising it with faint damns, and it deserves better. 
This is not a speculation or a vague schema but a very detailed history of the rise of 
technologies and techniques of communication and information-processing, and their use 
for controlling social and economic processes, prefaced by a general discussion of these
subjects and their importance for history. 

His thesis is that modern information technologies, and with them the "information society",
began to take shape in the 1830s with the introduction of railroads, and really took off 
after 1880 with full industrialization. Because machine industry involves huge, fast flows
of goods, it cannot be managed without a high level of information technology (in which 
Beniger includes things like product standardization, bureaucracy and advertising, as well
as the usual mechanical devices): and if it isn't managed it simply cannot work. The first
part of the economy to move at industrial speed were the railroads, and the accompanying
jump in the size of the information sector is dramatic. So industrial production is a good 
reason to improve your information technology; and conversely, improved control technology
makes new industrial developments possible.

Beniger puts the modern synthesis (not his phrase) of industry and information in the period
1880-1920. By the latter date, the technology of control had been so perfected that the
economies of all the warring powers in the Great War could be managed by central planning 
--- those of the Allies, by combined planning. (Since this performance was repeated during
the Second War, I'm tempted to say that market forces are simply too inefficient to be 
trusted with anything important, but this is not the place for those rants.) Since then,
he says, we have been in essentially the same industrial-economic-technological phase. The
advent of computers was obviously very important, but they didn't usher in the information
society, because we already were one (which, I suspect, is why they were able to spread so
quickly --- Beniger does not, alas, discuss computerization in detail).
```
