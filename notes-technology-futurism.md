*[Word count](https://wordcounter.net/): 3,100*

## What is this?

This is the "technology and futurism" section of [this notebook](https://github.com/monastri/monastri.github.io/blob/master/quotes.md), last updated Apr 6, 2019, which got so big (1.2 million char) that GitHub refused to render the whole page anymore, ruining my original dream of having my entire notebook in one long page for two purposes: (1) zero-latency clickthrough to make up for my working memory's sand-thru-sieve transience; (2) lower activation energy for continuous document-wide refactoring, to aid recall and cross-domain interlinking.

That said, this is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping. (Or at least that was the original intent before I had to break up the notebook. Now I'm not so sure I can do this.)

I also really, *really* hate experiences of the [Jeremy Bentham type](https://github.com/monastri/monastri.github.io/blob/master/notes-amazing-people.md#Jeremy-bentham). This document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [The Singularity](#the-singularity)
	1. [The three main schools of thought](#Three-Singularity-schools)
	2. [Replacing humans at every step of the economic chain](#replacing-humans-at-every-step-of-the-economic-chain)
	2. [Disneyland with no children](#Disneyland-with-no-children)
	3. [The Singularity has already happened](#the-singularity-has-happened)
2. [The origins of information society](#Origins-of-information-society)
3. [Rambling and AI](#rambling-and-ai)

---------------------------------------------------

<a name="#rambling-and-ai"></a>
### Rambling and AI
([overview](#overview))

From Robin Hanson's post [Better Babblers](http://www.overcomingbias.com/2017/03/better-babblers.html), speculating on how the world might change in the proliferation of GPT-2s: 

```markdown
Let me call styles of talking (or music, etc.) that rely mostly on low order correlations 
“babbling”. Babbling isn’t meaningless, but to ignorant audiences it often appears to be 
based on a deeper understanding than is actually the case. When done well, babbling can be
entertaining, comforting, titillating, or exciting. It just isn’t usually a good place to
learn deep insight.

As we slowly get better at statistics and machine learning, our machines will slowly get 
better at babbling. The famous Eliza chatbot went surprisingly far using very low order
correlations, and today chatbots best fool us into thinking they are human when they stick
to babbling style conversations. So what does a world of better babblers look like?

First, machines will better mimic low quality student essays, so schools will have to try 
harder to keep such students from using artificial babblers.

Second, the better machines get at babbling, the more humans will try to distinguish
themselves from machines via non-babbling conversational styles. So expect less use of 
simple easy-to-understand-and-predict speech in casual polite conversation, inspirational 
speeches, and public intellectual talk.

One option is to put a higher premium on talk that actually makes deep sense, in terms of 
deep concepts that experts understand. That would be nice for those of us who have always 
emphasized such things. But alas there are other options.

A second option is to put a higher premium on developing very distinctive styles of talking.
This would be like how typical popular songs from two centuries ago could be sung and 
enjoyed by most anyone, compared to how popular music today is matched in great detail to 
the particular features of particular artists. Imagine most all future speakers having as 
distinct a personal talking style.

A third option is more indirect, ironic, and insider style talk, such as we tend to see on
Twitter today. People using words and phrases and cultural references in ways that only folks
very near in cultural space can clearly accept as within recent local fashion. Artificial 
babblers might not have enough data to track changing fashions in such narrow groups.

Bottom line: the more kinds of conversation styles that simple machines can manage, the more 
humans will try to avoid talking in those styles, a least when not talking to machines.
```

<a name="#the-singularity"></a>
## The Singularity
([overview](#overview))

<a name="#Three-Singularity-schools"></a>
### Three Singularity schools
([overview](#overview))

Eliezer Yudkowsky's [essay](http://yudkowsky.net/singularity/schools/) continues to be a good summary of how things stand. First a complaint:

```markdown
I’ve heard (many) other definitions of the Singularity attempted, but I usually find them 
to lack separate premises and conclusions. For example, the old Extropian FAQ used to define
the “Singularity” as the Inflection Point, “the time when technological development will be
at its fastest” and just before it starts slowing down. 

But what makes this an interesting point in history apart from its definition? What are the
consequences of this assumption? To qualify as a school of thought or even a thesis, one needs
an internal structure of argument, not just a definition.
```

On to the three schools: 

```markdown
**Accelerating Change:**

Core claim: Our intuitions about change are linear; we expect roughly as much change as has
occurred in the past over our own lifetimes. But technological change feeds on itself, and 
therefore accelerates. Change today is faster than it was 500 years ago, which in turn is 
faster than it was 5000 years ago. Our recent past is not a reliable guide to how much change
we should expect in the future.

Strong claim: Technological change follows smooth curves, typically exponential. Therefore we
can predict with fair precision when new technologies will arrive, and when they will cross key
thresholds, like the creation of Artificial Intelligence.

Advocates: Ray Kurzweil, Alvin Toffler(?), John Smart

**Event Horizon:**

Core claim: For the last hundred thousand years, humans have been the smartest intelligences 
on the planet. All our social and technological progress was produced by human brains. Shortly,
technology will advance to the point of improving on human intelligence (brain-computer 
interfaces, Artificial Intelligence). This will create a future that is weirder by far than most
science fiction, a difference-in-kind that goes beyond amazing shiny gadgets.

Strong claim: To know what a superhuman intelligence would do, you would have to be at least
that smart yourself. To know where Deep Blue would play in a chess game, you must play at Deep
Blue’s level. Thus the future after the creation of smarter-than-human intelligence is absolutely
unpredictable.

Advocates: Vernor Vinge

**Intelligence Explosion:**

Core claim: Intelligence has always been the source of technology. If technology can significantly 
improve on human intelligence – create minds smarter than the smartest existing humans – then this
closes the loop and creates a positive feedback cycle. What would humans with brain-computer
interfaces do with their augmented intelligence? One good bet is that they’d design the next 
generation of brain-computer interfaces. Intelligence enhancement is a classic tipping point; the
smarter you get, the more intelligence you can apply to making yourself even smarter.

Strong claim: This positive feedback cycle goes FOOM, like a chain of nuclear fissions gone 
critical – each intelligence improvement triggering an average of>1.000 further improvements of
similar magnitude – though not necessarily on a smooth exponential pathway. Technological progress 
drops into the characteristic timescale of transistors (or super-transistors) rather than human
neurons. The ascent rapidly surges upward and creates superintelligence (minds orders of magnitude
more powerful than human) before it hits physical limits.

Advocates: I. J. Good, Eliezer Yudkowsky
```

Note that while the core claims support each other, the strong claims are contradictory:

```markdown
If you extrapolate our existing version of Moore’s Law past the point of smarter-than-human AI to 
make predictions about 2099, then you are contradicting both the strong version of the Event 
Horizon (which says you can’t make predictions because you’re trying to outguess a transhuman mind)
and the strong version of the Intelligence Explosion (because progress will run faster once smarter-
than-human minds and nanotechnology drop it into the speed phase of transistors).
```

<a name="#the-singularity-has-happened"></a>
### The Singularity has happened
([overview](#overview))

Cosma Shalizi is usually hard to do justice to when quoting him, because he's so densely hyperlinked to so many cool further reads. That caveat out of the way, here's from [The Singularity in Our Past Light-Cone](http://bactra.org/weblog/699.html).

He's also great because in contrast to others of "his type", he's very well read in history, and so tends to provide appropriate historical contextualizing to recent events like general civilizational tech progress or whatever. This shows in the quote below.

```markdown
The Singularity has happened; we call it "the industrial revolution" or "the long nineteenth 
century". It was over by the close of 1918.

Exponential yet basically unpredictable growth of technology, rendering long-term extrapolation 
impossible (even when attempted by geniuses)? Check.

Massive, profoundly dis-orienting transformation in the life of humanity, extending to our 
ecology, mentality and social organization? Check.

Annihilation of the age-old constraints of space and time? Check.

Embrace of the fusion of humanity and machines? Check.

Creation of vast, inhuman distributed systems of information-processing, communication and
control, "the coldest of all cold monsters"? Check; we call them "the self-regulating market 
system" and "modern bureaucracies" (public or private), and they treat men and women, even those 
whose minds and bodies instantiate them, like straw dogs.

An implacable drive on the part of those networks to expand, to entrain more and more of the world
within their own sphere? Check. ("Drive" is the best I can do; words like "agenda" or "purpose" 
are too anthropomorphic, and fail to acknowledge the radical novely and strangeness of these 
assemblages, which are not even intelligent, as we experience intelligence, yet ceaselessly 
calculating.)

Why, then, since the Singularity is so plainly, even intrusively, visible in our past, does science
fiction persist in placing a pale mirage of it in our future? Perhaps: the owl of Minerva flies at 
dusk; and we are in the late afternoon, fitfully dreaming of the half-glimpsed events of the day, 
waiting for the stars to come out.
```

<a name="#Disneyland-with-no-children"></a>
### Disneyland with no children
([overview](#overview))

Finally dug up this quote! From Scott Alexander's legendary [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), quoting Nick Bostrom's *Superintelligence*:

```markdown
It is conceivable that optimal efficiency would be attained by grouping capabilities in
aggregates that roughly match the cognitive architecture of a human mind… But in the 
absence of any compelling reason for being confident that this so, we must countenance 
the possibility that human-like cognitive architectures are optimal only within the
constraints of human neurology (or not at all). When it becomes possible to build 
architectures that could not be implemented well on biological neural networks, new design
space opens up; and the global optima in this extended space need not resemble familiar 
types of mentality. Human-like cognitive organizations would then lack a niche in a
competitive post-transition economy or ecosystem.

We could thus imagine, as an extreme case, a technologically highly advanced society,
containing many complex structures, some of them far more intricate and intelligent than 
anything that exists on the planet today – a society which nevertheless lacks any type of
being that is conscious or whose welfare has moral significance. In a sense, this would be
an uninhabited society. It would be a society of economic miracles and technological 
awesomeness, with nobody there to benefit. A Disneyland with no children.
```

Scott then argues:

```markdown
The last value we have to sacrifice is being anything at all, having the lights on inside.
With sufficient technology we will be “able” to give up even the final spark.

*(Moloch whose eyes are a thousand blind windows!)*

Everything the human race has worked for – all of our technology, all of our civilization,
all the hopes we invested in our future – might be accidentally handed over to some kind of
unfathomable blind idiot alien god that discards all of them, and consciousness itself, in
order to participate in some weird fundamental-level mass-energy economy that leads to it 
disassembling Earth and everything on it for its component atoms.

*(Moloch whose fate is a cloud of sexless hydrogen!)*

Bostrom realizes that some people fetishize intelligence, that they are rooting for that 
blind alien god as some sort of higher form of life that ought to crush us for its own 
“higher good” the way we crush ants. He argues (Superintelligence, p. 219):

*The sacrifice looks even less appealing when we reflect that the superintelligence could 
realize a nearly-as-great good (in fractional terms) while sacrificing much less of our own
potential well-being. Suppose that we agreed to allow almost the entire accessible universe
to be converted into hedonium – everything except a small preserve, say the Milky Way, which
would be set aside to accommodate our own needs. Then there would still be a hundred billion
galaxies dedicated to the maximization of [the superintelligence’s own values]. But we would
have one galaxy within which to create wonderful civilizations that could last for billions 
of years and in which humans and nonhuman animals could survive and thrive, and have the 
opportunity to develop into beatific posthuman spirits.*

Remember: Moloch can’t agree even to this 99.99999% victory. Rats racing to populate an island
don’t leave a little aside as a preserve where the few rats who live there can live happy lives
producing artwork. Cancer cells don’t agree to leave the lungs alone because they realize it’s
important for the body to get oxygen. Competition and optimization are blind idiotic processes
and they fully intend to deny us even one lousy galaxy.

*They broke their backs lifting Moloch to Heaven! Pavements, trees, radios, tons! lifting the 
city to Heaven which exists and is everywhere about us!*

We will break our back lifting Moloch to Heaven, but unless something changes it will be his 
victory and not ours.
```

<a name="#Origins-of-information-society"></a>
## Origins of information society
([overview](#overview))

From Cosma Shalizi's [book review](http://bactra.org/reviews/beniger/) of James Beniger's *The Control Revolution: Technological and Economic Origins of the Information Society*:

```markdown
I was going to say that this is undoubtedly the best work ever done by a professor of 
communications, but that would be praising it with faint damns, and it deserves better. 
This is not a speculation or a vague schema but a very detailed history of the rise of 
technologies and techniques of communication and information-processing, and their use 
for controlling social and economic processes, prefaced by a general discussion of these
subjects and their importance for history. 

His thesis is that modern information technologies, and with them the "information society",
began to take shape in the 1830s with the introduction of railroads, and really took off 
after 1880 with full industrialization. Because machine industry involves huge, fast flows
of goods, it cannot be managed without a high level of information technology (in which 
Beniger includes things like product standardization, bureaucracy and advertising, as well
as the usual mechanical devices): and if it isn't managed it simply cannot work. The first
part of the economy to move at industrial speed were the railroads, and the accompanying
jump in the size of the information sector is dramatic. So industrial production is a good 
reason to improve your information technology; and conversely, improved control technology
makes new industrial developments possible.

Beniger puts the modern synthesis (not his phrase) of industry and information in the period
1880-1920. By the latter date, the technology of control had been so perfected that the
economies of all the warring powers in the Great War could be managed by central planning 
--- those of the Allies, by combined planning. (Since this performance was repeated during
the Second War, I'm tempted to say that market forces are simply too inefficient to be 
trusted with anything important, but this is not the place for those rants.) Since then,
he says, we have been in essentially the same industrial-economic-technological phase. The
advent of computers was obviously very important, but they didn't usher in the information
society, because we already were one (which, I suspect, is why they were able to spread so
quickly --- Beniger does not, alas, discuss computerization in detail).
```
