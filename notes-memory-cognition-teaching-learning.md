*[Word count](https://wordcounter.net/): 25,500*

## What is this?

This is the "memory, cognition, teaching and learning" section of [this notebook](https://github.com/monastri/monastri.github.io/blob/master/quotes.md), last updated Apr 6, 2019, which got so big (1.2 million char) that GitHub refused to render the whole page anymore, ruining my original dream of having my entire notebook in one long page for two purposes: (1) zero-latency clickthrough to make up for my working memory's sand-thru-sieve transience; (2) lower activation energy for continuous document-wide refactoring, to aid recall and cross-domain interlinking.

That said, this is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

(There's also the parallel to [perpetual beta](https://breakingsmart.com/en/season-1/running-code-and-perpetual-beta/) for web-based software: scaffolding for extensive ongoing experimentation within the already-deployed app. Never start, as Gwern says.)

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

Relatedly, I aspire towards the vision Tiago Forte fleshes out in his [digital notes as second brain essay](https://github.com/monastri/monastri.github.io/edit/master/notes-memory-cognition-teaching-learning.md#Offload-thinking-to-second-brain).

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping. (Or at least that was the original intent before I had to break up the notebook. Now I'm not so sure I can do this.)

I also really, *really* hate experiences of the [Jeremy Bentham type](https://github.com/monastri/monastri.github.io/blob/master/notes-amazing-people.md#Jeremy-bentham). This document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [Augmenting long-term memory](#augmenting-long-term-memory), e.g. Anki
2. [Cognitive science](#cognitive-science)
2. [Collective cognition](#Collective-cognition)
2. [Externalizing the brain](#external-brain), e.g. Google, writing
	1. [The social computational hindbrain and the planetwide metabrain](#Social-computational-hindbrain), e.g. newsletter/blog+commentariat
	2. [Personal knowledge management: what, why, when, where, how](#Personal-knowledge-management)
	3. [Offload thinking to second brain](#Offload-thinking-to-second-brain), ft. Tiago Forte's Evernote work 
2. [Learning](#learning)
	1. [Errors vs bugs](#errors-vs-bugs)
	2. [Learning as maximizing the throughput of assumptions invalidated](#maximizing-the-throughput-of-assumptions-invalidated)
	2. [Important findings on learning](#important-findings-on-learning), ft. 25 principles by U of Missouri
3. [Names matter](#names-matter)
	1. [Concept handles](#Concept-handles)
	2. [Naming functions in large versioned codebases](#Naming-functions-in-large-codebases), on Wolfram's 30+ year *Mathematica* odyssey
3. [Polymathy, or being a generalist](#polymathy)
	1. [Semicolon-shaped people](#Semicolon-shaped-people)
	2. [Polymathy via designing knowledge-based programming languages](#polymathy-via-designing-general-programming-languages)
2. [Procedural vs declarative memory](#procedural-vs-declarative-memory)
4. [Steve Yegge on memory](#Yegge-on-memory)
5. [Wisdom](#wisdom)
	1. [The predictive uselessness of folk wisdom](#The-predictive-uselessness-of-folk-wisdom) 
	

----------------------------------


<a name="#Learning"></a>
## Learning
([overview](#overview))

<a name="#maximizing-the-throughput-of-assumptions-invalidated"></a>
### Maximizing the throughput of assumptions invalidated
([overview](#overview))

From Tiago Forte's most personally memorable Ribbonfarm essay, [The throughput of learning](https://www.ribbonfarm.com/2017/01/31/the-throughput-of-learning/).

Stages of learning:

```markdown
When you first start learning, early in life, there is a bottleneck in the amount
of information you have access to. You soak up everything like a sponge, because 
you are open and there is relatively little to absorb.

But very quickly, in elementary school, your access to information stops being the
limiting factor. You take home a few giant textbooks, and suddenly the bottleneck
moves to ways of structuring and contextualizing the information.

In high school, you learn a variety of methods to structure information — outlines,
diagrams, underlining and highlighting, reports, essays, notebooks and binders. The
bottleneck moves to your ability to synthesize this information, to turn it into 
new ideas.

In college, if you make it that far, the bottleneck moves to insight generation. You
start questioning the world as given, and find that the juiciest intellectual rewards
are ideas that shift how you view it. You start hunting for the revolutionary, the 
controversial, steering your learning toward the red pills of paradoxes and
contradictions.

If you are lucky enough to go beyond this, the bottleneck moves once again: to your 
assumptions. They constrain your view, what you are allowed to see, and thereby the 
thoughts and actions available to you. You start getting a kick out of unearthing new
assumptions, shining a light on blindspots that, by definition, you didn’t know you 
didn’t know about. This process is unbounded, because with enough examination, all 
your beliefs are revealed to be assumptions.

There are many ways to reveal assumptions. Interesting experiences, traveling, genuine
conversation, and reading fiction all help you question your own point of view.
```

So far so good; I liked his "level-upping" in the same way I liked Terry Tao's "three stages of mathematical rigor". 

So what does Tiago add here? The idea of modeling learning as throughput, the way he's framed it above, invites using frameworks/metaphors from manufacturing production experience (cf Eliyahu Goldratt's *The Goal*):

```markdown
What does the history of manufacturing production experience teach us about how to
maximize throughput?

The first lesson is that quantity and quality are not opposing forces. You don’t have 
to sacrifice one to get more of the other. One can be used to enhance the other, like 
two sides of a coin.

There is a common misconception about throughput: that it is just a matter of “more 
widgets produced.” That it seeks to maximize total output, without regard to the 
quality of the thing being produced. But nothing could be further from the truth. 

In fact, *throughput accounting* is almost fanatical in its insistence that a sale has
not been made until the final customer has purchased. This is in sharp contrast to 
mainstream cost accounting, which allows each producer in the supply chain to record a 
sale as soon as they’ve passed on the product to the next link in the chain. What this 
means is that the entire supply chain is held accountable to the most demanding standard
of innovation and quality: the final customer spending her own money.

The most skilled practitioners of this yin and yang between quantity and quality is, of 
course, Toyota. Toward the end of his life, Taiichi Ohno, the creator of the Toyota 
Production System (TPS) that revolutionized manufacturing around the world, said a 
curious thing:

	I’m proud to be Japanese and I wanted my country to succeed. I believed
	my system was a way that could help us become a modern industrial nation.
	That is why I had no problem sharing it with other Japanese companies, 
	even my biggest competitors. But I was very, very concerned that you
	Americans and Europeans would understand what we were doing, copy it, and
	defeat us in the marketplace. I did my best to prevent the visitors from 
	fully grasping our overall approach. I explained it by talking about 
	reduction of the seven wastes (muda)…and by talking about techniques with
	Japanese names like kanban.
	
What Ohno’s quote and the most recent research seem to suggest is that we copied their 
system at the wrong level of abstraction. TPS is not a process for maximizing the 
throughput of finished products, as we’ve always assumed and admired. It is a process
for maximizing the throughput of process improvements, even at the expense of short-
term profitability.

Adding inventory between work centers, for example, would actually make the production 
process more adaptive in the short term, since interruptions in one place wouldn’t 
immediately disrupt the others. But the fragility of “single-piece flow” is its most 
important feature: by preventing extra resources from “flowing around” the problem, it 
forces everyone to improve the consistency and reliability of all parts of the process.
And quality is improved as a byproduct. The system is designed to break in ways that 
surface the most useful lessons for improvement. The point of idealized goals like “zero 
waste” is not to obtain an outcome (which is literally impossible), but to create 
momentum in a direction that is likely to lead to lots of informative failures.

Similarly, modern learning is not a process for maximizing the throughput of insights, 
but for maximizing the throughput of learning process improvements. The best assumptions
to invalidate in our quest for learning are assumptions about learning itself. This is 
why meditation retreats, globe-trotting, and having kids will always be net productivity
gains, broadly defined: even a slight improvement in the machinery of learning (via a 
shift in perspective, for example) will pay dividends over time far greater than a mere 
few months of lost labor.	
```

I'll admit to not being very convinced of the analogy, because with the former I can concretely imagine what's being improved while the latter stumps me, and Tiago isn't really helping beyond giving a cool-sounding sales pitch and a bunch of pretty diagrams. I still like the historical anecdotes he brings up though. 

Turns out that Tiago (seems to) allay my concerns in the very next paragraph with another historical anecdote, this time about inventory turnover accelerating and then going negative:

```markdown
But here we’re presented with a dilemma: how does one learn about one’s own assumptions about learning? Since you don’t know what you don’t know, it’s difficult to learn anything about it. Our assumptions about learning seem to be permanently out of reach.

Another historical example gives us a possible clue: the history of inventory turnover.

Inventory turnover is a measure of the number of times inventory is sold or used in a
time period. In the 1970s, U.S. manufacturing firms had an average turnover of 3.7 (in
other words, they sold their full stock of inventory, on average, 3.7 times per year).
The Japanese average was only a little higher, at 5.5. By the 1980’s, this had
accelerated dramatically to nearly 20, with the highest-performing Western firms 
achieving turnover of between 30 and 80. Soon after, Japanese firms achieved an 
astounding three-digit inventory turnover, selling their stock 100 or more times per 
year. No one thought it could go any higher.

And then something weird happened: turnover sped up a little more, and suddenly turned
negative. What happened was that production got so fast that the manufacturer was able
to receive payment for the final product before they even had to pay their own suppliers.
The customer could pay for and eat his hamburger before the restaurant has paid for the
meat.

This shift, a linear change in speed that inverted the whole logic of business, 
presented us with some powerful, if challenging, questions: if we’re able to charge the 
customer before paying for the supplies, why should we be limited by our initial 
investment or working capital? Why not do the selling before production, so we know 
exactly what to make, and how much? What other stages of production don’t have to happen
in sequence? We’re still working out the implications to this day: Kickstarter campaigns
sell a vision to deliver on later; plentiful credit and falling barriers to entry make 
borrowing from the future easier than ever; the field of Lean UX provides us a ready-made
toolkit of methods for validating before producing.

And note that this vast acceleration did not happen at the expense of quality. Over the 
same time period, we went from talking about defects in terms of “yield” (losses of more
than 10%), to “parts per million,” an improvement of four orders of magnitude in under 
two decades.
```

Another oft-repeated lesson: dramatic-enough quantitative changes beget qualitative changes. More is different.

```markdown
This example illustrates one way blindspots can be revealed: by accelerating a system 
so much that its rules break, forcing everyone to confront its underlying assumptions.
The shift to negative inventory turnover wasn’t technically or logistically difficult, 
but it required the reassessment of a deeply-held assumption: that the stages of 
production have to happen in a particular order.

This works because, when it comes to complex systems, faster is different. When you push
a system to sufficient speed, it starts consuming itself, like a black hole turning
inside out and emerging through the wormhole into an alternate dimension, where all the
laws and rules are completely different. The system ceases to be a static backdrop where
things take place, and becomes merely another unit of flow in a larger, more abstract 
system. ...

...all the levels of abstraction are always present and possible, it’s just a matter of 
which level we want to operate at.
```

Relieving a constraint in a complex interdependent system makes it move:

```markdown
In manufacturing, relieving the constraint on production often moves it to distribution, 
as you try to get this new surplus of products out to stores. Relieving distribution
moves the constraint to supply, as your suppliers have trouble keeping up. Relieving 
supply moves it to sales and marketing, since you’re now able to deliver as many products
as you can sell. And so forth, forever.

What does this look like in a knowledge work organization? Let’s say you relieve the 
constraint on your software engineering team. This, of course, moves it to the product 
team, who has to develop product requirements faster in order to keep up. As the CEO, 
you could treat this movement of the constraint as a finish line, as evidence that you’ve
succeeded. Pat yourself on the back, maybe take a break to do some “change management.” Or
you could treat it as a starting line, just the table stakes in a different game at a 
higher level.

You could continue to accelerate this process, pushing your team to continuously identify
and relieve bottlenecks as soon as they emerge. “Solving” for a particular constraint is no
longer an objective in itself, but a way to keep the game going. Crucially, this doesn’t 
necessarily involve more work, just different work. Most problems you don’t bother trying
to solve and can just ignore, because you know that pushing the system to the next level 
of emergence will change all the rules anyway, bringing fresh problems. You focus your 
resources only on the constraints keeping you from pushing the system just past the edge
of its activation boundary, letting the momentum from constraint to constraint do most of
the work.
```

(I wish he included more examples. Tiago could rightly counter that he'd given enough, so let him pontificate.)

Of course, we were talking about personal learning in the first place:

```markdown
The difficulty in applying this concept to individual learning is that, in this case, 
you are the system. It’s a little disconcerting being accelerated, turned inside out, 
and then sucked into an alternate dimension where everything you were sure was true is
wrong. Or worse, irrelevant.

The key is to realize that you are not a thing, which can be deformed and broken, but 
an environment. Like a factory, you could say. We have entryways and exits, windows and
doors, well-lit areas and dark, forgotten corners. There’s a lot of machinery, much of 
it outdated, and it’s arguably more difficult to move around our mental machinery than 
the equipment on a shop floor.

It’s often said that knowledge work is totally non-linear, and thus completely
incomparable to a production line. But there is another sense in which we do operate in 
a linear world: we operate in time. Time is even less forgiving than the tightest
conveyor belt — there is no cord to stop it, and not even one minute that’s lost can ever
be recovered. Not one hour can be reworked, and our supply is fundamentally limited.

But we work with ideas, which live forever, right? Maybe not. In another sense, ideas are 
highly perishable. The requirements for a piece of software take a lot of effort to collect,
but quickly go out of date if not acted upon, as the needs of users evolve. This is fitting,
as legend has it that Ohno was first inspired by the American grocery store and its on-
demand replenishment of perishable goods.

But knowledge work is different! It doesn’t have the long setup times or changeovers of the
old days. And yet it does: we know that the first stage of flow is struggle, as we get 
physically, mentally, and emotionally situated to the task at hand. And this doesn’t get any
faster over time. We have to go through it every time we switch contexts.

It seems like what we’re left with as knowledge workers is a cavernous mental environment 
filled with heavy machinery, which we use to process highly perishable ideas through a 
rigidly linear flow of time, with long, delicate setups that have to be changed over every 
few hours or so. And no breaks — we’re always in there.

Our design problem, at least, is clear: we have to design our mental environment to maximize
the throughput of invalidated assumptions, accelerating it to the point that the rules of our
learning process break, thereby surfacing even more assumptions, which we can exploit to 
further improve this process.
```

Okay, so how to do that? Don't see, listen:

```markdown
Most theories of action sooner or later seem to settle on seeing as the most important 
leverage point in a situation, from Toyota’s “Go and see” to Boyd’s Observation/Orientation. I
think what is required to make this model of learning work is, instead, a different way of 
*listening*. Specifically, listening for assumptions.

Listening for assumptions is a peculiar skill, not at all natural, that requires a continuous
disassociation of what someone is saying, from the model they’re constructing in their mind as
they go along. It’s like plucking fish from a stream, the actual words and sentences becoming 
just a delivery vehicle for potential entry points into their mental model.

But this is a lot harder than fishing, because there is no discrete activity you can focus on,
to the exclusion of everything else. You have to actually listen to the words, even though they
are mere delivery vehicles. You have to allow yourself to be emotionally impacted, because your
intuition has a much higher bandwidth than your conscious thought. You have to care about what
they care about, subordinate your interests to their framing, pretend their priorities matter 
so convincingly that you yourself are convinced. You have to stand IN the stream.

True listening requires giving up the prerogative of your own mental model. You have to allow
them to set the rules of engagement, no matter how bizarre, so that they let their guard down
and realize you are not a threat, because you have no intention of blaming them for anything. 
The way they set these rules will reveal their assumptions and constraints, which thoughts and
actions are open to them. If you can tell an authentic story that speaks to these assumptions,
you can break through, because stories speak to emotions expressed in the body, which
fortunately refuses to go along with even our most well-reasoned rationalizations.
```

I'll admit that sounds dangerous, since some people (hedgehogs especially) are more liable to brainwashing than others.

```markdown
The interesting thing about constraints is that they are never on you. They are constraints
on your context, shaping the space of possibilities you allow yourself to consider. You can’t 
change anyone’s mind (have you noticed?), but you may be able to change how they perceive their
context. If you succeed, they may be able to step out of the criteria by which they enumerate
their options. Life does not present itself in the form of multiple-choice questions. It is we 
who choose the choices, and we do it together.

What all this has to do with learning is that the deepest assumptions can only be revealed 
through experience and stories, not by reading books or having intellectual arguments. We do 
these things through the same old lens, and thus cannot examine the lens. It takes another free
mind, reaching up and taking off our spectacles, to show us the cracks and the foggy areas. At 
some point, this way of listening turns into a way of thinking, as you apply it to your own
thoughts. Unmoored from your own certain beliefs, you step back from what seemed just a moment 
ago to be your very identity, only to find that it is just a mental object. With each step 
backward, you distinguish your self one by one from bodily sensations, from emotions, from 
opinions, from thoughts, from principles, from values, from systems, from goals. They are all 
tools, to be taken up and put down again when no longer needed.
```

Bit of a non sequitur in the middle there, but I like what he says towards the end. (Honestly it all sounds like [Romeo Stevens on Buddhism's core loop](https://github.com/monastri/monastri.github.io/blob/master/notes-philosophy-erisology-altruism-culture-spirituality.md#Buddhism-is-impenetrable-because-it-hasnt-been-translated).)

<a name="#errors-vs-bugs"></a>
### Errors vs bugs
([overview](#overview))

Sarah Constantin is one of my favorite writers -- she appears a *lot* in these notebooks. She has a blog, [Otium](https://srconstantin.wordpress.com/), and goes by celandine13 on LiveJournal. This quote on the *error vs bug model of learning* comes from her LJ essay [Errors vs Bugs and the End of Stupidity](https://celandine13.livejournal.com/33599.html).

```markdown
A common mental model for performance is what I'll call the "error model."  In the error model, a person's 
performance of a musical piece (or performance on a test) is a perfect performance plus some random error.  
You can literally think of each note, or each answer, as x + c times epsilon_i, where x is the correct note 
/ answer, and epsilon_i is a random variable, iid Gaussian or something.  Better performers have a lower 
error rate c.  Improvement is a matter of lowering your error rate.  This, or something like it, is the model 
that underlies school grades and test scores. Your grade is based on the percent you get correct.  Your 
performance is defined by a single continuous parameter, your accuracy.

But we could also consider the "bug model" of errors.  A person taking a test or playing a piece of music is 
executing a program, a deterministic procedure.  If your program has a bug, then you'll get a whole class of 
problems wrong, consistently.  Bugs, unlike error rates, can't be quantified along a single axis as less or 
more severe.  A bug gets everything that it affects wrong.  And fixing bugs doesn't improve your performance 
in a continuous fashion; you can fix a "little" bug and immediately go from getting everything wrong to 
everything right.  You can't really describe the accuracy of a buggy program by the percent of questions it 
gets right; if you ask it to do something different, it could suddenly go from 99% right to 0% right.  You 
can only define its behavior by isolating what the bug does.

Often, I think mistakes are more like bugs than errors.  My clinkers weren't random; they were in specific 
places, because I had sub-optimal fingerings in those places.  A kid who gets arithmetic questions wrong 
usually isn't getting them wrong at random; there's something missing in their understanding, like not 
getting the difference between multiplication and addition.  Working generically "harder" doesn't fix bugs 
(though fixing bugs does require work). 

Once you start to think of mistakes as deterministic rather than random, as caused by "bugs" (incorrect 
understanding or incorrect procedures) rather than random inaccuracy, a curious thing happens.

You stop thinking of people as "stupid."

Tags like "stupid," "bad at X", "sloppy," and so on, are ways of saying "You're performing badly and I 
don't know why." Once you move it to "you're performing badly because you have the wrong fingerings," or 
"you're performing badly because you don't understand what a limit is," it's no longer a vague personal 
failing but a causal necessity. Anyone who never understood limits will flunk calculus. 

It's not you, it's the bug.
```

The rest of the linked article is *fantastic*. I had trouble quoting it because I felt like quoting everything.

<a name="#important-findings-on-learning"></a>
### Important findings on learning
([overview](#overview))

From the University of Missouri's [25 Learning Principles to Guide Pedagogy and the
Design of Learning Environments](http://whaaales.com/25principlesoflearning.pdf). I only picked those I found interesting. 

Contiguity Effects:

```markdown
Ideas that need to be associated should be presented contiguously in space and time in the
multimedia learning environment. For example, the verbal label for a picture needs to be placed
spatially near the picture on the display, not on the other side of the screen. An explanation of an
event should be given when the event is depicted rather than many minutes, hours, or days later.
```

Perceptual-motor Grounding:

```markdown
Whenever a concept is first introduced, it is important to ground it in a concrete
perceptual-motor experience. The learner will ideally visualize a picture of the concept, will be
able to manipulate its parts and aspects, and will observe how it functions over time. The teacher
and learner will also gain a common ground (shared knowledge) of the learning material.

Perceptual-motor experience is particularly important when there is a need for precision, such as
getting directions to find a spatial location. For example, a course in statistics is not grounded in
perceptual-motor experience when the teacher presents symbols and formulae that have no
meaning to the student and cannot be visualized
```

Dual Code and Multimedia Effects:

```markdown
Information is encoded and remembered better when it is delivered in multiple modes
(verbal and pictorial), sensory modalities (auditory and visual), or media (computers and
lectures) than when delivered in only a singe mode, modality, or medium. Dual codes provide
richer and more varied representations that allow more memory retrieval routes. 

However, the amount of information should not overwhelm the learner because attention is 
split or cognitive capacities are overloaded.
```

Testing Effect:

```markdown
There are direct and indirect effects of taking frequent tests. One indirect benefit is that
frequent testing keeps students constantly engaged in the material. Although students will learn
from testing without receiving feedback, there is less forgetting if students receive informative
feedback about their performance. Multiple tests slow forgetting better than a single test.
Formative assessment refers to the use of testing results to guide teachers in making decisions
about what to teach. Learners also benefit if they use test results as a guide for their own
learning. 
```

The best way to do this is via spaced repetition. It's the whole idea behind Anki flashcards. See also [here](#augmenting-long-term-memory), as well as the next section -- Spaced Effects:

```markdown
Spaced schedules of testing (like spaced schedules of studying) produce better long-term
retention than a single test. When a single test is administered immediately after learning,
students obtain high scores, but long-term retention is reduced with a single immediate test
relative to spaced testing. When a test is given immediately after learning has occurred, learners
still have the newly-learned information in a primary memory system and therefore obtain high
test scores. Both teachers and learners often misjudge their high scores on a test given
immediately after learning as evidence of good retention, when, in fact, long-term retention
suffers with this practice
```

Generation Effect:

```markdown
Learning is enhanced when learners produce answers compared to having them recognize
answers. Free recall or essay tests which require the test taker to generate answers with minimal
cues produce better learning than multiple choice tests in which the learner only needs to be able
to recognize correct answers. In fact, free recall tests produce as much learning as restudying the
material
```

Organization effects (part of the reason this document exists in the first place!):

```markdown
Outlining, integrating, and synthesizing information produces better learning than rereading
materials or other more passive strategies. Students frequently report that when they study they
reread materials they already read once. Strategies that require learners to be actively engaged
with the material to-be-learned produce better long-term retention than the passive act of reading.
Learners should develop their own mini-testing situations as they review, such as stating the
information in their own words (without viewing the text) and synthesizing information from
multiple sources, such as from class and textbooks.
```

Coherence effect (Robert Frost does this well in [his answers on Quora](https://www.quora.com/profile/Robert-Frost-1/answers)):

```markdown
The learner needs to get a coherent, well connected representation of the main ideas to be
learned. It is important to remove distracting, irrelevant material, even when the added
information is artistically appealing. Seductive details that do not address the main points to be
conveyed run the risk of consuming the learner’s attention and effort at the expense of their
missing the main points. 
```

Stories and example cases (see also [John Baez on why math publications should take a page from the storytelling skillbook](#why-math-is-boring)):

```markdown
Stories and other forms of narrative are easier to read, comprehend, and remember than
other types of learning materials. For many millennia, the primary way of passing wisdom down
from generation to generation was through stories. Stories have concrete characters, objects,
locations, plot, themes, emotions, and actions that bear some similarity to everyday experiences.
Many stories also convey a point or moral that can be generalized to many situations. Example
cases in a story-like format are persuasive, easy to comprehend, and very memorable. 
```

Negative suggestion effects:

```markdown
Just as people learn correct information with frequent testing, they also can learn wrong
information this way. For example, when incorrect alternatives on multiple choice tests are
presented, the wrong answers can be learned instead of the correct answers. This effect is also
found on short answer essay questions when students do not know the answers and use their
general knowledge about the field to construct a response that seems reasonable to them. In this
situation, learners recall their incorrect, but logically consistent response as being correct. These
effects can be reduced when learners receive feedback immediately after taking a test which
allows them to revise their memory and understanding without delay.
```

Desirable Difficulties (struggle promotes long-term recall -- true more so in math):
 
```markdown
Learning is enhanced when learners have to organize the information themselves or exert
additional effort during acquisition or retrieval than in conditions in which the information to be
learned or retrieved does not require effort. One possible explanation for this effect is that
learners create multiple retrieval paths which make the information more accessible at retrieval.
These practices slow initial learning, but promote long-term recall.
```

Explanation effects:

```markdown
Explanations consist of causal analyses of events, logical justifications of claims, and
functional rationales for actions. Explanations provide coherence to the material and justify why
information is relevant and important. Students may be prompted to give self-explanations of the
material through think aloud protocols or questioning tasks that elicit explanations that connect
the material to what they already know. Self-explanations and the activity of studying good
explanations facilitate deeper comprehension, learning, memory, and transfer. 
```

Deep questions:

```markdown
Deep explanations of material and reasoning are elicited by questions such as why, how,
what-if-and what-if not, as opposed to shallow questions that require the learner to simply fill in
missing words, such as who, what, where, and when. Training students to ask deep questions
facilitates comprehension of material from text and classroom lectures. The learner gets into the
mindset of having deeper standards of comprehension and the resulting representations are more
elaborate. 
```

Cognitive disequilibrium:

```markdown
Cognitive disequilibrium stimulates inquiry, curiosity, thinking, and deep questions,
which in turn lead to deeper learning. Cognitive disequilibrium occurs when there are obstacles
to goals, contradictions, conflicts, anomalous events, breakdown scenarios, salient gaps in
knowledge, uncertainty, equally attractive alternatives, and other types of impasses. When these
impasses occur, the learner needs to engage in reasoning, thought, problem solving, and planning
in route to restoring cognitive equilibrium. There is a higher incidence of deep questions,
thought, reasoning, and study efforts when learners undergo cognitive disequilibrium.
```

Goldilocks principle: 

```markdown
Assignments should not be too hard or two easy, but at the right level of difficulty for the
student’s level of skill or prior knowledge. The definition of the zone of proximal development
(ZPD) is a bit more technical: the difference in learning that occurs with versus without a
learning scaffold (e.g., tutor, teacher, text, and computer). Researchers have identified a number
of zones that reflect how much learning, memory, mastery, or satisfaction occurs along a
continuum of task difficulty and that is sensitive to individual differences among learners. When
the material is too easy for the learner, the student is not challenged and may get bored. When it
is too difficult, the student acquires very little and gets frustrated or tunes out
```

Anchored learning:

```markdown
Anchored learning occurs when students work in teams for several hours or days trying to
solve a challenging practical problem that matters to the student. The activity is linked to
background knowledge of the learner on a topic that is interesting. The problem is challenging,
so the learner needs to engage in problem solving and recruit multiple levels of knowledge and
skills. These activities are coherently organized around solving the practical problem. Examples
of anchored learning are problem-based curricula in medical schools where students work on
genuine medical cases and communities of practice where students try to solve problems of
pollution in their city.
```

<a name="#polymathy"></a>
## Polymathy
([overview](#overview))

Here's Eric Drexler on [how to understand everything](http://metamodern.com/2009/05/17/how-to-understand-everything-and-why/) and [how to learn about everything](http://metamodern.com/2009/05/27/how-to-learn-about-everything/).

I feel a sort of kinship with what people like Drexler are trying to do here, albeit for different reasons. To oversimplify by mapping my motivations in doing the same to the ‘carrot-and-stick’ model: my carrot is this intrinsic need to ‘see the Systems of the World’ (paraphrasing Neal Stephenson, evoking the unnamed protagonist in Ted Chiang’s Understand, etc); my stick is the pain of being blindsided by unknown unknowns (so I have to at least know the outlines of everything, if not their contents, and how they all fit together). A deeper understanding than the “teacher’s password” awareness of trivia competition champions and my high school self, but not much deeper, not so deep as to sacrifice breadth.

But I digress. Eric contends that we need "knowledge of extent and structure of human knowledge on a trans-disciplinary scale":

```markdown
Formal education in science and engineering centers on teaching facts and problem-solving skills in a 
series of narrow topics. It is true that a few topics, although narrow in content, have such broad 
application that they are themselves integrative: These include (at a bare minimum) substantial chunks 
of mathematics and the basics of classical mechanics and electromagnetism, with the basics of 
thermodynamics and quantum mechanics close behind.

Most subjects in science and engineering, however, are narrower than these, and advanced education 
means deeper and narrower education. What this kind of education omits is knowledge of extent and 
structure of human knowledge on a trans-disciplinary scale. This means understanding — in a particular,
limited sense — everything.
```

How to figure out the outlines of a field, and knowledge about knowledge:

```markdown
To avoid blunders and absurdities, to recognize cross-disciplinary opportunities, and to make sense of
new ideas, requires knowledge of at least the outlines of every field that might be relevant to the 
topics of interest. By knowing the outlines of a field, I mean knowing the answers, to some reasonable
approximation, to questions like these:

What are the physical phenomena?
What causes them?
What are their magnitudes?
When might they be important?
How well are they understood?
How well can they be modeled?
What do they make possible?
What do they forbid?

And even more fundamental than these are questions of knowledge about knowledge:

What is known today?
What are the gaps in what I know?
When would I need to know more to solve a problem?
How could I find what I need?

This sort of knowledge is a kind of specialty, really — a limited slice of learning, but oriented
crosswise. Because of this orientation, though, it provides leverage in integrating knowledge from
diverse sources.
```

Why care? Problem recognition is very important:

```markdown
It takes far less knowledge to recognize a problem than to solve it, yet in key respects, that bit 
of knowledge is more important: With recognition, a problem may be avoided, or solved, or an idea 
abandoned. Without recognition, a hidden problem may invalidate the labor of an hour, or a lifetime.
Lack of a little knowledge can be a dangerous thing.
```

Eric distinguishes between learning everything (which is impossible) and learning *about* everything (which is not):

```markdown
Note that the title above isn’t “how to learn everything”, but “how to learn about everything”. The 
distinction I have in mind is between knowing the inside of a topic in deep detail — many facts and 
problem-solving skills — and knowing the structure and context of a topic: essential facts, what 
problems can be solved by the skilled, and how the topic fits with others.

This knowledge isn’t superficial in a survey-course sense: It is about both deep structure and practical
applications. Knowing about, in this sense, is crucial to understanding a new problem and what must be 
learned in more depth in order to solve it. The cross-disciplinary reach of nanotechnology almost
demands this as a condition of competence.
```

Some advice on going about it:

```markdown
To intellectually ambitious students I recommend investing a lot of time in a mode of study that may 
feel wrong. An implicit lesson of classroom education is that successful study leads to good test 
scores, but this pattern of study is radically different. It cultivates understanding of a kind that 
won’t help pass tests — the classroom kind, that is:

Read and skim journals and textbooks that (at the moment) you only half understand. Include Science 
and Nature.

Don’t halt, dig a hole, and study a particular subject as if you had to pass a test on it.

Don’t avoid a subject because it seems beyond you — instead, read other half-understandable journals 
and textbooks to absorb more vocabulary, perspective, and context, then circle back.

Notice that concepts make more sense when you revisit a topic.

Notice which topics link in all directions, and provide keys to many others. Consider taking a class.

Continue until almost everything you encounter in Science and Nature makes sense as a contribution to 
a field you know something about.
```

<a name="#polymathy-via-designing-general-programming-languages"></a>
### Polymathy via designing general programming languages
([overview](#overview))

Big-picture generalist's wet dream quote from Stephen Wolfram's [Ten thousand hours of design reviews](https://blog.stephenwolfram.com/2008/01/ten-thousand-hours-of-design-reviews/) (for more on how this is done via design reviews see [Single-handedly designing large codebases](https://github.com/monastri/monastri.github.io/edit/master/notes-software-development-computer-science-complex-systems.md#Single-handedly-designing-large-codebases)).

```markdown
It’s sometimes a little weird. One hour I’ll be intensely thinking about 
the higher mathematics of number theory functions. And the next hour I’ll
be intensely focused on how we should handle data about cities around the 
world. Or how we should set up the most general possible interfaces to 
external control devices.

But although the subject matter is very varied, the principles are at some
level the same.

I want to understand things at the most fundamental level—to see what the 
essential primitives should be. Then I want to make sure those primitives
are built so that they fit in as well as possible to the whole existing
structure of Mathematica—and so they are as easy as possible for people to
understand, and work with.

Doing design reviews and nailing down the functional design of Mathematica 
is a most satisfying intellectual activity. It’s incredibly diverse in
subject matter. And in a sense always very pure.

It’s about a huge range of fundamental ideas—and working out how to fit them
all together to create a coherent system that all makes sense.

It’s certainly as hard as anything I know about in science. But in many ways
it’s more creative. One’s not trying to decode what exists in the world. One’s
trying to create something from scratch—to build a world that one can then work
within.
```

<a name="#Semicolon-shaped-people"></a>
### Semicolon-shaped people
([overview](#overview))

From Venkat Rao's [Breaking Smart newsletter](https://us1.campaign-archive.com/?u=78cbbb7f2882629a5157fa593&id=f280d3e632).

Venkat first starts off with T-shaped people, which he'll later contrast with semicolons like himself:

```markdown
2/  Back in grad school 15 years ago, I read a book called Tomorrow's Professor
by Newport's spiritual predecessor, Richard Reis (and clearly learned nothing 
from it).

3/ In it, I encountered the idea of a T-shaped person. Reis' model was that you
ought to have a broad understanding of adjacent fields and a deep understanding
of your own to be a "good" academic.

4/ You've probably heard the phrase. T-shaped people are what  career counsellors
and HR people have in mind when they talk about "talent." Not just depth in craft,
but breadth in vision.

5/ You have your horizontal bar representing shallow generalist skills, and your 
vertical stem representing deep specialist skills.

6/ The horizontal bar represents your socialization as a knowledge worker. It 
overlaps with others' horizontal bars, enabling you to communicate across 
specialist disciplines.

7/ You can think of the T as spanning knowledge/skill based roles you could occupy. 
It's your intellectual home. T for turf. T for territory. T for textbook. 

8/ People who have a highly socialized, institutionalized, and territorial 
understanding of knowledge and work love the T-shaped-person idea. Academics 
particularly love it.

9/ In Freudian terms, T-shapes are how your superego thinks you ought to work. You
find your T, make friends and find collaborators along the bar, and dive deep along
the stem.

10/ All the while staying harmoniously connected to your intellectual community
oriented around a shared sense of "up" and directing your work "down" in the mines.  

11/ So long as you stay in your digging lane indicated by the stem, you'll also be
a good citizen of a knowledge economy, respecting others' expertise, and quietly 
proud of your own.

12/ T's stack and overlap nicely. They can be used to build stable big structures,
with good redundancy properties and natural paths of career development.

13/ In a knowledge map covered by overlapping T's, your T can grow bigger or sink
deeper. Your stem may be in your mentor's bar. Your current stem may become your 
future bar.

14/ T's induce natural authority relationships. If two people are trying to occupy
the same turf, the one for whom the position is shallower has authority (because 
they can go deeper).

15/ As in Tetris, T's are easy to work with. They are fine, upstanding citizens. 
They chair committees, organize interdisciplinary conferences, give TED talks, and
write solid books.

16/ They are nice people. To use an academic term of art, they are "collegial" types.
They can afford to be. They know where they belong in the world, and it's a good
place.

17/ Think T for Tenure. The American system of academic tenure is the perfect example
of an institution of, by, and for T-shaped people.

18/ Don't get me wrong. Unlike empty suit types, scenesters, and pretenders, T-
shaped people are people of substance. They are sincere and they do good, sometimes
great work.

19/ I have nothing but respect for T-shaped types. The backstop the reliability of 
civilizational knowledge, and are pillars of the community in an entirely positive 
sense of the term.

20/ You should WANT your Statistics 101 professors, doctors, lawyers, accountants,
bankers to be T-shaped. Trust in institutions is about the presence of T-shaped people
within them.

21/ So to summarize. T for textbook, T for tenure, T for turf, T for territory, T for
trust, T for talent. All good things. If you are T-shaped, or want to be T-shaped, 
more power to you. The world needs T's.
```

And now a taster for semicolons, illustrated with an example:

```markdown
22/ That said, I personally find T-shaped people boring, and am personally a lousy 
T. By the norms of T-shaped people, I am a could-have-been T who betrayed the values
and virtues of T-dom.

23/ Fortunately, I don't navigate by T-shaped norms. I navigate by the belief that 
there is an entire universe of deep knowing and doing that cannot be accessed by T-
shaped means.

24/ Here's an example. My buddy Kyle just released version 1.0 of a powerful React-
based open-source website building tool called Gatsby, after a year of heavy effort.

25/ It's definitely deep work, but throughout, Kyle was active on Twitter and social 
media. More importantly, the work didn't get done in a traditional institutional 
context. It was free-agent deep work.

26/ The work itself is not "T-shaped." It is a work of tech art in the rhizomatic 
frankenstack mess that is web technology.

27/ One of the interesting things about watching the project evolve was how it was 
driven by Kyle's contrarian view that the messy explosion of Javascript frameworks
and libraries is a good thing.

28/ That was his Thiel-ian "secret" -- the belief, not shared by most programmers, 
that the Javascript jungle is a good thing for the web, not a swamp to be drained. 
He vigorously defends this view online.

29/ The world of Javascript is decidedly unfriendly to systematic T-shaped approaches
to organizing, mapping, and navigating intellectual territories. T-shaped programmers
don't do well there.

30/ It is not surprising that the evolution of Javascript, unlike that of backend
computing technologies, has been strongly driven outside of academia. 

31/ It is also not surprising that you need a contrarian view of apparent messiness, 
and a willingness to ignore disciplinary maps and categories, to navigate and build 
on such rhizomatic knowledge.

32/ Javascript for example, began life as an underpowered browser language and evolved
into a powerful language for both backend and frontend systems in unexpected ways.

33/ To do deep work in the world of Javascript, you can't afford to be a T-shaped 
person. You have to be what I call a semi-colon shaped person.
```

Semicolons:

```markdown
34/ The dot of the semi-colon represents the anchor community for your deep work. 
In Gatsby's case, the world of Javascript sprawling messily across industry and 
open-source worlds.

35/ The curvy tail is the rhizomatic structure you explore to do something deep. 
It will sprawl untidily across a map built out of T's. It will offend sensibilities
and violate sacred beliefs about what goes where.

36/ The gap between the dot and the tail is what I call the Explorer's Chasm. To 
do deep work in a rhizomatic zone, you must have a "secret" separating you from 
your community.

37/ The chasm is CRUCIAL. It represents a sort of epistemic estrangement from the
nearest socialized zone of knowledge. This is necessary for work that is not just 
deep, but has truly ORIGINAL elements.

38/ The chasm also symbolizes the possibility that a work of deep knowing and doing
does not have a necessary, pre-determined and fixed connection to a locus of 
socialization that "owns" the work. 

39/ This means the knowledge could be subversive and challenge institutional 
authority. Squint a little and the tail of the semicolon might appear attached to a
different dot.

40/ And finally, a semicolon is both smaller and lower in a line of text than a T. 
I like to think this symbolizes its capacity for a) going deeper b) being efficient
at a smaller scale.
```

Contrasting T's and semicolons:

```markdown
41/ Paradoxically, because being semicolon-shaped means being less attached to a 
default social home locus, it requires you to be more agile, nomadic, alive, and 
active in relating to social context.

42/ Tenured professors with status in a discipline can tune out the world and do 
"deep work" peers recognize as "important" before it is done (with accompanying
ivory-tower/angels-on-pinhead risks).

43/ But a free agent, with no institutional safety net, no underwriting of 
exploratory expeditions by disciplinary consensus, and no research grants, cannot 
afford this luxury.

44/ To do deep work in semicolon mode, you must be plugged in, despite being 
fundamentally alone on your path, continuously renegotiating the meaning of what 
you're doing with the social context. 

45/ If T-shaped work is like climbing a sheer rock face with all kinds of safety
equipment and ropes, semicolon-shaped work is like free climbing. Riskier, but
much more rewarding if you pull it off.

46/ Almost all the people I personally find interesting, and learn a lot from,
tend to be semicolon shaped. This may be a personal preference, but I think
there's more to it.

47/ I suspect, for T-shaped people, exploration is at best an instrumental activity
that furthers their social development. For them, belongingness trumps curiosity.
That's why it can stay in its lane.

48/ But curiosity ungoverned by belongingness motives does not stay in fixed lanes,
seek permission to stray, apologize for wandering, or express contrition for 
"moving fast and breaking things."

49/ Such curiosity can be unapologetically rude, abrasive, damaging, combative, 
difficult, and intransigent when it conflicts with belongingness. It can appear to
be spoiling for a fight for no good reason.

50/ Belongingness-governed curiosity is great for stewarding existing 
institutionalized knowledge and building systematically and cautiously upon it. 
T-shaped curiosity is the bedrock of living traditionalism.

51/ You don't need formal institutions like universities for this kind of collective
knowledge environment. Even mature and developed cultural scenes have this 
characteristic. 

58/ Most T-shaped people and semicolon shaped ones probably find their own kind
interesting and admirable, and the opposite kind offensive and boring. Voice people
and exit people don't mix well.

59/ Most of the time, most people are (and should be), T-shaped. But in times of
institutional decay, renewal, and churn, you should be asking yourself: should I 
perhaps be semicolon-shaped?
```

Why are T's dominant? Venkat draws parallels to Pournelle's law; I first thought of atheism vs organized religion actually:

```markdown
53/ In a way, the dominance of T-shaped people and their modes of knowing and doing
are a consequence of Pournelle's Iron's Law of bureaucracy generalized to any social
system. 

54/ Pournelle's Law, restated for this context: *in any community, those who 
prioritize primal belongingness will eventually wrest control from those who
prioritize primal curiosity*.

55/ By contrast, semicolon shaped people and their modes of knowing and doing reflect 
a primacy of curiosity over belongingness. A willingness to sacrifice social harmony 
and relationships to the exploratory urge.

56/ This means, of course, that semicolon shaped people can be (but need not be, unless 
challenged or obstructed) socially disruptive, destabilizing, unreliable, disloyal, and
a "threat to society."

57/ But on the other hand, they also represent creative-destructive potential, and the
possibility of societies renewing themselves through the actions of those who don't 
feel strong belonging.
```

<a name="#names-matter"></a>
## Names matter
([overview](#overview))

In Michael Nielsen's essay [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html) there's a section recounting a famous story in physics I've always taken to heart, the one by Dick Feynman dismissing the value of knowing the names of things:

```markdown
One kid (a know-it-all) says to me, “See that bird? What kind of bird is that?” 

I said, “I haven't the slightest idea what kind of a bird it is.” 

He says, “It'a brown-throated thrush. Your father doesn't teach you anything!” 

But it was the opposite. He (Feynman's father) had already taught me: “See that bird?” 
he says. “It's a Spencer's warbler.” (I knew he didn't know the real name.) “Well, in 
Italian, it's a Chutto Lapittida. In Portuguese, it's a Bom da Peida… You can know the name 
of that bird in all the languages of the world, but when you're finished, you'll know absolutely
nothing whatever about the bird! You'll only know about humans in different places, and what they
call the bird. So let's look at the bird and see what it's *doing* — that's what counts.” (I
learned very early the difference between knowing the name of something and knowing something.)
```

Dick might've also been influenced by the following sentiment, expressed in *Computers From The Inside Out*:

```markdown
One of the miseries of life is that everybody names things a little bit wrong.
```

Roger Zelazny puts it far more poetically in his novel *Lords of Light* in one of the most powerful passages I've ever read:

```markdown
Sam sat with his eyes closed for several minutes, then said softly:

"I have many names, and none of them matter." He opened his eyes slightly then, but he did not 
move his head. He looked upon nothing in particular.

"Names are not important," he said. "To speak is to name names, but to speak is not important.

"A thing happens once that has never happened before. Seeing it, a man looks on reality. He cannot
tell others what he has seen. Others wish to know, however, so they question him saying, 'What is 
it like, this thing you have seen?'

"So he tries to tell them. Perhaps he has seen the very first fire in the world. He tells them, 
'It is red, like a poppy, but through it dance other colors. It has no form, like water, flowing 
everywhere. It is warm, like the sun of summer, only warmer. It exists for a time on a piece of wood,
and then the wood is gone, as though it were eaten, leaving behind that which is black and can be 
sifted like sand. When the wood is gone, it too is gone.'

"Therefore, the hearers must think reality is like a poppy, like water, like the sun, like that which
eats and excretes. They think it is like to anything that they are told it is like by the man who has 
known it. But they have not looked upon fire. They cannot really *know* it. They can only know *of* it.

"But fire comes again into the world, many times. More men look upon fire. After a time, fire is 
as common as grass and clouds and the air they breathe. They see that, while it is like a poppy, 
it is not a poppy, while it is like water, it is not water, while it is like the sun, it is not 
the sun, and while it is like that which eats and passes wastes, it is not that which eats and
passes wastes, but something different from each of these apart or all of these together. So they
look upon this new thing and they make a new word to call it. They call it 'fire.'

"If they come upon one who still has not seen it and they speak to him of fire, he does not know 
what they mean. So they, in turn, fall back upon telling him what fire is like. As they do, they 
know from their own experience that what they are telling him is not the truth, but only a part of it.
They know that this man will never know reality from their words, though all the words in the world 
are theirs to use. He must look upon the fire, smell of it, warm his hands by it, stare into its heart,
or remain forever ignorant.

"Therefore, 'fire' does not matter, 'earth' and 'air' and 'water' do not matter. 'I' do not matter.
No word matters.

"But man forgets reality and remembers words. The more words he remembers, the cleverer do his fellows
esteem him. He looks upon the great transformations of the world, but he does not see them as they were
seen when man looked upon reality for the first time. Their names come to his lips and he smiles as he 
tastes them, thinking he knows them in the naming. The thing that has never happened before is still 
happening. It is still a miracle. The great burning blossom squats, flowing, upon the limb of the world,
excreting the ash of the world, and being none of these things I have named and at the same time 
all of them, and *this* is reality—the Nameless.”
```

Nielsen pushes back against this attitude insofar as it goes too far:

```markdown
It's a good story. But it goes too far: names do matter. Maybe not as much as the know-it-all kid
thought, and they're not usually a deep kind of knowledge. But they're the foundation that allows 
you to build up a network of knowledge.

This trope that names don't matter was repeatedly drilled into me during my scientific training.
When I began using Anki, at first I felt somewhat silly putting questions about names for things 
into the system. But now I do it enthusiastically, knowing that it's an early step along the way 
to understanding.

Anki is useful for names of all kinds of things, but I find it particularly helpful for non-verbal
things. For instance, I put in questions about artworks, like: “What does the artist Emily Hare's 
painting Howl look like?”.

I put that question in for two reasons. The main reason is that I like to remember the experience of
the painting from time to time. And the other is to put a name to the painting.( Actually, a better
question for that is to be shown the painting and asked what its name is.) If I wanted to think more 
analytically about the painting – say, about the clever use of color gradients – I could add more
detailed questions. But I'm pretty happy just committing the experience of the image to memory.

Friends sometimes complain that many books are over-padded essays. Perhaps a benefit of such padding
is that it enforces an Anki-like spaced repetition, since readers take weeks to read the book. This
may be an inefficient way to memorize the main points, but is better than having no memory of the 
book at all.
```

Names are hard to get right too. Carlos Bueno, [The Mature Optimization Handbook](http://carlos.bueno.org/optimization/mature-optimization.pdf):

```markdown
Naming things has been half-jokingly called the second-hardest
problem in computer science. Anyone can name the things they
build anything they want, and they do. That’s the problem. The
computer doesn’t care about names. They’re for the benefit of
humans so there are no technical arguments to fall back on.
Excess jargon is the sawdust of new technology, and the mental
friction it imposes is scandalous. Whoever figures out how to
sweep it up does a service to mankind.

Take the word we’ve been using for intervals of real time,
“walltime”. Perhaps it’s more properly called “duration”. Time
spent on the CPU could be called “cpu_duration”; time spent
waiting for the database “db_duration” and so on. And why
not be explicit about the units, eg “duration_cpu_usec”? If you
have a strong preference either way, I humbly suggest that it’s a
matter of taste and not universal truth. Walltime sounds more
natural to me because that was the jargon I was first exposed
to. But who actually has clocks on their walls any more? The
term is as dated as “dialing” a phone number.

For that matter, take instructions. Now that we’ve decided
to round to the nearest thousand, is the name “instructions”
misleading? Is “kilo_instructions” too cumbersome to type? Is
“kinst” too obscure to remember?

This all might sound mincing and pedantic, but a) you have
to pick something and b) you’ll have to deal with your choices
for a long time. So will the people who come after you. You
can’t clean up the world but you can mind your own patch.

Even an ugly scheme, if it’s consistently ugly, is better than
having to remember that walltime here is duration over there
and response_time_usec somewhere else. Whatever ontology
you build, *write it down* somewhere it will be noticed. Explain
what the words mean, the units they describe, and be firm about
consistency
```

<a name="#Concept-handles"></a>
### Concept handles
([overview](#overview))

One way how names matter is in providing concept handles. 

The term comes from Scott Alexander in [Nonfiction writing advice](https://slatestarcodex.com/2016/02/20/writing-advice/):

```markdown
**Use strong concept handles**

The idea of concept-handles is itself a concept-handle; it means a catchy phrase
that sums up a complex topic.

Eliezer Yudkowsky is *really good* at this. “belief in belief“, “semantic stopsigns“,
“applause lights“, “Pascal’s mugging“, “adaptation-executors vs. fitness-maximizers“,
“reversed stupidity vs. intelligence“, “joy in the merely real” – all of these are
interesting ideas, but more important they’re interesting ideas with short catchy 
names that everybody knows, so we can talk about them easily.

I have very consciously tried to emulate that when talking about ideas like trivial
inconveniences, meta-contrarianism, toxoplasma, and Moloch.

I would go even further and say that this is one of the most important things a blog
like this can do. I’m not too likely to discover some entirely new social phenomenon
that nobody’s ever thought about before. But there are a lot of things people have 
vague nebulous ideas about that they can’t quite put into words. Changing those into
crystal-clear ideas they can manipulate and discuss with others is a big deal.

If you figure out something interesting and very briefly cram it into somebody else’s
head, don’t waste that! Give it a nice concept-handle so that they’ll remember it and
be able to use it to solve other problems!
```

Eric Raymond, [rule-swarm attacks](http://esr.ibiblio.org/?p=8153):

```markdown
It not news to readers of this blog that I like to find common tactics and traps 
in programming that don’t have names and name them. I don’t only do this because 
it’s fun. When you have named a thing you give your brain permission to reason 
about it as a conceptual unit. Bad jargon obfuscates, map hiding territory; good 
jargon reveals, aiding reflection on and and improvement of your practice.
```

Pushback against concept handles from David Barry for two reasons -- high barrier to entry for new readers when enough jargon has accumulated, and risk of wrong concepts "sticking" encouraging use of faulty ideas:

```markdown
I’m not a huge fan of the concept handles though. Of Eliezer’s “short catchy names 
that everybody knows”, I’m only confident that I could define one of them, and have
a decent guess at a couple of others. Building up such a jargon can improve 
communication efficiency within your blog reader community, but it puts up a larger
barrier to entry than perhaps there ought to be.

I also think that if a concept is wrong, then turning it into a handle encourages
overuse of a faulty idea. I think your “tolerate the outgroup” post was wonderful 
and describes a real dynamic that I can feel in my own reactions to events and which
I can now more easily recognise in others. But your “staying classy” post looks much
weaker to me, and I expect that better treatments exist somewhere in an academic 
literature that I’m unfamiliar with. Over time, as you continue to build up an
edifice of related ideas, you’ll sometimes link to the outgroup post and sometimes 
link to the classy post, and as a result I think some pieces of your arguments will
be resting on solid foundations and some will be very shaky. Those shaky bits don’t
deserve the apparent authority of a blue and underlined word or two.
```

<a name="#Naming-functions-in-large-codebases"></a>
### Naming functions in large codebases
([overview](#overview)) 

A real-world still-in-action example of names mattering, from Stephen Wolfram's [Ten thousand hours of design reviews](https://blog.stephenwolfram.com/2008/01/ten-thousand-hours-of-design-reviews/) (for more on how this is done via design reviews see [Single-handedly designing large codebases](https://github.com/monastri/monastri.github.io/edit/master/notes-software-development-computer-science-complex-systems.md#Single-handedly-designing-large-codebases)).

The importance of good naming:

```markdown
One of the things that happens in design reviews is that we finalize the 
names for functions.

Naming is a quintessential design review process. It involves drilling down
to understand with as much as clarity as possible what a function really 
does, and is really about. And then finding the perfect word or two that 
captures the essence of it.

The name has to be something that’s familiar enough to people who should be
using the function that they’ll immediately have an idea of what the function
does. But that’s general enough that it won’t restrict what people will think
of doing with the function.

Somehow the very texture of the name also has to communicate something about
how broad the function is supposed to be. If it’s fairly specialized, it 
should have a specialized-sounding name. If it’s very broad, then it can 
have a much simpler name—often a much more common English word.
```

Expanding upon the latter points a bit more -- make every function as general as possible, and recognize that function names determine how people think about them, right down to the "texture" of the names:

```markdown
In building Mathematica, we’ve had the longstanding principle of always 
trying to make every function as general as possible—so that it is applicable
to as wide a range of situations as possible. Sometimes, though, a function 
will have one particular, familiar, use. But if the name of the function 
reflects only that use, one is shortchanging the function. For without a more
general name, people will never think to apply it in other cases. (So, for 
example, it’s List, not “vector”, and it’s Outer, not “outer product”.)

And indeed, one of the responsibilities of function naming is that it is the
names of functions that to a large extent directly determine how people will
think about a function. If they are led in a particular direction by the name,
that will be the direction in which they will go in using the function.

And even the very “texture” of the name is important in getting people to think
correctly about functions. A sophisticated function should have a sophisticated
name (like DynamicModule or EventHandler).  A straightforward, common, function
should have a simple name (like Length or Total). A function that does a clear 
but unusual thing should have an unexpected name (like Thread or Through).
```

Test and criteria for good naming:

```markdown
I always have a test for candidate names. If I imagine making up a sentence 
that explains what the function does, will the proposed name be something that
fits into that sentence? Or will one end up always saying that the function 
with name X does something that is described as Y?

Sometimes it takes us days to come up with the right name for a function. But
usually one knows when it’s right. It somehow just fits. And one can
immediately remember it.

I take great pains to name every new function in the best possible way.

First, one must leverage on peoples’ existing knowledge and understanding. If 
there is a familiar name that’s already widely used, then if at all possible
one must use it.

Of course, sometimes that name may only be familiar in some particular area. 
And it may be very short—perhaps a single letter—and incomprehensible without 
further context. And in that case, what we typically do in Mathematica is to 
burn into the name some kind of stylized context. (So, for example, the Fresnel
integral S(x) has the name FresnelS in Mathematica.)

When one finds a good name for a function, one of the things that happens is
that when people hear the name, they can successfully “unpack” it into a one-
sentence description of what the function must do—often in effect just by using 
the name of the function as the main part of a sentence. And indeed, when we’re
stuck in trying to find a good name for a function, I’ll often suggest that we
try to write a sentence that describes what the function does—that we can
perhaps use in the Documentation Center for the function, but then condense 
down into the nugget we need for the name itself.
```

Conventions develop:

```markdown
There are definite conventions about what particular kinds of names mean.
(Functions that end in List generate lists; functions that begin with Image
operate on images; functions that begin with Find involve some kind of 
searching; and so on.) There are ways that names tend to appear together in
typical usage of the language. And there are definite conceptual frameworks—and 
metaphors—that have developed in the language and the system. (Nest refers to 
repeated function application; Flat refers to flattening of nested structures;
Dynamic refers to dynamic interactivity; and so on.)
```

Precedents (from previous codebase versions) matter:

```markdown
By now in Mathematica there are a great many precedents for how functions should
be named. And we always try to follow these precedents whenever possible. First,
because they often represent good solutions to the naming problems we’re now
trying to solve. And second, because by following them one is maintaining a 
certain consistency that makes it easier for the system to grow, and for people 
to learn the system—and to guess about functionality they do not already know.
```

Names carve reality at the joints. It is impossible to carve perfectly:

```markdown
One of the painful aspects of function naming is that however clever you are about
it, it can never be perfect. I often claim that the only language that is perfectly
consistent is the one that does nothing. As soon as there is actual functionality 
to represent, there are inevitably awkward cases and corners. For example, one 
wants to maintain consistent simplicity in naming in each area of the system. But 
then at the overlaps between these areas there are inconsistencies.
```


Contrasting names and word evolution in natural human languages and function names in codebases:

```markdown
In ordinary human languages, new words typically develop by some form of natural
selection. Usually a word will be introduced—perhaps at first as a phrase—by 
one person. And then it spreads, sometimes changing a bit, and either becomes
popular enough to be widely understood and useful for general communication, or
disappears.

But for a computer language the pattern is necessarily different. For once a 
function name—that corresponds to a “word” in the language—has been introduced, 
it must immediately be a full, permanent, element of the language. For programs
will be written that contain that name, and they would all have to be found and
updated if that name was changed.

There is also another difference between words in human languages and function 
names in a computer language. In a human language, there is no ultimate,
absolute, meaning defined for most words. Instead, the best we can do is—like
in a dictionary—to define words by relating them to other words.

But in a computer language, each function name ultimately refers to a particular
piece of functionality that is defined in an absolute way, and can be implemented
by a specific precise program.

This doesn’t usually make it any easier to come up with function names, though.
It just means that there’s a clearer notion of the “right name”: the name where
a human has the best chance of correctly figuring out from it what the function
does.

Function names are in a sense ultimate points of human-machine communication. 
They’re the places where all that internal computational activity has to be 
connected with something that humans can understand. When the functionality is
simple there are pictorial and other alternatives. But when the functionality 
is diverse or sophisticated we don’t know any possibility other than to use 
language—and the linguistic construct of names for things.

The function names in Mathematica are ultimately based on English, and for the
most part, they consist of ordinary English words. In ordinary natural human 
languages, it is possible to introduce a completely new word, and have it
gradually gain popularity and understanding. But in the dynamics of computer 
languages—with their necessarily sudden introduction of new names—one has no 
choice but to leverage on people’s existing understanding of a human language,
like English.

Still, when we come up with function names in Mathematica today, they are in a
sense not based just on “raw English”. They also rely on the web of meaning 
that has developed through the several thousand other functions that already 
exist in Mathematica.
```

The problem with basing names on human language is that sometimes the latter is limited -- hence analogize:

```markdown
And sometimes one runs into limitations of English: there just isn’t any 
familiar word or phrase for a concept, perhaps because that concept is 
somehow new to our experience. And in such cases what one typically has to 
do—just like in natural language—is to come up with an analogy.

Some of the analogies and metaphors we consider start quite wild and 
outlandish. But eventually they become tamer—like Sow and Reap or Throw and
Catch—and an important way to extend the linguistic base for names in
Mathematica.
```

Another problem is ambiguity in human languages, which needs resolving:

```markdown
It might be nice if English—like Mathematica—had the feature that a particular
word meant only a particular thing, or at least a class of things. But that is
not how it works. A single word can act as different parts of speech, and can
have wildly different meanings. Usually in actual English usage, one can 
disambiguate by context.

But in the tiny length of a single function name, one does not have that option.
And quite often that means one has to reject some wonderful word just in order 
to avoid a possible misunderstanding from a different way it can be used in 
English. (So, for example, “Live” or “Active” can’t be candidates for Dynamic—
they’re just too easy to misunderstand.)

If one is lucky, a thesaurus (these days in Wolfram|Alpha) will give one a word 
that captures the same concept but avoids the potential misunderstanding. But 
sometimes one has to rearrange the whole structure of the name to avoid the 
possibility of misunderstanding.
```

Example of function naming:

```markdown
In Mathematica 6, a typical case of function naming was Manipulate.

It took quite a while to come up with that name.

We created this great function. But what should it be called? Interface? 
Activate? Dynamic? Live?
What?

Interface might seem good, because, after all, it creates an interface. But 
it’s a particular kind of interface, not a generic one.

Activate might be good, because it makes things active. But again it’s too
generic.

Dynamic: again it sounds too general, and also a bit too technical. And 
anyway we wanted to use that name for something else.

Live… that’s a very confusing word. It’s even hard to parse when one reads it.
Does it say “make it alive”, or “here’s something that is alive”, or what?

Well, after a while one realizes that one has to understand with more clarity
just what it is that this great new function is doing.

Yes, it’s creating an interface. Yes, it’s making things active, dynamic,
alive. But really, first and foremost, what it’s doing is to provide a way to 
control something. It’s attaching knobs and switches and so on to let one 
control almost anything.

So what about a word like Control? Again, very hard to understand. Is the thing
itself a control? Or is it exerting control?

Handle? Again, too hard to understand.

Harness? A little better. But again, some ambiguity. And definitely too much of
a “horse” motif.

Yoke? That one survived for several days. But finally the oxen jokes overwhelmed
it.

And then came Manipulate.

At first, it was, “Oh, that’s too long a word for such a great and important 
function.”

But in my experience it often “feels right” to have a fairly long word for a 
function that does so much. Of course there were jokes about it sounding 
“manipulative”.

But as we went on talking about the function, we started just calling it Manipulate
among ourselves. And everyone who joined the conversation just knew what it meant.
And as we went on developing all its detailed capabilities, it still seemed to fit.
It gave the right sense of controlling something, and making something happen.

So that’s how Manipulate got its name. It’s worked well.
```

The general experience of naming functions:

```markdown
n developing Mathematica 6, we had to name nearly 1000 functions. And each name has
to last—just as the names in Mathematica 1 have lasted.

Occasionally it was fairly obvious what a function should be called.

Perhaps it had some standard name, say in mathematics or computing, such as Norm or
StringSplit.

Perhaps it fit into some existing family of names, like ContourPlot3D.

But most of the time, each name took lots and lots of work to invent. Each one is
sort of a minimal expression of a concept that a primitive in Mathematica implements.

Unlike human languages that grow and mutate over time, Mathematica has to be defined
once and for all. So that it can be implemented, and so that both the computers and 
the people who use it can know what everything in it means.

As the Mathematica system has grown, it’s in some ways become more and more difficult
to do the design. Because every new thing that’s added has to fit in with more and 
more that’s already there.

But in some ways it’s also become easier. Because there are more precedents to draw
on. But most importantly, because we’ve gotten (and I think I personally have 
gotten) better and better at doing the design.

It’s not so much that the quality of the results has changed. It’s more that we’ve
gotten faster and faster at solving design problems.

There are problems that come up today that I can solve in a few minutes—yet I 
remember twenty years ago it taking hours to solve similar problems.

Over the years, there’ve been quite a few “old chestnuts”: design problems that we 
just couldn’t crack. Places where we just couldn’t see a clean way to add some 
particular kind of functionality to Mathematica.

But as we’ve gotten better and better at design, we’ve been solving more and more 
of these. Dynamic interactivity was one big example. And in fact Mathematica 6 has
a remarkable number of them solved.
```

Wolfram expands on this a bit more in a slighty later essay, [the poetry of function naming](https://blog.stephenwolfram.com/2010/10/the-poetry-of-function-naming/):

```markdown
The naming of functions is a strange and difficult art—a bit like an ultimately
abstracted form of poetry. The goal is to take the concept and functionality
of a function, and capture the essence of it in one, or two, or perhaps three,
words (like Riffle, or DeleteCases, or FixedPointList)—chosen so that when
someone sees those words, they immediately get the right idea about the
function. In even the most succinct forms of ordinary poetry, you get at least
a handful of words to communicate with. In function names, you typically get 
at most perhaps three.

With enough experience, it can sometimes be pretty easy to come up with that 
little gem of a name for a function. Sometimes it can even seem quite obvious
as soon as one thinks about the function. But sometimes it can take immense 
amounts of time—wrestling with what can seem like an insoluble problem of 
packing everything one needs to say about a function into that one little name.

It’s an unforgiving and humbling activity. And the issue is almost always the 
same. The reason you can’t find a good name is because you don’t really 
understand with complete and ultimate clarity what the function does.

And sometimes that’s because the function really isn’t designed quite right. 
There’s something muddled about it, that has to be unmuddled before you’ll ever
be able to find a good name.

It’s very satisfying, though, when you finally crack it. These days I’m usually 
working on design reviews with teams of people. And when we finally get the 
right name, everyone on the call (yes, it’s essentially always a phone call) 
immediately says “Oh yes, that’s it”. And we all feel a little stupid that we 
just spent an hour, or however long, just coming up with one or two words.
```

<a name="#cognitive-science"></a>
## Cognitive science
([overview](#overview))

From Cosma Shalizi's [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*:

```markdown
Cognition, whether human, animal or artificial, is a kind of information-processing, taking place, in our
case, in the brain. The information takes the form of representations (of sensory stimuli, of states of 
parts of the world, of facts, of relations, of possible states of parts of the world, of courses of action,
or what-not). The processing consists of the transformation of these representations according to definite,
though perhaps stochastic, rules. (So far, we have not excluded the connectionist heretics.) An immense amount
of information-processing takes place subconsciously, particularly that which turns raw irritation of the 
afferent nerves into useful perceptions of the world about us, and turns volitions into raw stimulations of
the efferent nerves. To recognize a dagger you see before you involves a lot of computational work; some people,
having been wounded in the parts of the brain which do the computations, cannot. At least at some level of 
abstraction, the representations and transformations are usefully, conveniently and/or accurately thought of as
structures of symbols and as algorithms, respectively. (This does rule out the connectionists.) The algorithms 
may be (or, if you like, instantiate) rules of inference, or rules for producing new representations from old 
ones more generally ("production systems"). 

One particularly well-studied kind of cognition, sometimes taken as the paradigm of all cognition, is
problem-solving, conceived of as turning a representation of the problem, step by step, into a representation of
a solution, or something close enough to a solution to satisfy the problem-solver. (Expertise in solving a kind 
of problem consists in knowing good algorithms to apply to it, being able to represent a problem in a way which
makes it easy to solve, and being able to recognize a solution when you have one.) In principle, all this takes 
place in the brain; in practice, we can fake a larger and more accurate memory than we possess by either using 
external symbols, or by taking advantage of regular and persistent parts of our environment.
```

<a name="#external-brain"></a>
## External brain 
([overview](#overview))

Alfred North Whitehead, *Symbolism: Its Meaning And Effect*:

```markdown
“Civilization advances by extending the number of
important operations which we can perform without thinking about them.”
```

Aids to memory have been opposed for millennia. Here's Socrates, in Plato's Phaedrus, circa 370 BCE, bemoaning the deleterious effects of the new technology of "writing":

```markdown
If men learn this, it will implant forgetfulness in their souls; they will cease to exercise
memory because they rely on that which is written, calling things to remembrance no longer 
from within themselves, but by means of external marks. What you have discovered is a recipe 
not for memory, but for reminder. And it is no true wisdom that you offer your disciples, but
only its semblance, for by telling them of many things without teaching them you will make 
them seem to know much, while for the most part they know nothing, and as men filled, not with
wisdom, but with the conceit of wisdom, they will be a burden to their fellows.
```

Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally) and the comments are great. It's from 2009; a decade hence, they're more applicable than ever. Here's some neat quotes.

```markdown
How many of the things you "know" do you have memorized?

Do you remember how to spell all of those words you let the spellcheck catch?  Do you remember
what fraction of a teaspoon of salt goes into that one recipe, or would you look at the list of
ingredients to be sure?  Do you remember what kinds of plastic they recycle in your neighborhood,
or do you delegate that task to a list attached with a magnet to the fridge?

If I asked you what day of the month it is today, would you know, or would you look at your
watch/computer clock/the posting date of this post?

Before I lost my Palm Pilot, I called it my "external brain".  It didn't really fit the description; 
with no Internet access, it mostly held my contact list, class schedule, and grocery list.  And a 
knockoff of Minesweeper.  Still, in a real enough sense, it remembered things for me.The vast arena 
of knowledge at our fingertips in the era of constant computing has, ironically, brought it farther
away.  It seems nearer: after all, now, if you are curious about Zanzibar, Wikipedia is a few 
keystrokes away.  Before the Internet, you'd probably have been looking at a trip to the library and
a while wrestling with the card catalog; and that would be if you lived in an affluent, literate society. 
If you didn't, good luck knowing Zanzibar exists in the first place!

But if you were an illiterate random peasant farmer in some historical venue, and you needed to know
the growing season of taro or barley or insert-your-favorite-staple-crop-here, Wikipedia would have
been superfluous: you would already know it.  It would be unlikely that you would find a song lyrics
website of any use, because all of the songs you'd care about would be ones you really knew, in the
sense of having heard them sung by real people who could clarify the words on request, as opposed to
the "I think I heard half of this on the radio at the dentist's office last month" sense.
```

Per Kaj Sotala, the distributed cognition paradigm of research is all about exploring the idea behind "externalizing" or "outsourcing" our brains to the environment. An excerpt from [this primer](http://www.isr.uci.edu/~jpd/classes/ics234bs03/13-HollanEtAl-TOCHI.pdf):

```markdown
In several environments we found subjects using space to simplify choice by creating arrangements 
that served as heuristic cues. For instance, we saw them covering things, such as garbage disposal 
units or hot handles, thereby hiding certain affordances or signaling a warning and so constraining
what would be seen as feasible. At other times they would highlight affordances by putting items 
needing immediate attention near to them, or creating piles that had to be dealt with. We saw them lay
down items for assembly in a way that was unambiguously encoding the order in which they were to be 
put together or handed off. That is, they were using space to encode ordering information and so were 
off-loading memory. These are just a few of the techniques we saw them use to make their dedecision
problems combinatorially less complex.

We also found subjects reorganizing their workspace to facilitate perception: to make it possible to 
notice properties or categories that were not noticed before, to make it easier to find relevant items,
to make it easier for the visual system to track items. One subject explained how his father taught him 
to place the various pieces of his dismantled bicycle, many of which were small, on a sheet of newspaper.
This made the small pieces easier to locate and less likely to be kicked about. In videos of cooking we 
found chefs distinguishing otherwise identical spoons by placing them beside key ingredients or on the 
lids of their respective saucepans, thereby using their positions to differentiate or mark them. We found
jigsaw puzzlers grouping similar pieces together, thereby exploiting the capacity of the visual system to
note finer differences between pieces when surrounded by similar pieces than when surrounded by different
pieces.

Finally, we found a host of ways that embodied agents enlist the world to perform computation for them. 
Familiar examples of such off-loading show up in analog computations. When the tallest spaghetti noodle
is singled out from its neighbors by striking the bundle on a table, a sort computation is performed by 
using the material and spatial properties of the world. But more prosaically we have found in laboratory 
studies of the computer game Tetris that players physically manipulate forms to save themselves 
computational effort [Kirsh 2001; Kirsh and Maglio 1995]. They modify the environment to cue recall, to 
speed up identification, and to generate mental images faster than they could if unaided. In short, they 
make changes to the world to save themselves costly and potentially error-prone computations.
```

Sotala takes the distributed cognition idea-seed and runs away with it:

```markdown
Information processing doesn't only happen inside brains and computers. The paradigm of distributed 
cognition studies human societies as information-processing systems, with individuals being parts of
the larger system. For instance, the operation of an airliner cockpit's crew has been studied from 
this perpective [1]. For a flight to proceed without trouble, the different crew members need to be 
aware of information relating to their areas of responsibility at any given moment. If the crew is 
experienced and well trained, they'll constantly stay up to date by e.g. simply listening to other 
crew members converse with flight control. As flight control informs the captain of a new flight 
altitude, the rest of the pilots begin to adjust the altitude even while the captain is still
finishing up the communication. The cockpit functions as a unified system, and relevant information 
is propagated to wherever needed. Several crew members hearing the same information also allows for
error correction. If the message is unclear and the captain can't make out flight control's words, 
he can ask the others for clarification. The co-pilot answers the captain's query: even though one 
part of the system has failed to absorb the information received from outside the system, the same 
information has been stored in another part, which may then attempt to re-send it where needed.

Several other fields have been studied in the same manner, ranging from a child's language learning 
[2] to creativity [3]. A child doesn't learn language by itself and in a vacuum, but via interaction
with adults and older children. Creativity, on the other hand, requires common, shared "idea resources"
which individuals may use to come up with their own inventions and then give them back for others to 
refine further. Another theory of innovation considers inventions to be responses to problems encountered
by the community. Things such as bad laws or ineffective ways of doing things show up in community, and 
are considered problems by its members. This leads the community - the system - into a need state,
mobilizing its members to seek solutions until they're found.

One central idea is that social communities are cognitive architectures the same way that individual
minds are [4]. The argument is as follows. Cognitive processes involve trajectories of information 
(transmission and transformation), so the patterns of these information trajectories, if stable,
reflect some underlying cognitive architecture. Since social organization - plus the structure added
by the context of activity - largely determines the way information flows through a group, social 
organization may itself be viewed as a form of cognitive architecture.

[1] Hutchins, E. & Klausen, T. (1995) Distributed Cognition in an Airline Cockpit.

[2] Spurrett, D. & Cowley, S.J. (2004) How to do things without words: infants, utterance-activity and
distributed cognition. Language Sciences, 6, 443-466.

[3] Miettinen, R. (2006) The Sources of Novelty: A Cultural and Systemic View of Distributed Creativity.
Creativity and Innovation Management. Vol. 15, no. 2.

[4] Hollan, J. & Hutchins, E. & Kirsh, D. (2000) Distributed Cognition: Toward a New Foundation for 
Human-Computer Interaction Research. ACM Transactions on Computer-Human Interaction. Vol 7, no. 2.
```

In other words, says Sotala, "probably nothing to be worried about. Just normal human use of the environment."

From Andy Clark's book *Supersizing the Mind*, a [comment by trent](https://nforum.ncatlab.org/discussion/1927/unpopularity-of-category-theory/) I found on the nForum showing how Feynman understood the notion of the external brain:

```markdown
When historian Charles Weiner found pages of Nobel Prize-winning physicist Richard Feynman’s
notes, he saw it as a “record” of Feynman’s work. Feynman himself, however, insisted that the
notes were not a record but the work itself. In Supersizing the Mind, Andy Clark argues that 
our thinking doesn’t happen only in our heads but that “certain forms of human cognizing include
inextricable tangles of feedback, feed-forward and feed-around loops: loops that promiscuously 
criss-cross the boundaries of brain, body and world.” The pen and paper of Feynman’s thought
are just such feedback loops, physical machinery that shape the flow of thought and enlarge the
boundaries of mind.
```

For context, this whole thread was about trying to figure out why category theory was unpopular. Trent adds:

```markdown
I see theory as helping one adopt elegant solutions like that, and, more generally I think that
the more physicists understand the role things seemingly outside of the mind such as notation
play in cognition, the more they will see the importance of work in mathematical physics which 
places physics in the most elegant possible notation. It’s not just theory addicts trying to 
justify their work when they say that it aids in problem solving, it’s how cognition works.
```

<a name="#[Personal-knowledge-management"></a>
### Personal knowledge management
([overview](#overview))

From Carol Hixson and Jason Frand (of UCLA! Anderson School of Management! Woot woot)'s [eponymous working paper](https://scholarsbank.uoregon.edu/xmlui/handle/1794/24358) (here slide version).

What is personal knowledge management, or PKM? 

```markdown
A conceptual framework to organize and integrate information that we, as individuals,
feel is important so that it becomes part of our personal knowledge base

A strategy for transforming what might be random pieces of information into something
that is more systematic and expands our personal knowledge 
```

Why?

```markdown
**Information explosion**
More than 30,000 new journals each year, more than 1000 new WWW sites each day
Makes keeping track of information difficult
Volume of information in the world degrades value due to redundancy and noise

**Information chaos**
Analogy -- if the WWW were compared to a library:
- "books" on its shelves would keep changing their relative locations as well as
their sizes and names
- Individual "pages" in those publications would be shuffled ceaselessly
- Much of the data on those pages would be revised, updated, extended, shortened 
or even deleted without warning almost daily
```

And then there's also the shift in who's responsible for knowledge management as the bulk of the world's information transitions from traditional media to the web:

| Attribute | Traditional | Web |
| --- | --- | --- |
| Cost of production | High | Low |
| Cost of updating | Very high | Relatively low |
| Cycle time | Years | Hours |
| Distribution | Physical | Electronic |
| Number of producers | Controlled | Unlimited |
| Editorial review | Prior to publication | Essentially none |
| Content evaluation | By professional | By users |

When?

```markdown
Must become part of routine and used whenever working with information and knowledge:
- creating
- acquiring
- evaluating/assessing
- organizing/storing
- cataloging/classifying/indexing
- retrieving from personal memory (your mind or your hard disk) 
```

Where? 

```markdown
One schema for all:
- Paper documents
- Electronic documents
- Web bookmarks
- Personal home library
```

How?

```markdown
Create an organizational structure which facilitates your finding and relating 
personal and professional information 

Use technology as an organic tool, an extension of your own memory, enhancing
natural abilities, skills, and talents for synthesis and processing of ideas 
for more effective problem solving and decision making

Use the hard disk of your computer as a tool for initiating these processes 
```

Three basic approaches to organizing knwoledge: when, what, how. Pros and cons:

- When: easy to setup and maintain, but hard to search info (must think in terms of *when* get info, not what's *needed*)
- What: easier to search, but gets harder to create/maintain categories and concepts can cross boundaries
- How: easiest to search (because look for info in terms of context in which you need it), but hardest to figure out ontology and it changes

<a name="#Offload-thinking-to-second-brain"></a>
### Offload thinking to second brain
([overview](#overview))

A great resource is Tiago Forte's [second brain](https://praxis.fortelabs.co/basboverview/) essay. 

Key observation:

```markdown
We spend countless hours every year reading, listening, and watching informational
content. And yet, where has all that valuable knowledge gone? Where is it when we 
need it? Our brain can only store a few thoughts at any one time. 

Our brain is for *having* ideas, not storing them.
```

What I want, which he teaches (although I'm not willing to sign up for the expensive-ass course), is "an external, centralized, digital repository" to "save and systematically remind myself of the ideas, insights and connections gained through experience". 

Tiago continues:

```markdown
Being effective in the world today requires managing many different kinds of 
information – emails, text messages, messaging apps, online articles, books, 
podcasts, webinars, memos, and many others. All of these kinds of content have
value, but trying to remember all of it is overwhelming and impractical. By 
consolidating ideas from these sources, you’ll develop a valuable body of work
to advance your projects and goals. ...

We are already doing most of the work required to consume this content. We spend
a significant portion of our careers creating snippets of text, outlines, photos,
videos, sketches, diagrams, webpages, notes, or documents. Yet without a little
extra care to preserve these valuable resources, our precious knowledge remains 
siloed and scattered across dozens of different locations. We fail to build a 
collection of knowledge that both appreciates in value and can be reused again 
and again.

By offloading our thinking onto a “second brain,” we free our biological brain to
imagine, create, and simply be present. We can move through life confident that we
will remember everything that matters, instead of floundering through our days 
struggling to keep track of every detail.

Your second brain will serve as an extension of your mind, not only protecting you
from the ravages of forgetfulness but also amplifying your efforts as you take on 
creative challenges.
```

Okay, so how to build a second brain using digital notes? Capture ideas/insights worth saving in a single centralized repo:

```markdown
Ask yourself:

- What are the recurring themes and questions that I always seem to return to in 
my work and life?
- What insightful, high-value, impactful information do I already have access to
that could be valuable?
- Which knowledge do I want to interconnect, mix and match, and periodically 
resurface to stimulate future thinking on these subjects?

Most of the time we tend to capture information haphazardly – we email ourselves
a quick note, brainstorm some ideas in a Word document, or take notes on books we
read – but then don’t do anything with it. We are already consuming or producing 
this information, we just need to keep it in a single, centralized place, such as
a digital note-taking app like Evernote, Microsoft OneNote, Bear, Notion, or others.
These apps facilitate capturing small “snippets” of text, and can also store
hyperlinks, images, webpages, screenshots, PDFs, and other attachments, all of which
are saved permanently and synced across all your devices.

By keeping a diverse collection of information in one centralized place, it is free
to intermix and intermingle, helping us see unexpected connections and patterns in 
our thinking. This also gives us one place to look when we need creative raw
material, supporting research, or a shot of inspiration.
```

Guidelines for capturing only relevant info:

```markdown
**Think like a curator**
It is tempting to turn on our mobile device or computer and immediately become 
immersed in the flow of juicy information we are presented with. Much of this 
information is useful and interesting – articles written by experts that could
make us more productive, tips on exercise or nutrition, or fascinating stories 
from around the world. But unless we make conscious, strategic decisions about 
what we consume, we’ll always be at the mercy of what others want us to see.

Instead, adopt the mindset of a curator – objective, opinionated, and reflective.
As you come across social media updates, online articles, and podcasts throughout
your day, instead of diving in immediately, save them for future consideration.
As you begin to collect content, you’ll be able to choose which sources to
consume in a deliberate way.

**Organize your content by project**
How should you organize the content once you’ve captured it? Instead of 
organizing your files primarily by topic (for example, web design or psychology),
which is time-consuming and mentally taxing, organize them according to the 
projects you are actively working on. This ensures that you are consuming 
information with a purpose – to advance your projects and goals – and only at a
time and place where you’ll be able to put it to use.

The PARA organizational system takes this principle – organizing information by
when you would like to see it next – and applies it to your entire digital life.
Instead of organizing each one of the information management tools you use in a 
completely different way, use your projects as universal categories across all 
of them. This helps reduce the fragmentation of your project files, without
requiring you to only use one tool for everything.

**Keep only what resonates**
The word “organization” often brings to mind an analytical way of thinking. But
analysis is time-consuming and tiring. In deciding which passages, images,
theories, or quotes to keep, don’t make it a highly intellectual, analytical
decision.

Instead, your rule of thumb should be to save anything that “resonates” with 
you on an intuitive level. This is often because it connects to something you
care about, wonder about, or find inherently intriguing. By training ourselves
to notice when something resonates with us at a deeper level, we improve not
only our ability to see opportunities, but also our understanding of ourselves
and how we work.
```

As regards actionable note summarization, I like most the advice to "organize opportunistically":

```markdown
**Organize opportunistically, a little bit at a time**
It can be tempting to spend a lot of time to create highly structured, 
perfectionistic notes. The problem is, you often have no idea which sources will
end up being valuable until much later. Instead of investing a lot of effort 
upfront, organize your notes opportunistically, in small bits over time.

Your rule of thumb should be: **add value to a note every time you touch it**. This
could include adding an informative title the first time you come across a note,
highlighting the most important points the next time you see it, and adding a 
link to a related note sometime later. By spreading out the heavy work of 
organizing your notes over time, you not only save time and effort, but **ensure 
that the most frequently used (and thus most valuable) notes surface organically**,
like a ski slope where the most popular routes naturally end up with deeper grooves.
```

It's this eventual vision that I aspire to:

```markdown
As your second brain gains momentum over weeks and months, you will start to become
different. You will no longer think about things in isolation, but as part of a
network of ideas in which everything affects everything else. You’ll realize that 
something you learned at work about effective communication also applies to your 
family vacation debate.. A random fact you read in an airplane magazine will somehow
end up being useful in a blog post you’re writing. A lesson from Ancient Greek 
history you picked up from a podcast on your morning commute will help you deal 
with a crisis at the office. You will start to think in terms of the systems and
principles that you’ve gleaned through your summarizing and reviewing, and see them 
everywhere.

Your mind will start to work differently, learning to depend on this external tool 
to draw on resources, references, and research far beyond what it can remember on 
its own. You will start to conceive of “your work” as an integrated whole that you
can actually point to, shape, and navigate in a direction of your choosing. You’ll 
be more objective and unattached, because if any single idea doesn’t work out, you
know you have a huge trove of others ready to go.

Over time, you will start to recognize that everything you are learning and 
experiencing makes sense. You can see, mapped in the notes you are cultivating, the
underlying structure of your life. Why you do things, what you really want, what’s
really important and what isn’t. Your second brain becomes like a mirror, reflecting
back to you who you think you are, who you want to be, and who you could become. 
Because you know how to capture and make use of anything, every experience you have
becomes an opportunity to learn and to grow.

As this self-understanding dawns, you will look around at the notes you’ve collected,
and you will realize that you already have everything you need to get started. You 
will start combining the ideas together, forming new perspectives, new theories, and 
new strategies. Ideas about society, about art, about psychology, about spirituality,
about technology will start intermixing and spawning ideas you’ve never consciously
considered. You’ll be shocked, in fact, at the elegance and power of what pops out of 
your notes.

This epiphany won’t just exist in your head. People can tell. They’ll start to notice 
that you can draw on an unusually large body of knowledge at a moment’s notice. They
will admire your amazing memory, but what they don’t know is that you never try to 
remember anything. They’ll admire your incredible self-discipline and dedication at 
developing ideas over time, not knowing that you’ve created a system in which
insights and connections emerge organically. They’ll be impressed by your ability to
produce so much creative output, but in reality, you never lock yourself in a room 
to “crank out” some work. You just let your projects simmer until they’re ready.

Building a Second Brain is an integrated set of behaviors for turning incoming
information into completed creative projects. Instead of endlessly optimizing 
yourself, trying to become a productivity machine that never deviates from the plan,
it has you optimize an *external system* that is more reliable than you will ever be.
This frees you to imagine, to wonder, to *wander* toward whatever makes you come alive 
here and now in the moment.
```

<a name="#Social-computational-hindbrain"></a>
### Social computational hindbrain
([overview](#overview))

From Venkat Rao's [Passport to the metabrain](https://us1.campaign-archive.com/?u=78cbbb7f2882629a5157fa593&id=0f299f9100). Starts from the idea of distributed cognition, but segues into [collective cognition](#Collective-cognition). This subsection's topical placement should hence be taken as recall-aiding, not strict hierarchical.

```markdown
1/ We think about digital social technology with a connection metaphor: the social graph. 
In this metaphor other people form a space we enter via connections.

2/ I want to introduce you to the alternative *non-exclusive mutual containment* metaphor:
the social hind-brain. In this metaphor, each of us has a second brain which other people 
inhabit. ...

6/ For concreteness, consider this email list of 4500 odd. Only a small minority of you 
also have your own email lists. But what if ALL did and we were all on each other’s lists.

7/ Imagine that participation inequality — the 90:9:1 rule — applied. For each of us, 1%
of subscribers (450) would be active conversation partners, 9% (405) would be casual 
commenters, and 90% (4050) would be lurkers.

8/ This is very different from the idea of a single shared hive mind. There are 4500 
*different* hive minds here, each with a different 90:9:1 "neural" structure and 
interconnections within them.

9/ The themes of the 4500 email lists, the writing style, collaborative thinking style,
Q&A style, socialization style, all would make each second brain very unique.

10/ And this is just email and text mediated semi-public relationships. Once you take into
account for the media mix, content types, and access structure each of uses, you get 
serious uniqueness.

11/ In fact, each of us maintains an entire city-like social space, with a variety of formal
and tacit rules, gatekeeping processes, conversations flows, memory structures (Evernote, 
blogs) and so on. Our second brains are complete societies of other minds.

12/ How these societies get “filled up” over time as we age and non-coercively “collect” 
other people is non-trivial.

13/ If you couple this concept of a second brain with the idea of fingers of mind, or 
Fingerspitzengehful, which we talked about last season, you get a very different sense of
your species nature.

14/ Even with hardware hacking of the human brain and body, we are already a very different 
species than the one that inhabits our default self-perceptions.

15/ We are like the reverse of the Ood species in Doctor Who. We use technological prosthetics 
to create a universe of hive minds, to which we’re connected via a sub-linguistic digital 
tissue. ...

19/ But humans are neither as collective and hive-mind-like as the Ood, nor as individualistic
as many earth species. We just use ideology to wishfully mark where we think we are on the 
spectrum.

20/ Leftists place us more towards the global collective end. Libertarians place us towards
the individualist end. Conservatives focus on connection structures rather than degree.

21/ There is truth to all three ideological views of our species nature. Technology today 
allows each of these abstract ideologies to be concretely realized via actual connection 
modalities.

22/ As you might expect, the old “containment as exclusive collection" ethos is not dead. 
Many would like to see social media turn into new serfdoms defined by unified collectives.
No opting out (did you know you can't block Mark Zuckerberg on Facebook? 😂)

23/ But the idea that each of us has a unique, individual, social hindbrain is much more 
powerful and generative. Why enslave each other when we can put partial digital copies of 
ourselves in each other’s second brains?

24/ This newsletter itself is an example of second-brain work: I have used the living 
ideas/work of several people, whom I know to varying degrees, for my own ends, without 
"enslaving" them on my manorial estate.

25/ The second brain, or social hindbrain (I use the terms interchangeably) would be useless
without a corresponding evolution in the structure of our agency.

26/ Today, 12,000-15,000 years after the Neolithic revolution, we have evolved far beyond
being dependent on our muscles and opposable thumbs, guided primarily by our eyes and ears.

27/ Our sensory-tactile loop, through which our two-brained minds acts in the world, is now
much more a metaphoric tactile structure of tentacles, emanating from our face like the Ood:
the fingers of our mind.

29/ Anything we can do with the visual-aural-tactile loop, machines can already do better, 
or soon will. What we are getting better at is things that require the “fingers of the mind.”

30/ Programming is the classic example. You grope your way in the dark, so to speak, through
complex, ever-changing toolchains and stacks far beyond our sensory capacities.

31/ These tools are based on advanced mathematics (such as category theory for functional 
programming) far beyond the 3d visualizable geometries we can intuitively navigate with eyes.

32/ Unlike our ancestors, who threw spears at mammoths using the eye-ear-arm-hand loop, 
programmers act on the world through feedback structures like REPL (read-eval-print loop)

33/ More and more things that humans do will involve feedback structures like the REPL loop
that interface with entire armies of bots and machines spanning the datacenter-to-Internet-
of-things gap at planet scale.

34/ As we are each developing a unique second brain, a social hindbrain, with which to think,
we are also collectively developing a planet-scale shared body through which to act. 

35/ Conflicts over use of the body — two programmers fighting to control the smart traffic 
grid of a city full of driverless cars for instance — will be mediated by conflicts between
two extended minds. ...

37/ Our individual second brains — theoretically 7 billion of them — are only the lowest 
layer of a much more complex emerging meta-brain containing brains of all scales.

38/ This metabrain is neither individualist nor collective. Neither a stifling, claustrophobic
condition of forced "belonging" and Borg-like "assimilation", nor a terrifying one of isolation.

39/ Instead, it's a higher-dimensional version of our current one-dimensional individual-to-
collective notion of social possibilities. It includes ways of being for which we have no words.

40/ In this emerging metabrain, questions like "are you an introvert or extrovert" behave as 
weirdly as matter at near-speed-of-light velocities.

41/ Today's metabrain is a clumsy and wobbly structure of industrial age institutions — 
marriages, families, cities, nations, corporations.

42/ This industrial metabrain works by applying mutually-exclusive-collectively-exhaustive
(MECE) territorial “collection” logic to minds and fingers. ...

43/ Within a couple of centuries, we will probably have a meta-brain based on non-MECE 
contaiment logic. Each human will flexibly move around within a much vaster space than today.

44/ Yes, science-fictional, but there are no essential scientific or technological roadblocks
preventing this future, and a variety of signs already point in this direction.

45/ Today, we use vague, abstract concepts like egregores to think about ideologies, nations,
communities, and Arendtian publics, to understand the human condition.

46/ We are entering an age where all such abstractions can take concrete, literal form through
technologies as far beyond Twitter as Twitter is beyond a campfire or traditional public square.

47/ Future generations might think of our clumsy social groping on twitter and primitive “IoT
hacks” and “troll mobs” the way we think of cavewomen gathering berries in baskets and cavemen 
fumbling with fire and spears.

48/ They will laugh at our impoverished and primitive imagination, evident in ideas like the
Singularity, Gaia, or a single Borg-like hive mind, the way astronomers laugh at astrologers
today.

49/ But even the limited form of the emerging future that exists today is an enormous challenge
for our psyches, evolved for speech-and-text based collectivism and muscle power agency.
```

So what can we do to start living in this future? Venkat's suggestions are mostly the obvious ones, just with cool Lovecraftian-overtones metaphors: 

```markdown
51/ You can consciously learn to use your second brain and extended body: your extended 
phenotype as Richard Dawkins called it.

52/ The key to using your second brain well is to switch from the connection metaphor to
the non-exclusive containment metaphor, and consciously design how other humans “people”
it. Your brain has a public square attached now. 

53/ First, as a friend of mine once said, “if you keep an open mind, other people will 
throw trash in it.” The alternative is not a closed mind, but a managed, consciously 
peopled second brain, evolved as carefully as a beehive (you’re the queen).

54/ Second, you must learn to be a good citizen of other people’s second brains, to the 
extent you’re an active participant in them. If you are like me, almost everything you do
involves being a neuron in others’ second brains.

55/ Remember, every link you pass along, every action you suggest, is a neuron firing in 
the second brain of somebody who is struggling as hard as you are to get comfy in their
new digitally extended skins.

56/ Third, think of your ways of acting as technological tentacles emerging from where 
your mouth is. Your actions are always “programming” the live behavior of complex 
technological systems.

57/ Stop thinking of typing a tweet, for example, as 140 characters worth of hand-eye 
coordination. You are sending a quiver down a planet-scale tentacle that causes movement 
in tens of thousands of devices.

58/ Thinking with your second brain does not feel like either work or play, nor does it 
feel like either individual or collective modes of being. The first, biological brain is
simple not used to the feel of the second brain.

59/ To the first brain, an active second brain feels like somewhat tiring, exhausting, 
futile, dissipative, yet oddly addictive “wasting time on social media” or “idle browsing”
or “Evernote cut-and-paste going nowhere”

60/ Compared to the delightful feel of working on a clean math problem, writing a poem,
solving a puzzle, or "shipping" a product, second-brain thinking feels like long periods
of pointless messiness interspersed with short bursts of weird-fun play.

61/ Similarly, compared to the harmonious, positive feels of say cooking a leisurely meal
or playing a musical instrument alone, working through your technological tentacles via 
fingers-of-mind feels ugly and awkward.

62/ Recognize both feelings and states for what they are, and get used to managing them.
If you reject them as “unnatural” you will never learn to tell the good and bad apart 
within this emerging experiential space.

63/ Inhabiting modernity with a second social hindbrain and planet-scale tentacular 
hypertactile form of agency is, as Sarah Perry puts it, a messy condition. Whether it's a 
good or bad mess is up to you.

64/ Instead of yearning romantically, as Hannah Arendt did, for the lost Greek polis as the
ideal of a "public," you have to situate your Arendtian action in a roll-your-own community
within the metabrain.
```

Donald Trump as one of the first "truly powerful inhabitants of the metabrain of social hindbrains":

```markdown
65/ And if you’re ever inclined to doubt that you can indeed use a social second brain and
your tentacles in leveraged ways that lead to great power, just think of Donald Trump.

66/ Trump comes across as a sort of chaosbot/ubertroll who operates by very simple button-
pushing. This has made him one of the first truly powerful inhabitants of the metabrain.

67/ His rise is in a way a revenge of every person who has ever been dismissed with the
programmer’s insult, “I will replace you with a very small shell script.” ...

69/ But the fact that he’s one of the first true reverse-Oods does not mean he’s the best 
or the last. Anymore than the fact that porn is usually an early adopter of technology makes
the economy porn-based.

70/ The rest of us are slower in moving into this metabrain condition because we are, to our
credit, being more thoughtful about it. We are not as willing to hurt and abuse along the way.

71/ This is a good thing. If the entry condition for entering the metabrain is to be a small,
abusive script, and leaving the better side of human nature behind, then the passport is not
worth having.

72/ Figure out how to enter the metabrain on your own terms, and take what’s good and 
worthwhile about you. But don’t be too quick to judge good/bad as you pack your baggage.
```

<a name="#Collective-cognition"></a>
## Collective cognition
([overview](#overview))

Related to [distributed cognition](#external-brain), but not quite the same. Cosma Shalizi is a great read here. Here's the introduction he wrote to the [Collective Cognition:
Mathematical Foundations of Distributed Intelligence](http://csc.ucdavis.edu/~dynlearn/colcog/description.htm) workshop he co-organized awhile back, giving modern science and markets as examples:

```markdown
Many forms of individual cognition are enhanced by communication and collaboration with other 
intelligent agents. We propose to call this collective cognition, by analogy with the well known 
concept of collective action. People (and other intelligent agents) often "think better" in groups
and sometimes think in ways which would be simply impossible for isolated individuals. Perhaps the most
spectacular and important instance of collective cognition is modern science. An array of formal 
organizations and informal social institutions also can be considered means of collective cognition. For
instance, Hayek famously argued that competitive markets effectively calculate an adaptive allocation of
resources that could not be calculated by any individual market-participant. 

Hitherto the study of collective cognition has been qualitative, philosophical, even at times anecdotal.
Only recently, we believe, have the tools fallen into place to initiate a rigorous, quantitative science of
collective cognition. Moreover, it appears that soon there will be a real practical need for such a science.
```

A bit more on collective cognition:

```markdown
Collective cognition involves an interaction among three elements-the individual abilities of the agents,
their shared knowledge, and their communication structure. Cognitive collectives therefore resemble many
other complex systems which are collectives of goal-directed processes. Typically, the individual processes
know little of the detailed dynamics and the state of the overall system and, therefore, must use adaptive
techniques to achieve their goals. There are many naturally occurring examples, including human economies, 
human organizations, ecosystems, and even spin glasses. In addition, it has recently become clear that many
of the engineered systems of the future must be of this type, with massively distributed computational 
elements. There is optimism in the multi-agent system (MAS) field that widely applicable solutions to large,
distributed problems are close at hand. Some experts now believe that, in the information and
telecommunications networks of today, we have nascent examples of artificial cognitive collectives.
```

Collective cognition touches on a dazzling variety of fields: 

```markdown
Cognitive science
Situated agents
Emergent computation
Bounded rationality
Institutional economics
Economies of information
Evolutionary game theory
Cognitive ethology
Collective phenomena in physics
Neural computation and distributed representations
Distributed computation
Mechanism design
General equilibrium theory
Population biology
Robustness
Swarm intelligences
Reinforcement learning
Adaptive control
Cultural evolution
Cognitive sociology and the sociology of science
Telecommunications data routing
```

Some basic (i.e. foundational) questions of interest, at least for Cosma's workshop:

```markdown
Interaction between communication structure and cognitive performance.

How are knowledge dynamics and communication structure related? How are computational structure and 
communication structure related?

Why do some collectives not support much cognition and others support substandard or even pathological
forms?

When, and to what extent, can we attribute cognition, or at least computation, to the collective as 
such, rather than its individual members?

How much do we need to know about individual cognition to do adequate models of collective intelligence?

How can the capacity for collective cognition evolve?

Collective-action problems: Why should an agent contribute to the collective? Conversely, does the 
collective-cognition perspective shed any light on collective action in the conventional sense?

Incentive design: When we cannot directly control the goals individual agents, how can we still configure 
the system so that each agent has incentives to pursue a goal that is both readily achievable and good
for the overall collective?

How useful is fiat money in collectives whose agents are not human beings?

When and how should agents be induced to form teams?

When and how does agent heterogeneity enhance collective cognitive performance? Are "disagreement"
and "controversy" among agents always bad, or sometimes desirable for the collective?

How is robustness of behavior against external perturbation related to quality of behavior?
```

Cosma writes about collective cognition in a more fun-to-read way in his [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*. It begins like so:

```markdown
Human beings coordinate their actions to do things which would be hard or impossible for them 
individually. This is not a particularly recondite fact, and the recognition of it is ancient; 
it is in the fifth book of Lucretius's De Rerum Natura, for instance. It was a commonplace of 
the Enlightenment, that most sociable age, and the philosophes were even, it seems, the first to 
realize that thinking, too, can be a collective activity, one conducted and amplified by social
groups --- which is not to say that societies have thoughts. ...

The nineteenth century, and to a lesser degree this one, have witnessed a dramatic expansion in 
the numbers of us engaged in administration, bureaucracy, management, oversight --- that is to say,
in formally-organized tasks of collective cognition and control. We did not invent bureaucracy, 
the mainstay of the ancient empires, but we're much, much better at it than they were. A random 
American town of 200,000 --- Piffleburg, WI, let us say --- will have police, a rescue squad, a fire
department, a hospital, universal schooling, several large factories, insurance offices, banks, a 
community college, a public library with several thousand volumes at least, a post office, public 
utilities, political parties, garbage collection, paved and usable roads everywhere, mercantile 
connections stretching across the country, and, with some luck, unions. These are corrupt, 
inefficient institutions which work poorly; every election, Piffleburg's citizens mutter something 
like "what do we pay taxes for anyway?" Yet to run any one of these institutions at the level of 
honesty, efficiency and efficacy which makes Piffleburg grumble would have demanded the full powers
and attention of even the ablest Roman propraetor or T'ang magistrate. That all of those institutions,
plus the ones not restricted to a single city, could be run at once, and while governed by a very 
ordinary slice of common humanity, would have seemed to such officials flatly impossible.
```

So why are we, as Cosma puts it, "so much better at collective endeavors than the ancients"? This is fairly easy to address:

```markdown
To a first approximation, the answer is: brute force and massive literacy. We teach nearly everyone to
read and write, and to do it, by historical standards, at a high level. This lets us staff large
bureaucracies (by some estimates, over 40% of the US workforce does data-handling), which lets us run an
industrial economy (the trains run on time), which makes us rich enough to afford to educate everyone 
and keep them in bureaucratic employment, with some surplus left over to expand the system. 

 This would do us no good if our ideas of administration were as shabby as those of our ancestors in the 
 dark ages, but they're not: we inherited those of the ancient empires, and have had quite a while to 
 improve upon them (and improvements are made easier and faster by the large number of administrators and
 the high standard of literacy). Among the improvements are many techniques (standardized procedures,
 standardized parts, standardized credentials and jobs, explicit qualifications for jobs and goods, files,
 standardized categories) and devices (forms, punch cards, punch card tabulators, adding machines, card
 catalogs, and, recently, computers) for making the administration of people and things easier.
```

Now while this is all splendid, it's "in the realm of technique"; when it comes to theory, nobody has any real idea how to explain what's going on:

```markdown
We don't really have a good theory about how collective action and cognition work, when and why they do, 
how they can be made to work better, why they fail, what they can and cannot accomplish, and so forth. 

Intellectually, these are large, tempting problems; technologically, they have obvious relevance to the 
design of parallel and distributed computers; economically, they could mean real money, not just billions;
and, in general, it'd be nice to know what it is we've gotten ourselves into.
```

Enter Edwin Hutchins, who conducted fieldwork studying navigation on a US navy ship based in San Diego with the problem of theoretically grounding collective cognition in mind. His fieldwork is interesting:

```markdown
Hutchins's field evidence consists of very detailed records, taken in the early 1980s, on the performance 
of the navigation crew of a helicopter carrier ship he calls the Palau, principally as they fix their 
location and plot their course near shore. The way it worked, in those pre-GPS days, was, roughly, this: 
three land-marks on shore, of known location on the navigation charts, would be selected by the main
person in charge, the "quartermaster of the watch." Then they'd "take bearings" on these, i.e. find the
orientation of the line from the landmark to the ship. These lines would be drawn on the chart. Now, it's 
an elementary result in Euclidean geometry that any two lines meet at a single point (unless they're parallel);
three lines form a triangle (unless they all meet at the same point). Somewhere within that triangle is the 
ship: this fixes the current position. The position of the ship at the next fix is estimated by "dead
reckoning," which is simply taking the current position and heading of the ship, and its planned speed, and 
extrapolating forward along the line of its heading. A single person can do this, if he's not too rushed.
Close to shore, the Navy gets worried, and demands fixes every few minutes, so the task gets broken down: 
naval flunkies take the bearings, a different flunky tells them when to take the bearings, and so on. There's 
a fairly rigid protocol for coordinating all these actions, and for communicating their results in a usable
form, and specialized instruments for making the job easier.

So, what does all this actually show? Well, that cognitive tasks can get spread over several people; that, 
in this instance, those who do tasks which require input from other people are generally superior to them in
rank; that the official job descriptions do not quite correspond to what people do; that people have a hard 
time believing things which are strange to them, and tend to ask those who report them questions along the 
lines of "Are you sure?"; that, if you don't know what something looks like, a verbal description can be very
unhelpful; that the right tools can make the job simpler; and that building computation into tools can make 
the job simpler for people, since it's easier to use a slide-rule than take a sine or a logarithm in your head;
that, if you can't talk about something, it's hard to make plans with someone else about it. There's more, but 
they're along the same lines.

These are not exactly earth-shaking results; in fact, they're about what common sense says to us. This doesn't 
make it useless to check them, since common sense is so often wrong; but even then, Hutchins has checked them
against the performance of one task (navigation; more particularly, location fixing), in one set of social 
groups (a couple of ships of the US Navy) --- ones where the social system is designed, and has received several
centuries of re-design from people whose common sense more or less agrees with the above. (One wonders if they
did things differently aboard the Potemkin.)
```

Unfortunately his conclusions are rubbish.

<a name="#procedural-vs-declarative-memory"></a>
## Procedural vs declarative memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), talking about procedural vs declarative memory in the context of using Anki flashcards:

```markdown
There's a big difference between remembering a fact and mastering a process. For instance, while 
you might remember a Unix command when cued by an Anki question, that doesn't mean you'll recognize
an opportunity to use the command in the context of the command line, and be comfortable typing it 
out. And it's still another thing to find novel, creative ways of combining the commands you know, 
in order to solve challenging problems.

Put another way: to really internalize a process, it's not enough just to review Anki cards. You need
to carry out the process, in context. And you need to solve real problems with it.
```

From Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally):

```markdown
Propositional knowledge is being gradually supplanted by the procedural.  You need only know *how to find*
information, to be able to use it after a trivial delay.  This requires some snippet of propositional data
- to find a song lyric, you need a long enough string that you won't turn up 99% noise when you try to 
Google it! - but mostly, it's a skill, not a fact, that you need to act like you knew the fact.

It's not clear to me whether this means that we should be alarmed and seek to hone our factual memories...
or whether we should devote our attention to honing our Google-fu, as our minds gradually become
server-side operations.
```

<a name="#augmenting-long-term-memory"></a>
## Augmenting long-term memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), which is an all-around great essay you should read in its entirety. One of its theses:

```markdown
Many people treat memory ambivalently or even disparagingly as a cognitive skill: for instance,
people often talk of “rote memory” as though it's inferior to more advanced kinds of understanding.
I'll argue against this point of view, and make a case that memory is central to problem solving 
and creativity.
```

To expand on his point:

```markdown
It's a mistake to underestimate the importance of memory. I used to believe such tropes about the 
low importance of memory. But I now believe memory is at the foundation of our cognition.

There are two main reasons for this change, one a personal experience, the other based on evidence
from cognitive science.

Let me begin with the personal experience.

Over the years, I've often helped people learn technical subjects such as quantum mechanics. Over 
time you come to see patterns in how people get stuck. One common pattern is that people think 
they're getting stuck on esoteric, complex issues. But when you dig down it turns out they're having 
a hard time with basic notation and terminology. It's difficult to understand quantum mechanics when
you're unclear about every third word or piece of notation! Every sentence is a struggle.

It's like they're trying to compose a beautiful sonnet in French, but only know 200 words of French.
They're frustrated, and think the trouble is the difficulty of finding a good theme, striking 
sentiments and images, and so on. But really the issue is that they have only 200 words with which to
compose.

My somewhat pious belief was that if people focused more on remembering the basics, and worried less 
about the “difficult” high-level issues, they'd find the high-level issues took care of themselves.

But while I held this as a strong conviction about other people, I never realized it also applied to
me. And I had no idea at all how strongly it applied to me. Using Anki to read papers in new fields 
disabused me of this illusion. I found it almost unsettling how much easier Anki made learning such 
subjects. I now believe memory of the basics is often the single largest barrier to understanding. If
you have a system such as Anki for overcoming that barrier, then you will find it much, much easier 
to read into new fields.

This experience of how much easier Anki made learning a new technical field greatly increased my 
visceral appreciation for the importance of memory.
```

<a name="#Yegge-on-memory"></a>
## Yegge on memory
([overview](#overview))

From one of his more memorable posts, [Done and gets things smart](https://steve-yegge.blogspot.com/2008/06/done-and-gets-things-smart.html):

```markdown
So we all think we're smart for different reasons. Mine was memorization. Smart, eh? 
In reality I was just a giant, uncomprehending parrot. I got my first big nasty surprise
when I was in the Navy Nuclear Power School program in Orlando, Florida, and I was 
setting historical records for the highest scores on their exams. The courses and exams 
had been carefully designed over some 30 years to maximize and then test "literal 
retention" of the material. They gave you all the material in outline form, and made you
write it in your notebook, and your test answers were graded on edit-distance from the 
original notes. (I'm not making this up or exaggerating in the slightest.) They had set 
up the ultimate parrot game, and I happily accepted. I memorized the entire notebooks 
word-for-word, and aced their tests.

They treated me like some sort of movie star — that is, until the Radar final lab exam in
electronics school, in which we had to troubleshoot an actual working (well, technically,
not-working) radar system. I failed spectacularly: I'd arguably set another historical 
record, because I had no idea what to do. I just stood there hemming and hawing and pooing
myself for three hours. I hadn't understood a single thing I'd memorized. Hey man, I was 
just playing their game! But I lost. I mean, I still made it through just fine, but I lost
the celebrity privileges in a big way.

Having a good memory is a serious impediment to understanding. It lets you cheat your way
through life. I've never learned to read sheet music to anywhere near the level I can play
(for both guitar and piano.) I have large-ish repertoires and, at least for guitar, good 
technique from lots of lessons, but since I could memorize the sheet music in one sitting,
I never learned how to read it faster than about a measure a minute. (It's not a 
photographic memory - I have to work a little to commit it to memory. But it was a lot 
less work than learning to read the music.) And as a result, my repertoire is only a 
thousandth what it could be if I knew how to read.

My memory (and, you know, overall laziness) has made me musically illiterate.

But when you combine the Dunning-Kruger effect (which affects me just as much as it does 
you) with having one or two things I've been good at in the past, it's all too easy to 
fall into the trap of thinking of myself as "smart", even if I know better now. All you 
have to do, to be "smart", is have a little competency at something, anything at all, just
enough to be dangerous, and then the Dunning-Kruger Effect makes you think you're God's 
gift to that field, discipline, or what have you.
```

<a name="#wisdom"></a>
## Wisdom
([overview](#overview))

Scott Alexander on wisdom in [Does age bring wisdom?](https://slatestarcodex.com/2017/11/07/does-age-bring-wisdom/) resonated pretty strongly with me:

```markdown
We’ve been talking recently about the high-level frames and heuristics that organize other 
concepts. They’re hard to transmit, and you have to rediscover them on your own, sometimes
with the help of lots of different explanations and viewpoints (or one very good one). 
They’re not obviously apparent when you’re missing them; if you’re not ready for them, they
just sound like platitudes and boring things you’ve already internalized.

Wisdom seems like the accumulation of those, or changes in higher-level heuristics you get
once you’ve had enough of those. I look back on myself now vs. ten years ago and notice
I’ve become more cynical, more mellow, and more prone to believing things are complicated. 
For example:

1. Less excitement about radical utopian plans to fix everything in society at once
2. Less belief that I’m special and can change the world
3. Less trust in any specific system, more resignation to the idea that anything useful
requires a grab bag of intuitions, heuristics, and almost-unteachable skills.
4. More willingness to assume that other people are competent in aggregate in certain
ways, eg that academic fields aren’t making incredibly stupid mistakes or pointlessly
circlejerking in ways I can easily detect.
5. More willingness to believe that power (as in “power structures” or “speak truth to
power”) matters and infects everything.
6. More belief in Chesterton’s Fence.
7. More concern that I’m wrong about everything, even the things I’m right about, on 
the grounds that I’m missing important other paradigms that think about things completely 
differently.
8. Less hope that everyone would just get along if they understood each other a little
better.
9. Less hope that anybody cares about truth (even though ten years ago I would have
admitted that nobody cares about truth).

All these seem like convincing insights. But most of them are in the direction of elite 
opinion. There’s an innocent explanation for this: intellectual elites are pretty wise, 
so as I grow wiser I converge to their position. But the non-innocent explanation is that 
I’m not getting wiser, I’m just getting *better socialized*. Maybe in medieval Europe, the 
older I grew, the more I would realize that the Pope was right about everything.
```

A particular example:

```markdown
I’m pretty embarassed by Parable On Obsolete Ideologies, which I wrote eight years ago. 
It’s not just that it’s badly written, or that it uses an ill-advised Nazi analogy. It’s
that it’s an impassioned plea to jettison everything about religion immediately, because
institutions don’t matter and only raw truth-seeking is important. If I imagine myself 
entering that debate today, I’d be more likely to take the opposite side. But when I read 
Parable, there’s…nothing really wrong with it. It’s a good argument for what it argues for.
I don’t have much to say against it. Ask me what changed my mind, and I’ll shrug, tell you
that I guess my priorities shifted. But I can’t help noticing that eight years ago, New 
Atheism was really popular, and now it’s really unpopular. Or that eight years ago I was in
a place where having Richard Dawkins style hyperrationalism was a useful brand, and now I’m
(for some reason) in a place where having James C. Scott style intellectual conservativism 
is a useful brand. A lot of the “wisdom” I’ve “gained” with age is the kind of wisdom that
helps me channel James C. Scott instead of Richard Dawkins; how sure am I that this is the
right path?
```

This is the "money quote", the whole reason I started this subheading:

```markdown
Sometimes I can almost feel this happening. First I believe something is true, and say so.
Then I realize it’s considered low-status and cringeworthy. Then I make a principled decision 
to avoid saying it – or say it only in a very careful way – in order to protect my reputation 
and ability to participate in society. Then when other people say it, I start looking down on
them for being bad at public relations. Then I start looking down on them just for being low-
status or cringeworthy. Finally the idea of “low-status” and “bad and wrong” have merged so 
fully in my mind that the idea seems terrible and ridiculous to me, and I only remember it’s
true if I force myself to explicitly consider the question. And even then, it’s in a 
condescending way, where I feel like the people who say it’s true deserve low status for not
being smart enough to remember not to say it. This is endemic, and I try to quash it when I 
notice it, but I don’t know how many times it’s slipped my notice all the way to the point 
where I can no longer remember the truth of the original statement.
```

Are old people really wiser? Why do they sound so crankily conservative? A model:

```markdown
And if I accept my intellectual changes as “gaining wisdom”, shouldn’t I also believe that 
old people are wiser than I am? And old people mostly seem to go around being really 
conservative and saying that everything was better in the old days and the youth are corrupt
and Facebook is going to be the death of us. I could model this as two different processes –
a real wisdom-related process that ends exactly where I am now, plus a false rose-colored-
glasses-related process that ends with your crotchety great-uncle talking about how things 
have been going downhill since the war – but that’s a lot of special pleading. I remember 
when I was twenty, I thought the only reason adults were less utopian than I was, was
because of their hidebound rose-colored self-serving biases. Pretty big coincidence that I 
was wrong then, but I’m right about everyone older than me *now.*
```

John "Erisology" Nerst responds:

```markdown
I think there could be selection effect. Not all people get wiser as they age and many hit 
a ceiling at some time. Maybe those are the ones most likely to make their opinions heard 
(I mean, it’s hardly the case that the wisest are the loudest among the younger population
either). And the really wise ones stay silent because their wisdom has become impossible to
communicate?

It reminds me of the quote from Julian Barnes’ Staring at the Sun:

*Everything you wanted to say required a context. If you gave the full context, people
thought you a rambling old fool. If you didn’t give the context, people thought you a
laconic old fool.”*
```

Worst-case scenario -- wisdom as "NMDA reception function change with age":

```markdown
There’s one more possibility that bothers me even worse than the socialization or
traumatization theory. I’m going to use science-y sounding terms just as an example, but I 
don’t actually think it’s this in particular – we know that the genes for liberal-conservative 
differences are mostly NMDA receptors in the brain. And we know that NMDA receptor function 
changes with aging. It would be pretty awkward if everything we thought was “gaining wisdom
with age” was just “brain receptors consistently functioning differently with age”. If we
were to find that were true – and furthermore, that the young version was intact and the older 
version was just the result of some kind of decay or oxidation or something – could I trust 
those results? Intuitively, going back to earlier habits of mind would feel inherently 
regressive, like going back to drawing on the wall with crayons. But I don’t have any *proof.*
```

This is pithily captured in the quote (attribution unknown, phrasing by "Keith"):

```markdown
He who isn’t radical as a youth has no heart. 
And he who isn’t conservative as an adult has no brain.
```

What this looks like in science -- the "grand old academics" phenomenon:

```markdown
I’ve noticed a vaguely related trend in science:

you get a number of grand old academics, the kind of people who continue to hang out at the
institution long after they’re officially retired who are an absolute goldmine for various 
minutiae of their subject.

They’ve tried many approaches over the decades and can warn you about dead ends….

but they also often have an overabundance of cynicism.

Often they remember that approach X didn’t work, they may not remember the exact details as 
to why. their memory of the event gets pared down to “that’s a dead end”… and then at some 
point a new generation of grad students come along and eventually someone ignores the advice 
that X is a dead end and it turns out that in the 30 years that have passed the things that 
made X a dead end no longer apply. The sequencing methods can now read through long-repeats 
or the chemistry used for some step is improved or some background piece of knowledge has
been added to the field that now allows people to power through the former roadblock.

Is that wisdom? Knowing lots of dead ends can be useful and can save resources…but it can also
be maladaptive as the world changes around you.
```

Commenter Deej's response:

```markdown
we need to distinguish between individual people changing their views as they get older, 
and the centre grounds shifting as younger people are more liberal than their predecessors.
My feeling is that for economic issues people’s individual views probably do shift 
rightwards as they get older, but for social issues it’s seems likely that it’s the centre
ground that’s moving. Although for today’s more exterme identity politics left youths that
might change.

Third. I think it’s worth distinguising between types of people and how their views might 
change. People who are properly interested in politics, for example, are – I would exepct 
– much more likely top have big changes in their views, than those that aren’t. See ex-
trotskists now in the Tory party or at least Blairite in the UK. I expect that the people
interested in politics changes are likely to be relatively more driven by learning from 
experience and reflective thought than people just slowly change their views over time from,
for example, a bit left to a bit right of centre. Or left to a bit left less left, right to
a bit less right etc.
```

Somewhat relevant are these quotes from Robin Hanson's *Age of Em*:

```markdown
Controlling for birth cohort, individual productivity does not peak until at least age 60,
and may never peak (Cardoso et al. 2011; Göbel and Zwick 2012). […] Also, any falling
productivity after age 60 for humans today may be primarily caused by declining physical
abilities, not declining mental abilities

Today, our abilities at different kinds of tasks peak at different ages. For example, raw 
cognitive processing peaks in late teens, learning and remembering names in early 20s, 
short-term memory about age 30, face recognition in early 30s, social understanding about
age 50, and word knowledge above age 65 (Hartshorne and Germine 2015).
```

<a name="#The-predictive-uselessness-of-folk-wisdom"></a>
### The predictive uselessness of folk wisdom
([overview](#overview))

Keith E. Stanovich, [How to Think Straight About Psychology](http://www.pearsonhighered.com/assets/hip/us/hip_us_pearsonhighered/samplechapter/0205914128.pdf):

```markdown
Often a person uses some folk proverb to explain a behavioral event even though, on an earlier occasion, this 
same person used a directly contradictory folk proverb to explain the same type of event. For example, most of 
us have heard or said, “look before you leap.” Now there’s a useful, straightforward bit of behavioral advice—
except that I vaguely remember admonishing on occasion, “he who hesitates is lost.” And “absence makes the heart 
grow fonder” is a pretty clear prediction of an emotional reaction to environmental events. But then what about 
“out of sight, out of mind”? And if “haste makes waste,” why do we sometimes hear that “time waits for no man”? 
How could the saying “two heads are better than one” not be true? Except that “too many cooks spoil the broth.” 
If I think “it’s better to be safe than sorry,” why do I also believe “nothing ventured, nothing gained”? And if 
“opposites attract,” why do “birds of a feather flock together”? I have counseled many students to “never to put 
off until tomorrow what you can do today.” But I hope my last advisee has never heard me say this, because I just 
told him, “cross that bridge when you come to it.”

The enormous appeal of clichés like these is that, taken together as implicit “explanations” of behavior, they 
cannot be refuted. No matter what happens, one of these explanations will be cited to cover it. No wonder we all 
think we are such excellent judges of human behavior and personality. We have an explanation for anything and 
everything that happens. Folk wisdom is cowardly in the sense that it takes no risk that it might be refuted.

That folk wisdom is “after the fact” wisdom, and that it actually is useless in a truly predictive sense, is why 
sociologist Duncan Watts titled one of his books: Everything Is Obvious—Once You Know the Answer (2011). Watts 
discusses a classic paper by Lazarsfeld (1949) in which, over 60 years ago, he was dealing with the common 
criticism that “social science doesn’t tell us anything that we don’t already know.” Lazarsfeld listed a series 
of findings from a massive survey of 600,000 soldiers who had served during World War II; for example, that men 
from rural backgrounds were in better spirits during their time of service than soldiers from city backgrounds. 
People tend to find all of the survey results to be pretty obvious. In this example, for instance, people tend 
to think it obvious that rural men would have been used to harsher physical conditions and thus would have 
adapted better to the conditions of military life. It is likewise with all of the other findings—people find them 
pretty obvious. Lazarsfeld then reveals his punchline: All of the findings were the opposite of what was originally 
stated. For example, it was actually the case that men from city backgrounds were in better spirits during their 
time of service than soldiers from rural backgrounds. The last part of the learning exercise is for people to 
realize how easily they would have explained just the opposite finding. In the case of the actual outcome, people 
tend to explain it (when told of it first) by saying that they expected it because city men are used to working 
in crowded conditions and under hierarchical authority. They never realize how easily they would have concocted an 
explanation for exactly the opposite finding.
```
