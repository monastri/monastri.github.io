*[Word count](https://wordcounter.net/): 9,800*

## What is this?

This is the "memory, the brain and extended cognition" section of [this notebook](https://github.com/monastri/monastri.github.io/blob/master/quotes.md), last updated Apr 6, 2019, which got so big (1.2 million char) that GitHub refused to render the whole page anymore, ruining my original dream of having my entire notebook in one long page for two purposes: (1) zero-latency clickthrough to make up for my working memory's sand-thru-sieve transience; (2) lower activation energy for continuous refactoring of contents list, to aid recall.

That said, this is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping. (Or at least that was the original intent before I had to break up the notebook. Now I'm not so sure I can do this.)

I also really, *really* hate experiences of the [Jeremy Bentham type](https://github.com/monastri/monastri.github.io/blob/master/notes-amazing-people.md#Jeremy-bentham). This document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [Augmenting long-term memory](#augmenting-long-term-memory), e.g. Anki
2. [Procedural vs declarative memory](#procedural-vs-declarative-memory)
2. [Externalizing the brain](#external-brain), e.g. Google, writing, Cosma on collective cognition
3. [Cognitive science](#cognitive-science)
3. [Names matter](#names-matter)
4. [Steve Yegge on memory](#Yegge-on-memory)
5. [Wisdom](#wisdom)
6. [The predictive uselessness of folk wisdom](#The-predictive-uselessness-of-folk-wisdom) (Stanovich)

----------------------------------

<a name="#The-predictive-uselessness-of-folk-wisdom"></a>
## The predictive uselessness of folk wisdom
([overview](#overview))

Keith E. Stanovich, [How to Think Straight About Psychology](http://www.pearsonhighered.com/assets/hip/us/hip_us_pearsonhighered/samplechapter/0205914128.pdf):

```markdown
Often a person uses some folk proverb to explain a behavioral event even though, on an earlier occasion, this 
same person used a directly contradictory folk proverb to explain the same type of event. For example, most of 
us have heard or said, “look before you leap.” Now there’s a useful, straightforward bit of behavioral advice—
except that I vaguely remember admonishing on occasion, “he who hesitates is lost.” And “absence makes the heart 
grow fonder” is a pretty clear prediction of an emotional reaction to environmental events. But then what about 
“out of sight, out of mind”? And if “haste makes waste,” why do we sometimes hear that “time waits for no man”? 
How could the saying “two heads are better than one” not be true? Except that “too many cooks spoil the broth.” 
If I think “it’s better to be safe than sorry,” why do I also believe “nothing ventured, nothing gained”? And if 
“opposites attract,” why do “birds of a feather flock together”? I have counseled many students to “never to put 
off until tomorrow what you can do today.” But I hope my last advisee has never heard me say this, because I just 
told him, “cross that bridge when you come to it.”

The enormous appeal of clichés like these is that, taken together as implicit “explanations” of behavior, they 
cannot be refuted. No matter what happens, one of these explanations will be cited to cover it. No wonder we all 
think we are such excellent judges of human behavior and personality. We have an explanation for anything and 
everything that happens. Folk wisdom is cowardly in the sense that it takes no risk that it might be refuted.

That folk wisdom is “after the fact” wisdom, and that it actually is useless in a truly predictive sense, is why 
sociologist Duncan Watts titled one of his books: Everything Is Obvious—Once You Know the Answer (2011). Watts 
discusses a classic paper by Lazarsfeld (1949) in which, over 60 years ago, he was dealing with the common 
criticism that “social science doesn’t tell us anything that we don’t already know.” Lazarsfeld listed a series 
of findings from a massive survey of 600,000 soldiers who had served during World War II; for example, that men 
from rural backgrounds were in better spirits during their time of service than soldiers from city backgrounds. 
People tend to find all of the survey results to be pretty obvious. In this example, for instance, people tend 
to think it obvious that rural men would have been used to harsher physical conditions and thus would have 
adapted better to the conditions of military life. It is likewise with all of the other findings—people find them 
pretty obvious. Lazarsfeld then reveals his punchline: All of the findings were the opposite of what was originally 
stated. For example, it was actually the case that men from city backgrounds were in better spirits during their 
time of service than soldiers from rural backgrounds. The last part of the learning exercise is for people to 
realize how easily they would have explained just the opposite finding. In the case of the actual outcome, people 
tend to explain it (when told of it first) by saying that they expected it because city men are used to working 
in crowded conditions and under hierarchical authority. They never realize how easily they would have concocted an 
explanation for exactly the opposite finding.
```

<a name="#names-matter"></a>
## Names matter
([overview](#overview))

In Michael Nielsen's essay [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html) there's a section recounting a famous story in physics I've always taken to heart, the one by Dick Feynman dismissing the value of knowing the names of things:

```markdown
One kid (a know-it-all) says to me, “See that bird? What kind of bird is that?” 

I said, “I haven't the slightest idea what kind of a bird it is.” 

He says, “It'a brown-throated thrush. Your father doesn't teach you anything!” 

But it was the opposite. He (Feynman's father) had already taught me: “See that bird?” 
he says. “It's a Spencer's warbler.” (I knew he didn't know the real name.) “Well, in 
Italian, it's a Chutto Lapittida. In Portuguese, it's a Bom da Peida… You can know the name 
of that bird in all the languages of the world, but when you're finished, you'll know absolutely
nothing whatever about the bird! You'll only know about humans in different places, and what they
call the bird. So let's look at the bird and see what it's *doing* — that's what counts.” (I
learned very early the difference between knowing the name of something and knowing something.)
```

Dick might've also been influenced by the following sentiment, expressed in *Computers From The Inside Out*:

```markdown
One of the miseries of life is that everybody names things a little bit wrong.
```

Roger Zelazny puts it far more poetically in his novel *Lords of Light* in one of the most powerful passages I've ever read:

```markdown
Sam sat with his eyes closed for several minutes, then said softly:

"I have many names, and none of them matter." He opened his eyes slightly then, but he did not 
move his head. He looked upon nothing in particular.

"Names are not important," he said. "To speak is to name names, but to speak is not important.

"A thing happens once that has never happened before. Seeing it, a man looks on reality. He cannot
tell others what he has seen. Others wish to know, however, so they question him saying, 'What is 
it like, this thing you have seen?'

"So he tries to tell them. Perhaps he has seen the very first fire in the world. He tells them, 
'It is red, like a poppy, but through it dance other colors. It has no form, like water, flowing 
everywhere. It is warm, like the sun of summer, only warmer. It exists for a time on a piece of wood,
and then the wood is gone, as though it were eaten, leaving behind that which is black and can be 
sifted like sand. When the wood is gone, it too is gone.'

"Therefore, the hearers must think reality is like a poppy, like water, like the sun, like that which
eats and excretes. They think it is like to anything that they are told it is like by the man who has 
known it. But they have not looked upon fire. They cannot really *know* it. They can only know *of* it.

"But fire comes again into the world, many times. More men look upon fire. After a time, fire is 
as common as grass and clouds and the air they breathe. They see that, while it is like a poppy, 
it is not a poppy, while it is like water, it is not water, while it is like the sun, it is not 
the sun, and while it is like that which eats and passes wastes, it is not that which eats and
passes wastes, but something different from each of these apart or all of these together. So they
look upon this new thing and they make a new word to call it. They call it 'fire.'

"If they come upon one who still has not seen it and they speak to him of fire, he does not know 
what they mean. So they, in turn, fall back upon telling him what fire is like. As they do, they 
know from their own experience that what they are telling him is not the truth, but only a part of it.
They know that this man will never know reality from their words, though all the words in the world 
are theirs to use. He must look upon the fire, smell of it, warm his hands by it, stare into its heart,
or remain forever ignorant.

"Therefore, 'fire' does not matter, 'earth' and 'air' and 'water' do not matter. 'I' do not matter.
No word matters.

"But man forgets reality and remembers words. The more words he remembers, the cleverer do his fellows
esteem him. He looks upon the great transformations of the world, but he does not see them as they were
seen when man looked upon reality for the first time. Their names come to his lips and he smiles as he 
tastes them, thinking he knows them in the naming. The thing that has never happened before is still 
happening. It is still a miracle. The great burning blossom squats, flowing, upon the limb of the world,
excreting the ash of the world, and being none of these things I have named and at the same time 
all of them, and *this* is reality—the Nameless.”
```

Nielsen pushes back against this attitude insofar as it goes too far:

```markdown
It's a good story. But it goes too far: names do matter. Maybe not as much as the know-it-all kid
thought, and they're not usually a deep kind of knowledge. But they're the foundation that allows 
you to build up a network of knowledge.

This trope that names don't matter was repeatedly drilled into me during my scientific training.
When I began using Anki, at first I felt somewhat silly putting questions about names for things 
into the system. But now I do it enthusiastically, knowing that it's an early step along the way 
to understanding.

Anki is useful for names of all kinds of things, but I find it particularly helpful for non-verbal
things. For instance, I put in questions about artworks, like: “What does the artist Emily Hare's 
painting Howl look like?”.

I put that question in for two reasons. The main reason is that I like to remember the experience of
the painting from time to time. And the other is to put a name to the painting.( Actually, a better
question for that is to be shown the painting and asked what its name is.) If I wanted to think more 
analytically about the painting – say, about the clever use of color gradients – I could add more
detailed questions. But I'm pretty happy just committing the experience of the image to memory.

Friends sometimes complain that many books are over-padded essays. Perhaps a benefit of such padding
is that it enforces an Anki-like spaced repetition, since readers take weeks to read the book. This
may be an inefficient way to memorize the main points, but is better than having no memory of the 
book at all.
```

Names are hard to get right too. Carlos Bueno, [The Mature Optimization Handbook](http://carlos.bueno.org/optimization/mature-optimization.pdf):

```markdown
Naming things has been half-jokingly called the second-hardest
problem in computer science. Anyone can name the things they
build anything they want, and they do. That’s the problem. The
computer doesn’t care about names. They’re for the benefit of
humans so there are no technical arguments to fall back on.
Excess jargon is the sawdust of new technology, and the mental
friction it imposes is scandalous. Whoever figures out how to
sweep it up does a service to mankind.

Take the word we’ve been using for intervals of real time,
“walltime”. Perhaps it’s more properly called “duration”. Time
spent on the CPU could be called “cpu_duration”; time spent
waiting for the database “db_duration” and so on. And why
not be explicit about the units, eg “duration_cpu_usec”? If you
have a strong preference either way, I humbly suggest that it’s a
matter of taste and not universal truth. Walltime sounds more
natural to me because that was the jargon I was first exposed
to. But who actually has clocks on their walls any more? The
term is as dated as “dialing” a phone number.

For that matter, take instructions. Now that we’ve decided
to round to the nearest thousand, is the name “instructions”
misleading? Is “kilo_instructions” too cumbersome to type? Is
“kinst” too obscure to remember?

This all might sound mincing and pedantic, but a) you have
to pick something and b) you’ll have to deal with your choices
for a long time. So will the people who come after you. You
can’t clean up the world but you can mind your own patch.

Even an ugly scheme, if it’s consistently ugly, is better than
having to remember that walltime here is duration over there
and response_time_usec somewhere else. Whatever ontology
you build, *write it down* somewhere it will be noticed. Explain
what the words mean, the units they describe, and be firm about
consistency
```

<a name="#cognitive-science"></a>
## Cognitive science
([overview](#overview))

From Cosma Shalizi's [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*:

```markdown
Cognition, whether human, animal or artificial, is a kind of information-processing, taking place, in our
case, in the brain. The information takes the form of representations (of sensory stimuli, of states of 
parts of the world, of facts, of relations, of possible states of parts of the world, of courses of action,
or what-not). The processing consists of the transformation of these representations according to definite,
though perhaps stochastic, rules. (So far, we have not excluded the connectionist heretics.) An immense amount
of information-processing takes place subconsciously, particularly that which turns raw irritation of the 
afferent nerves into useful perceptions of the world about us, and turns volitions into raw stimulations of
the efferent nerves. To recognize a dagger you see before you involves a lot of computational work; some people,
having been wounded in the parts of the brain which do the computations, cannot. At least at some level of 
abstraction, the representations and transformations are usefully, conveniently and/or accurately thought of as
structures of symbols and as algorithms, respectively. (This does rule out the connectionists.) The algorithms 
may be (or, if you like, instantiate) rules of inference, or rules for producing new representations from old 
ones more generally ("production systems"). 

One particularly well-studied kind of cognition, sometimes taken as the paradigm of all cognition, is
problem-solving, conceived of as turning a representation of the problem, step by step, into a representation of
a solution, or something close enough to a solution to satisfy the problem-solver. (Expertise in solving a kind 
of problem consists in knowing good algorithms to apply to it, being able to represent a problem in a way which
makes it easy to solve, and being able to recognize a solution when you have one.) In principle, all this takes 
place in the brain; in practice, we can fake a larger and more accurate memory than we possess by either using 
external symbols, or by taking advantage of regular and persistent parts of our environment.
```

<a name="#external-brain"></a>
## External brain 
([overview](#overview))

Aids to memory have been opposed for millennia. Here's Socrates, in Plato's Phaedrus, circa 370 BCE, bemoaning the deleterious effects of the new technology of "writing":

```markdown
If men learn this, it will implant forgetfulness in their souls; they will cease to exercise
memory because they rely on that which is written, calling things to remembrance no longer 
from within themselves, but by means of external marks. What you have discovered is a recipe 
not for memory, but for reminder. And it is no true wisdom that you offer your disciples, but
only its semblance, for by telling them of many things without teaching them you will make 
them seem to know much, while for the most part they know nothing, and as men filled, not with
wisdom, but with the conceit of wisdom, they will be a burden to their fellows.
```

Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally) and the comments are great. It's from 2009; a decade hence, they're more applicable than ever. Here's some neat quotes.

```markdown
How many of the things you "know" do you have memorized?

Do you remember how to spell all of those words you let the spellcheck catch?  Do you remember
what fraction of a teaspoon of salt goes into that one recipe, or would you look at the list of
ingredients to be sure?  Do you remember what kinds of plastic they recycle in your neighborhood,
or do you delegate that task to a list attached with a magnet to the fridge?

If I asked you what day of the month it is today, would you know, or would you look at your
watch/computer clock/the posting date of this post?

Before I lost my Palm Pilot, I called it my "external brain".  It didn't really fit the description; 
with no Internet access, it mostly held my contact list, class schedule, and grocery list.  And a 
knockoff of Minesweeper.  Still, in a real enough sense, it remembered things for me.The vast arena 
of knowledge at our fingertips in the era of constant computing has, ironically, brought it farther
away.  It seems nearer: after all, now, if you are curious about Zanzibar, Wikipedia is a few 
keystrokes away.  Before the Internet, you'd probably have been looking at a trip to the library and
a while wrestling with the card catalog; and that would be if you lived in an affluent, literate society. 
If you didn't, good luck knowing Zanzibar exists in the first place!

But if you were an illiterate random peasant farmer in some historical venue, and you needed to know
the growing season of taro or barley or insert-your-favorite-staple-crop-here, Wikipedia would have
been superfluous: you would already know it.  It would be unlikely that you would find a song lyrics
website of any use, because all of the songs you'd care about would be ones you really knew, in the
sense of having heard them sung by real people who could clarify the words on request, as opposed to
the "I think I heard half of this on the radio at the dentist's office last month" sense.
```

Per Kaj Sotala, the distributed cognition paradigm of research is all about exploring the idea behind "externalizing" or "outsourcing" our brains to the environment. An excerpt from [this primer](http://www.isr.uci.edu/~jpd/classes/ics234bs03/13-HollanEtAl-TOCHI.pdf):

```markdown
In several environments we found subjects using space to simplify choice by creating arrangements 
that served as heuristic cues. For instance, we saw them covering things, such as garbage disposal 
units or hot handles, thereby hiding certain affordances or signaling a warning and so constraining
what would be seen as feasible. At other times they would highlight affordances by putting items 
needing immediate attention near to them, or creating piles that had to be dealt with. We saw them lay
down items for assembly in a way that was unambiguously encoding the order in which they were to be 
put together or handed off. That is, they were using space to encode ordering information and so were 
off-loading memory. These are just a few of the techniques we saw them use to make their dedecision
problems combinatorially less complex.

We also found subjects reorganizing their workspace to facilitate perception: to make it possible to 
notice properties or categories that were not noticed before, to make it easier to find relevant items,
to make it easier for the visual system to track items. One subject explained how his father taught him 
to place the various pieces of his dismantled bicycle, many of which were small, on a sheet of newspaper.
This made the small pieces easier to locate and less likely to be kicked about. In videos of cooking we 
found chefs distinguishing otherwise identical spoons by placing them beside key ingredients or on the 
lids of their respective saucepans, thereby using their positions to differentiate or mark them. We found
jigsaw puzzlers grouping similar pieces together, thereby exploiting the capacity of the visual system to
note finer differences between pieces when surrounded by similar pieces than when surrounded by different
pieces.

Finally, we found a host of ways that embodied agents enlist the world to perform computation for them. 
Familiar examples of such off-loading show up in analog computations. When the tallest spaghetti noodle
is singled out from its neighbors by striking the bundle on a table, a sort computation is performed by 
using the material and spatial properties of the world. But more prosaically we have found in laboratory 
studies of the computer game Tetris that players physically manipulate forms to save themselves 
computational effort [Kirsh 2001; Kirsh and Maglio 1995]. They modify the environment to cue recall, to 
speed up identification, and to generate mental images faster than they could if unaided. In short, they 
make changes to the world to save themselves costly and potentially error-prone computations.
```

Sotala takes the distributed cognition idea-seed and runs away with it:

```markdown
Information processing doesn't only happen inside brains and computers. The paradigm of distributed 
cognition studies human societies as information-processing systems, with individuals being parts of
the larger system. For instance, the operation of an airliner cockpit's crew has been studied from 
this perpective [1]. For a flight to proceed without trouble, the different crew members need to be 
aware of information relating to their areas of responsibility at any given moment. If the crew is 
experienced and well trained, they'll constantly stay up to date by e.g. simply listening to other 
crew members converse with flight control. As flight control informs the captain of a new flight 
altitude, the rest of the pilots begin to adjust the altitude even while the captain is still
finishing up the communication. The cockpit functions as a unified system, and relevant information 
is propagated to wherever needed. Several crew members hearing the same information also allows for
error correction. If the message is unclear and the captain can't make out flight control's words, 
he can ask the others for clarification. The co-pilot answers the captain's query: even though one 
part of the system has failed to absorb the information received from outside the system, the same 
information has been stored in another part, which may then attempt to re-send it where needed.

Several other fields have been studied in the same manner, ranging from a child's language learning 
[2] to creativity [3]. A child doesn't learn language by itself and in a vacuum, but via interaction
with adults and older children. Creativity, on the other hand, requires common, shared "idea resources"
which individuals may use to come up with their own inventions and then give them back for others to 
refine further. Another theory of innovation considers inventions to be responses to problems encountered
by the community. Things such as bad laws or ineffective ways of doing things show up in community, and 
are considered problems by its members. This leads the community - the system - into a need state,
mobilizing its members to seek solutions until they're found.

One central idea is that social communities are cognitive architectures the same way that individual
minds are [4]. The argument is as follows. Cognitive processes involve trajectories of information 
(transmission and transformation), so the patterns of these information trajectories, if stable,
reflect some underlying cognitive architecture. Since social organization - plus the structure added
by the context of activity - largely determines the way information flows through a group, social 
organization may itself be viewed as a form of cognitive architecture.

[1] Hutchins, E. & Klausen, T. (1995) Distributed Cognition in an Airline Cockpit.

[2] Spurrett, D. & Cowley, S.J. (2004) How to do things without words: infants, utterance-activity and
distributed cognition. Language Sciences, 6, 443-466.

[3] Miettinen, R. (2006) The Sources of Novelty: A Cultural and Systemic View of Distributed Creativity.
Creativity and Innovation Management. Vol. 15, no. 2.

[4] Hollan, J. & Hutchins, E. & Kirsh, D. (2000) Distributed Cognition: Toward a New Foundation for 
Human-Computer Interaction Research. ACM Transactions on Computer-Human Interaction. Vol 7, no. 2.
```

In other words, says Sotala, "probably nothing to be worried about. Just normal human use of the environment."

From Andy Clark's book *Supersizing the Mind*, a [comment by trent](https://nforum.ncatlab.org/discussion/1927/unpopularity-of-category-theory/) I found on the nForum showing how Feynman understood the notion of the external brain:

```markdown
When historian Charles Weiner found pages of Nobel Prize-winning physicist Richard Feynman’s
notes, he saw it as a “record” of Feynman’s work. Feynman himself, however, insisted that the
notes were not a record but the work itself. In Supersizing the Mind, Andy Clark argues that 
our thinking doesn’t happen only in our heads but that “certain forms of human cognizing include
inextricable tangles of feedback, feed-forward and feed-around loops: loops that promiscuously 
criss-cross the boundaries of brain, body and world.” The pen and paper of Feynman’s thought
are just such feedback loops, physical machinery that shape the flow of thought and enlarge the
boundaries of mind.
```

For context, this whole thread was about trying to figure out why category theory was unpopular. Trent adds:

```markdown
I see theory as helping one adopt elegant solutions like that, and, more generally I think that
the more physicists understand the role things seemingly outside of the mind such as notation
play in cognition, the more they will see the importance of work in mathematical physics which 
places physics in the most elegant possible notation. It’s not just theory addicts trying to 
justify their work when they say that it aids in problem solving, it’s how cognition works.
```

Cosma Shalizi is a great read for collective cognition. Here's the introduction he wrote to the [Collective Cognition:
Mathematical Foundations of Distributed Intelligence](http://csc.ucdavis.edu/~dynlearn/colcog/description.htm) workshop he co-organized awhile back, giving modern science and markets as examples:

```markdown
Many forms of individual cognition are enhanced by communication and collaboration with other 
intelligent agents. We propose to call this collective cognition, by analogy with the well known 
concept of collective action. People (and other intelligent agents) often "think better" in groups
and sometimes think in ways which would be simply impossible for isolated individuals. Perhaps the most
spectacular and important instance of collective cognition is modern science. An array of formal 
organizations and informal social institutions also can be considered means of collective cognition. For
instance, Hayek famously argued that competitive markets effectively calculate an adaptive allocation of
resources that could not be calculated by any individual market-participant. 

Hitherto the study of collective cognition has been qualitative, philosophical, even at times anecdotal.
Only recently, we believe, have the tools fallen into place to initiate a rigorous, quantitative science of
collective cognition. Moreover, it appears that soon there will be a real practical need for such a science.
```

A bit more on collective cognition:

```markdown
Collective cognition involves an interaction among three elements-the individual abilities of the agents,
their shared knowledge, and their communication structure. Cognitive collectives therefore resemble many
other complex systems which are collectives of goal-directed processes. Typically, the individual processes
know little of the detailed dynamics and the state of the overall system and, therefore, must use adaptive
techniques to achieve their goals. There are many naturally occurring examples, including human economies, 
human organizations, ecosystems, and even spin glasses. In addition, it has recently become clear that many
of the engineered systems of the future must be of this type, with massively distributed computational 
elements. There is optimism in the multi-agent system (MAS) field that widely applicable solutions to large,
distributed problems are close at hand. Some experts now believe that, in the information and
telecommunications networks of today, we have nascent examples of artificial cognitive collectives.
```

Collective cognition touches on a dazzling variety of fields: 

```markdown
Cognitive science
Situated agents
Emergent computation
Bounded rationality
Institutional economics
Economies of information
Evolutionary game theory
Cognitive ethology
Collective phenomena in physics
Neural computation and distributed representations
Distributed computation
Mechanism design
General equilibrium theory
Population biology
Robustness
Swarm intelligences
Reinforcement learning
Adaptive control
Cultural evolution
Cognitive sociology and the sociology of science
Telecommunications data routing
```

Some basic (i.e. foundational) questions of interest, at least for Cosma's workshop:

```markdown
Interaction between communication structure and cognitive performance.

How are knowledge dynamics and communication structure related? How are computational structure and 
communication structure related?

Why do some collectives not support much cognition and others support substandard or even pathological
forms?

When, and to what extent, can we attribute cognition, or at least computation, to the collective as 
such, rather than its individual members?

How much do we need to know about individual cognition to do adequate models of collective intelligence?

How can the capacity for collective cognition evolve?

Collective-action problems: Why should an agent contribute to the collective? Conversely, does the 
collective-cognition perspective shed any light on collective action in the conventional sense?

Incentive design: When we cannot directly control the goals individual agents, how can we still configure 
the system so that each agent has incentives to pursue a goal that is both readily achievable and good
for the overall collective?

How useful is fiat money in collectives whose agents are not human beings?

When and how should agents be induced to form teams?

When and how does agent heterogeneity enhance collective cognitive performance? Are "disagreement"
and "controversy" among agents always bad, or sometimes desirable for the collective?

How is robustness of behavior against external perturbation related to quality of behavior?
```

Cosma writes about collective cognition in a more fun-to-read way in his [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*. It begins like so:

```markdown
Human beings coordinate their actions to do things which would be hard or impossible for them 
individually. This is not a particularly recondite fact, and the recognition of it is ancient; 
it is in the fifth book of Lucretius's De Rerum Natura, for instance. It was a commonplace of 
the Enlightenment, that most sociable age, and the philosophes were even, it seems, the first to 
realize that thinking, too, can be a collective activity, one conducted and amplified by social
groups --- which is not to say that societies have thoughts. ...

The nineteenth century, and to a lesser degree this one, have witnessed a dramatic expansion in 
the numbers of us engaged in administration, bureaucracy, management, oversight --- that is to say,
in formally-organized tasks of collective cognition and control. We did not invent bureaucracy, 
the mainstay of the ancient empires, but we're much, much better at it than they were. A random 
American town of 200,000 --- Piffleburg, WI, let us say --- will have police, a rescue squad, a fire
department, a hospital, universal schooling, several large factories, insurance offices, banks, a 
community college, a public library with several thousand volumes at least, a post office, public 
utilities, political parties, garbage collection, paved and usable roads everywhere, mercantile 
connections stretching across the country, and, with some luck, unions. These are corrupt, 
inefficient institutions which work poorly; every election, Piffleburg's citizens mutter something 
like "what do we pay taxes for anyway?" Yet to run any one of these institutions at the level of 
honesty, efficiency and efficacy which makes Piffleburg grumble would have demanded the full powers
and attention of even the ablest Roman propraetor or T'ang magistrate. That all of those institutions,
plus the ones not restricted to a single city, could be run at once, and while governed by a very 
ordinary slice of common humanity, would have seemed to such officials flatly impossible.
```

So why are we, as Cosma puts it, "so much better at collective endeavors than the ancients"? This is fairly easy to address:

```markdown
To a first approximation, the answer is: brute force and massive literacy. We teach nearly everyone to
read and write, and to do it, by historical standards, at a high level. This lets us staff large
bureaucracies (by some estimates, over 40% of the US workforce does data-handling), which lets us run an
industrial economy (the trains run on time), which makes us rich enough to afford to educate everyone 
and keep them in bureaucratic employment, with some surplus left over to expand the system. 

 This would do us no good if our ideas of administration were as shabby as those of our ancestors in the 
 dark ages, but they're not: we inherited those of the ancient empires, and have had quite a while to 
 improve upon them (and improvements are made easier and faster by the large number of administrators and
 the high standard of literacy). Among the improvements are many techniques (standardized procedures,
 standardized parts, standardized credentials and jobs, explicit qualifications for jobs and goods, files,
 standardized categories) and devices (forms, punch cards, punch card tabulators, adding machines, card
 catalogs, and, recently, computers) for making the administration of people and things easier.
```

Now while this is all splendid, it's "in the realm of technique"; when it comes to theory, nobody has any real idea how to explain what's going on:

```markdown
We don't really have a good theory about how collective action and cognition work, when and why they do, 
how they can be made to work better, why they fail, what they can and cannot accomplish, and so forth. 

Intellectually, these are large, tempting problems; technologically, they have obvious relevance to the 
design of parallel and distributed computers; economically, they could mean real money, not just billions;
and, in general, it'd be nice to know what it is we've gotten ourselves into.
```

Enter Edwin Hutchins, who conducted fieldwork studying navigation on a US navy ship based in San Diego with the problem of theoretically grounding collective cognition in mind. His fieldwork is interesting:

```markdown
Hutchins's field evidence consists of very detailed records, taken in the early 1980s, on the performance 
of the navigation crew of a helicopter carrier ship he calls the Palau, principally as they fix their 
location and plot their course near shore. The way it worked, in those pre-GPS days, was, roughly, this: 
three land-marks on shore, of known location on the navigation charts, would be selected by the main
person in charge, the "quartermaster of the watch." Then they'd "take bearings" on these, i.e. find the
orientation of the line from the landmark to the ship. These lines would be drawn on the chart. Now, it's 
an elementary result in Euclidean geometry that any two lines meet at a single point (unless they're parallel);
three lines form a triangle (unless they all meet at the same point). Somewhere within that triangle is the 
ship: this fixes the current position. The position of the ship at the next fix is estimated by "dead
reckoning," which is simply taking the current position and heading of the ship, and its planned speed, and 
extrapolating forward along the line of its heading. A single person can do this, if he's not too rushed.
Close to shore, the Navy gets worried, and demands fixes every few minutes, so the task gets broken down: 
naval flunkies take the bearings, a different flunky tells them when to take the bearings, and so on. There's 
a fairly rigid protocol for coordinating all these actions, and for communicating their results in a usable
form, and specialized instruments for making the job easier.

So, what does all this actually show? Well, that cognitive tasks can get spread over several people; that, 
in this instance, those who do tasks which require input from other people are generally superior to them in
rank; that the official job descriptions do not quite correspond to what people do; that people have a hard 
time believing things which are strange to them, and tend to ask those who report them questions along the 
lines of "Are you sure?"; that, if you don't know what something looks like, a verbal description can be very
unhelpful; that the right tools can make the job simpler; and that building computation into tools can make 
the job simpler for people, since it's easier to use a slide-rule than take a sine or a logarithm in your head;
that, if you can't talk about something, it's hard to make plans with someone else about it. There's more, but 
they're along the same lines.

These are not exactly earth-shaking results; in fact, they're about what common sense says to us. This doesn't 
make it useless to check them, since common sense is so often wrong; but even then, Hutchins has checked them
against the performance of one task (navigation; more particularly, location fixing), in one set of social 
groups (a couple of ships of the US Navy) --- ones where the social system is designed, and has received several
centuries of re-design from people whose common sense more or less agrees with the above. (One wonders if they
did things differently aboard the Potemkin.)
```

Unfortunately his conclusions are rubbish.

<a name="#procedural-vs-declarative-memory"></a>
## Procedural vs declarative memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), talking about procedural vs declarative memory in the context of using Anki flashcards:

```markdown
There's a big difference between remembering a fact and mastering a process. For instance, while 
you might remember a Unix command when cued by an Anki question, that doesn't mean you'll recognize
an opportunity to use the command in the context of the command line, and be comfortable typing it 
out. And it's still another thing to find novel, creative ways of combining the commands you know, 
in order to solve challenging problems.

Put another way: to really internalize a process, it's not enough just to review Anki cards. You need
to carry out the process, in context. And you need to solve real problems with it.
```

From Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally):

```markdown
Propositional knowledge is being gradually supplanted by the procedural.  You need only know *how to find*
information, to be able to use it after a trivial delay.  This requires some snippet of propositional data
- to find a song lyric, you need a long enough string that you won't turn up 99% noise when you try to 
Google it! - but mostly, it's a skill, not a fact, that you need to act like you knew the fact.

It's not clear to me whether this means that we should be alarmed and seek to hone our factual memories...
or whether we should devote our attention to honing our Google-fu, as our minds gradually become
server-side operations.
```

<a name="#augmenting-long-term-memory"></a>
## Augmenting long-term memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), which is an all-around great essay you should read in its entirety. One of its theses:

```markdown
Many people treat memory ambivalently or even disparagingly as a cognitive skill: for instance,
people often talk of “rote memory” as though it's inferior to more advanced kinds of understanding.
I'll argue against this point of view, and make a case that memory is central to problem solving 
and creativity.
```

To expand on his point:

```markdown
It's a mistake to underestimate the importance of memory. I used to believe such tropes about the 
low importance of memory. But I now believe memory is at the foundation of our cognition.

There are two main reasons for this change, one a personal experience, the other based on evidence
from cognitive science.

Let me begin with the personal experience.

Over the years, I've often helped people learn technical subjects such as quantum mechanics. Over 
time you come to see patterns in how people get stuck. One common pattern is that people think 
they're getting stuck on esoteric, complex issues. But when you dig down it turns out they're having 
a hard time with basic notation and terminology. It's difficult to understand quantum mechanics when
you're unclear about every third word or piece of notation! Every sentence is a struggle.

It's like they're trying to compose a beautiful sonnet in French, but only know 200 words of French.
They're frustrated, and think the trouble is the difficulty of finding a good theme, striking 
sentiments and images, and so on. But really the issue is that they have only 200 words with which to
compose.

My somewhat pious belief was that if people focused more on remembering the basics, and worried less 
about the “difficult” high-level issues, they'd find the high-level issues took care of themselves.

But while I held this as a strong conviction about other people, I never realized it also applied to
me. And I had no idea at all how strongly it applied to me. Using Anki to read papers in new fields 
disabused me of this illusion. I found it almost unsettling how much easier Anki made learning such 
subjects. I now believe memory of the basics is often the single largest barrier to understanding. If
you have a system such as Anki for overcoming that barrier, then you will find it much, much easier 
to read into new fields.

This experience of how much easier Anki made learning a new technical field greatly increased my 
visceral appreciation for the importance of memory.
```

<a name="#Yegge-on-memory"></a>
## Yegge on memory
([overview](#overview))

From one of his more memorable posts, [Done and gets things smart](https://steve-yegge.blogspot.com/2008/06/done-and-gets-things-smart.html):

```markdown
So we all think we're smart for different reasons. Mine was memorization. Smart, eh? 
In reality I was just a giant, uncomprehending parrot. I got my first big nasty surprise
when I was in the Navy Nuclear Power School program in Orlando, Florida, and I was 
setting historical records for the highest scores on their exams. The courses and exams 
had been carefully designed over some 30 years to maximize and then test "literal 
retention" of the material. They gave you all the material in outline form, and made you
write it in your notebook, and your test answers were graded on edit-distance from the 
original notes. (I'm not making this up or exaggerating in the slightest.) They had set 
up the ultimate parrot game, and I happily accepted. I memorized the entire notebooks 
word-for-word, and aced their tests.

They treated me like some sort of movie star — that is, until the Radar final lab exam in
electronics school, in which we had to troubleshoot an actual working (well, technically,
not-working) radar system. I failed spectacularly: I'd arguably set another historical 
record, because I had no idea what to do. I just stood there hemming and hawing and pooing
myself for three hours. I hadn't understood a single thing I'd memorized. Hey man, I was 
just playing their game! But I lost. I mean, I still made it through just fine, but I lost
the celebrity privileges in a big way.

Having a good memory is a serious impediment to understanding. It lets you cheat your way
through life. I've never learned to read sheet music to anywhere near the level I can play
(for both guitar and piano.) I have large-ish repertoires and, at least for guitar, good 
technique from lots of lessons, but since I could memorize the sheet music in one sitting,
I never learned how to read it faster than about a measure a minute. (It's not a 
photographic memory - I have to work a little to commit it to memory. But it was a lot 
less work than learning to read the music.) And as a result, my repertoire is only a 
thousandth what it could be if I knew how to read.

My memory (and, you know, overall laziness) has made me musically illiterate.

But when you combine the Dunning-Kruger effect (which affects me just as much as it does 
you) with having one or two things I've been good at in the past, it's all too easy to 
fall into the trap of thinking of myself as "smart", even if I know better now. All you 
have to do, to be "smart", is have a little competency at something, anything at all, just
enough to be dangerous, and then the Dunning-Kruger Effect makes you think you're God's 
gift to that field, discipline, or what have you.
```

<a name="#wisdom"></a>
## Wisdom
([overview](#overview))

Scott Alexander on wisdom in [Does age bring wisdom?](https://slatestarcodex.com/2017/11/07/does-age-bring-wisdom/) resonated pretty strongly with me:

```markdown
We’ve been talking recently about the high-level frames and heuristics that organize other 
concepts. They’re hard to transmit, and you have to rediscover them on your own, sometimes
with the help of lots of different explanations and viewpoints (or one very good one). 
They’re not obviously apparent when you’re missing them; if you’re not ready for them, they
just sound like platitudes and boring things you’ve already internalized.

Wisdom seems like the accumulation of those, or changes in higher-level heuristics you get
once you’ve had enough of those. I look back on myself now vs. ten years ago and notice
I’ve become more cynical, more mellow, and more prone to believing things are complicated. 
For example:

1. Less excitement about radical utopian plans to fix everything in society at once
2. Less belief that I’m special and can change the world
3. Less trust in any specific system, more resignation to the idea that anything useful
requires a grab bag of intuitions, heuristics, and almost-unteachable skills.
4. More willingness to assume that other people are competent in aggregate in certain
ways, eg that academic fields aren’t making incredibly stupid mistakes or pointlessly
circlejerking in ways I can easily detect.
5. More willingness to believe that power (as in “power structures” or “speak truth to
power”) matters and infects everything.
6. More belief in Chesterton’s Fence.
7. More concern that I’m wrong about everything, even the things I’m right about, on 
the grounds that I’m missing important other paradigms that think about things completely 
differently.
8. Less hope that everyone would just get along if they understood each other a little
better.
9. Less hope that anybody cares about truth (even though ten years ago I would have
admitted that nobody cares about truth).

All these seem like convincing insights. But most of them are in the direction of elite 
opinion. There’s an innocent explanation for this: intellectual elites are pretty wise, 
so as I grow wiser I converge to their position. But the non-innocent explanation is that 
I’m not getting wiser, I’m just getting *better socialized*. Maybe in medieval Europe, the 
older I grew, the more I would realize that the Pope was right about everything.
```

A particular example:

```markdown
I’m pretty embarassed by Parable On Obsolete Ideologies, which I wrote eight years ago. 
It’s not just that it’s badly written, or that it uses an ill-advised Nazi analogy. It’s
that it’s an impassioned plea to jettison everything about religion immediately, because
institutions don’t matter and only raw truth-seeking is important. If I imagine myself 
entering that debate today, I’d be more likely to take the opposite side. But when I read 
Parable, there’s…nothing really wrong with it. It’s a good argument for what it argues for.
I don’t have much to say against it. Ask me what changed my mind, and I’ll shrug, tell you
that I guess my priorities shifted. But I can’t help noticing that eight years ago, New 
Atheism was really popular, and now it’s really unpopular. Or that eight years ago I was in
a place where having Richard Dawkins style hyperrationalism was a useful brand, and now I’m
(for some reason) in a place where having James C. Scott style intellectual conservativism 
is a useful brand. A lot of the “wisdom” I’ve “gained” with age is the kind of wisdom that
helps me channel James C. Scott instead of Richard Dawkins; how sure am I that this is the
right path?
```

This is the "money quote", the whole reason I started this subheading:

```markdown
Sometimes I can almost feel this happening. First I believe something is true, and say so.
Then I realize it’s considered low-status and cringeworthy. Then I make a principled decision 
to avoid saying it – or say it only in a very careful way – in order to protect my reputation 
and ability to participate in society. Then when other people say it, I start looking down on
them for being bad at public relations. Then I start looking down on them just for being low-
status or cringeworthy. Finally the idea of “low-status” and “bad and wrong” have merged so 
fully in my mind that the idea seems terrible and ridiculous to me, and I only remember it’s
true if I force myself to explicitly consider the question. And even then, it’s in a 
condescending way, where I feel like the people who say it’s true deserve low status for not
being smart enough to remember not to say it. This is endemic, and I try to quash it when I 
notice it, but I don’t know how many times it’s slipped my notice all the way to the point 
where I can no longer remember the truth of the original statement.
```

Are old people really wiser? Why do they sound so crankily conservative? A model:

```markdown
And if I accept my intellectual changes as “gaining wisdom”, shouldn’t I also believe that 
old people are wiser than I am? And old people mostly seem to go around being really 
conservative and saying that everything was better in the old days and the youth are corrupt
and Facebook is going to be the death of us. I could model this as two different processes –
a real wisdom-related process that ends exactly where I am now, plus a false rose-colored-
glasses-related process that ends with your crotchety great-uncle talking about how things 
have been going downhill since the war – but that’s a lot of special pleading. I remember 
when I was twenty, I thought the only reason adults were less utopian than I was, was
because of their hidebound rose-colored self-serving biases. Pretty big coincidence that I 
was wrong then, but I’m right about everyone older than me *now.*
```

John "Erisology" Nerst responds:

```markdown
I think there could be selection effect. Not all people get wiser as they age and many hit 
a ceiling at some time. Maybe those are the ones most likely to make their opinions heard 
(I mean, it’s hardly the case that the wisest are the loudest among the younger population
either). And the really wise ones stay silent because their wisdom has become impossible to
communicate?

It reminds me of the quote from Julian Barnes’ Staring at the Sun:

*Everything you wanted to say required a context. If you gave the full context, people
thought you a rambling old fool. If you didn’t give the context, people thought you a
laconic old fool.”*
```

Worst-case scenario -- wisdom as "NMDA reception function change with age":

```markdown
There’s one more possibility that bothers me even worse than the socialization or
traumatization theory. I’m going to use science-y sounding terms just as an example, but I 
don’t actually think it’s this in particular – we know that the genes for liberal-conservative 
differences are mostly NMDA receptors in the brain. And we know that NMDA receptor function 
changes with aging. It would be pretty awkward if everything we thought was “gaining wisdom
with age” was just “brain receptors consistently functioning differently with age”. If we
were to find that were true – and furthermore, that the young version was intact and the older 
version was just the result of some kind of decay or oxidation or something – could I trust 
those results? Intuitively, going back to earlier habits of mind would feel inherently 
regressive, like going back to drawing on the wall with crayons. But I don’t have any *proof.*
```

This is pithily captured in the quote (attribution unknown, phrasing by "Keith"):

```markdown
He who isn’t radical as a youth has no heart. 
And he who isn’t conservative as an adult has no brain.
```

What this looks like in science -- the "grand old academics" phenomenon:

```markdown
I’ve noticed a vaguely related trend in science:

you get a number of grand old academics, the kind of people who continue to hang out at the
institution long after they’re officially retired who are an absolute goldmine for various 
minutiae of their subject.

They’ve tried many approaches over the decades and can warn you about dead ends….

but they also often have an overabundance of cynicism.

Often they remember that approach X didn’t work, they may not remember the exact details as 
to why. their memory of the event gets pared down to “that’s a dead end”… and then at some 
point a new generation of grad students come along and eventually someone ignores the advice 
that X is a dead end and it turns out that in the 30 years that have passed the things that 
made X a dead end no longer apply. The sequencing methods can now read through long-repeats 
or the chemistry used for some step is improved or some background piece of knowledge has
been added to the field that now allows people to power through the former roadblock.

Is that wisdom? Knowing lots of dead ends can be useful and can save resources…but it can also
be maladaptive as the world changes around you.
```

Commenter Deej's response:

```markdown
we need to distinguish between individual people changing their views as they get older, 
and the centre grounds shifting as younger people are more liberal than their predecessors.
My feeling is that for economic issues people’s individual views probably do shift 
rightwards as they get older, but for social issues it’s seems likely that it’s the centre
ground that’s moving. Although for today’s more exterme identity politics left youths that
might change.

Third. I think it’s worth distinguising between types of people and how their views might 
change. People who are properly interested in politics, for example, are – I would exepct 
– much more likely top have big changes in their views, than those that aren’t. See ex-
trotskists now in the Tory party or at least Blairite in the UK. I expect that the people
interested in politics changes are likely to be relatively more driven by learning from 
experience and reflective thought than people just slowly change their views over time from,
for example, a bit left to a bit right of centre. Or left to a bit left less left, right to
a bit less right etc.
```

Somewhat relevant are these quotes from Robin Hanson's *Age of Em*:

```markdown
Controlling for birth cohort, individual productivity does not peak until at least age 60,
and may never peak (Cardoso et al. 2011; Göbel and Zwick 2012). […] Also, any falling
productivity after age 60 for humans today may be primarily caused by declining physical
abilities, not declining mental abilities

Today, our abilities at different kinds of tasks peak at different ages. For example, raw 
cognitive processing peaks in late teens, learning and remembering names in early 20s, 
short-term memory about age 30, face recognition in early 30s, social understanding about
age 50, and word knowledge above age 65 (Hartshorne and Germine 2015).
```
