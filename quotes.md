*[Word count](https://wordcounter.net/): 167,600*

## What is this?

This is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping.

I also really, *really* hate experiences of the [Jeremy Bentham class](#Jeremy-bentham); this document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [Amazing people](#amazing-people)
   1. [Alexander Grothendieck](#alexander-grothendieck)
   2. [Andrey Kolmogorov](#Andrey-Kolmogorov)
   2. [Butler Lampson](#Butler-Lampson)
   2. [Charlie Fefferman](#charlie-fefferman)
   2. [Donald Trump](#donald-trump) as autistic child with real estate development special interest 
   2. [Ed Witten](#ed-witten)
   2. [Jeff Bezos](#Jeff-Bezos)
   2. [Jeremy Bentham](#Jeremy-bentham)'s thinking was inseparably (1) batshit, (2) shockingly ahead of his time
   2. [Johnny von Neumann](#johnny-von-neumann)
   2. [LeBron James](#lebron-james)
   2. [Richard Feynman](##Richard-Feynman)
   2. [Robin Hanson](#robin-hanson)
   2. [Scott Alexander](#Scott-alexander)
   2. [Srinivasa Ramanujan](#Srinivasa-Ramanujan)
   2. [Terry Tao](#terry-tao)
2. [Biology](#biology)
   1. [Evolution](#evolution)
      1. [Gene-centered view](#gene-centered-view)
   2. [Animals are not like us](#animals-are-not-like-us)
   2. [Why are we trichromats, and not say tetrachromats?](#why-we-are-trichromats)   
2. [Business, finance, entrepreneurship, management](#business-finance-entrepreneurship-management)
   1. [Real work and bullshit jobs](#Real-work-and-bullshit-jobs)
   1. [Consulting](#consulting)
      1. [Consulting's value-add](#consultings-value-add)
   2. [Management and leadership](#Management)
      1. [Being comfortable with uncertainty and imperfection](#comfort-with-uncertainty)
      1. [Business success measures](#Business-success-measures)
      2. [Dealing with message distortion](#Dealing-with-message-distortion)
      2. [High organizational standards](#high-standards)
      2. [Resisting organizational stasis, or, it is always Day 1](#always-day-1): obsess over customers, resist managing by proxy, disagree and commit, embrace external trends *(KIV for content list refactoring)*
   2. [Venture capital](#venture-capital)
      1. [Typical route to becoming a VC](#Typical-route-to-becoming-VC)
   3. [Insurance](#Insurance)
      1. [The main function of the insurance sector in an economy](#Main-function-of-insurance-sector-in-economy)
2. [Culture, community, and walled gardens](#Culture-community-and-walled-gardens)
   1. [Anglerfish and beacons, or why some blogs have higher comment quality](#Anglerfish-and-beacons), on geek subculture dilution by Chapman-sociopaths
2. [Doing good](#doing-good)
   1. [Effective altruism](#effective-altruism)
      1. [Weird EA](#weird-EA)
2. [Economics](#economics)
   1. [Critique of Freakonomics](#critique-of-freakonomics)
2. [Erisology and thinking less wrongly](#Erisology-and-thinking-less-wrongly)  
   1. [Argumentative charity](#argumentative-charity)
   2. [Chesterton's fence](#chestertons-fence)
   2. [Generalizing from one example, or the typical mind fallacy](#typical-mind-fallacy)
   3. [The absurdity heuristic doesn't work very well](#absurdity-heuristic)
2. [Feelings](#feelings)
   1. [Love](#love)
2. [Games](#games)
   1. [Board games](#board-games)
   2. [Game complexity measures](#Game-complexity-measures)
2. [General intelligence](#general-intelligence) 
   1. [LOGI](#logi)
2. [Hiking and trails](#hiking-and-trails)
   1. [Great trails](#great-trails)         
      1. [Tuttle Creek to Mt Langley](#tuttle-creek-to-mt-langley) (7,100' in 4.5 mi to 14er)
   2. [Opinion](#opinions-on-hiking)
      1. [Why say hello to fellow hikers?](#why-say-hello-to-fellow-hikers)
2. [Math](#math)
   1. [Advice](#math-advice)
      1. [Asking the right question](#asking-the-right-question)
      2. [Go to seminars, even if they're ostensibly irrelevant](#go-to-seminars)
      2. [Why you should read the masters in math](#reading-the-masters-in-math)
      3. [Why most mathematical writing sucks, and how to write better](#why-math-is-boring)
   2. [Opinions and observations](#math-opinions)
      1. [Defining combinatorics](##Defining-combinatorics)
      2. [Expositions should solve problems terribly](#Solving-math-problems-terribly)
      2. [Good mathematics](#good-mathematics)
      2. [How a 'Math Product Manager' would teach and do math](#product-management-of-math)
      2. [How famous open problems get solved](#solving-famous-open-problems)  
      2. [How mathematical structures are defined](#How-mathematical-structures-are-defined)
      2. [Mathematical maturity and understanding](#mathematical-maturity)
      2. [Thinking and explaining](#Thinking-and-explaining)
      3. [What every mathematician should know](#what-every-mathematician-should-know)
      3. [Why didn't the Tricki take off?](#why-tricki-failed)   
2. [Memory and the brain](#memory-and-the-brain)
   1. [Augmenting long-term memory](#augmenting-long-term-memory), e.g. Anki
   2. [Procedural vs declarative memory](#procedural-vs-declarative-memory)
   2. [Externalizing the brain](#external-brain), e.g. Google, writing, Cosma on collective cognition
   3. [Cognitive science](#cognitive-science)
   3. [Names matter](#names-matter)
   4. [Steve Yegge on memory](#Yegge-on-memory)
   5. [Wisdom](#wisdom)
2. [Philosophy](#philosophy)
   1. [General](#general-philosophy)
   2. [Diseased philosophy](#diseased-philosophy)
   2. [Morality, axiology, law](#morality-axiology-law)
   3. [Moral patienthood](#moral-patienthood)
   3. [Reading the masters in philosophy](#reading-the-masters-in-philosophy), ft. Aristotle as pro skater
   3. [Reality has a surprising amount of detail](#Reality-has-a-surprising-amount-of-detail), ft. the fabled *"Electron Band Structure In Germanium, My Ass"*
   3. [Slack and deliberate mediocrity](#slack-and-deliberate-mediocrity)
   4. [Moloch](#moloch)
   5. [Anthropic principle](#anthropic-principle)
      1. [Critiques](#critiques-of-the-anthropic-principle), ft. Cosma, Smolin
2. [Political science and public policy](#political-science)
   1. [Chinese perspective on Western elite](#chinese-perspective-on-western-elite)
2. [Psychology](#psychology)
2. [Rationality and postrationality](#rationality-and-postrationality)
   1. [Expert judgment accuracy](#expert-judgment)
   2. [Signaling](#signaling)
      1. [X isn't about Y](#x-isnt-about-y)
      2. [Countersignaling](#countersignaling), including metacontrarianism
2. [Reading and writing](#reading-and-writing)
   1. [Writing advice](#writing-advice)
      1. [Nonfiction](#nonfiction), e.g. microhumor
         1. [The art of plain talk](#the-art-of-plain-talk)
         2. [Readability](#readability)
      2. [Fiction](#fiction)
         1. [Show, don't tell -- or not](#show-dont-tell)
2. [Research and academia](#research-and-academia)
   1. [The general function of intellectual traditions](#role-of-intellectual-traditions)
   2. [Research in industry](#research-in-industry)
   2. [Why academic writing sucks](#why-academic-writing-sucks)
   2. [Why I left academia](#Why-I-left-academia)
   2. [Tactics and advice](#research-tactics)
      1. [Using Anki to read a paper in an unfamiliar field thoroughly](#anki-in-research)
   2. [Mindset](#research-mindset)
   2. [Debt, interpretive labor, distillation](#distillation-and-research-debt)
2. [Software development and computer science](#software-development-and-computer-science)
   1. [Amazon vs Google comparison (2011)](#Amazon-vs-Google-2011)
   2. [Codebase as organism](#codebase-as-organism)
   2. [Practical magic](#practical-magic)
   2. [Premature optimization](#premature-optimization)
   2. [Why platforms matter](#platforms)
   2. [Smalltalk](#smalltalk), ft. misconception corrections by Alan Kay
   3. [The conservative-liberal axis in software development](#yegges-conservative-liberal-axis)
   4. [CS is beyond the glass ceiling of popular science](#CS-is-beyond-the-glass-ceiling-of-pop-science)
   5. [Theoretical CS contributions, or, CS as quantitative epistemology](#Theoretical-CS-contributions)
   5. [Second-system effect, or well-intended redesign bloat](#Second-system-effect)
2. [Statistics](#statistics)
   1. [General](#general-stats)
   2. [The role of statistics in doing science](#the-role-of-statistics-in-science)
   2. [Statistical literacy](#statistical-literacy)
2. [Teaching and learning](#teaching-and-learning)
   1. [Errors vs bugs](#errors-vs-bugs)
   2. [Important findings on learning](#important-findings-on-learning), ft. 25 principles by U of Missouri
   2. [Polymathy, or being a generalist](#polymathy)
2. [Technology and futurism](#Technology-and-Futurism)
   1. [The Singularity](#the-singularity)
      1. [The three main schools of thought](#Three-Singularity-schools)
      2. [Replacing humans at every step of the economic chain](#replacing-humans-at-every-step-of-the-economic-chain)
      2. [Disneyland with no children](#Disneyland-with-no-children)
      3. [The Singularity has already happened](#the-singularity-has-happened)
   2. [The origins of information society](#Origins-of-information-society)
   3. [Rambling and AI](#rambling-and-ai)
2. [Miscellaneous](#miscellaneous)


<a name="#Culture-community-and-walled-gardens"></a>
## Culture, community, and walled gardens
([overview](#overview))

<a name="#Anglerfish-and-beacons"></a>
### Anglerfish and beacons
([overview](#overview))

Ben Hoffman talks about avoiding anglerfish, or Chapman's sociopaths, in a great essay I often come back to called [On the construction of beacons](http://benjaminrosshoffman.com/construction-beacons/). I tried not to just copy-paste everything, but Ben doesn't waste words, he's a precision writer, so it's hard to leave things out because "everything is in its place" so to speak.

First of all, why "anglerfish"?

```markdown
The anglerfish lives in waters too far beneath the surface of the sea for sunlight to reach.
It dangles a luminescent lure in front of itself. This resembles a fishing angle, whence 
comes its name. This lure attracts animals of the deep sea, which approach the anglerfish, 
and are consequently eaten by it.

Why - in the deep sea where no sunlight can reach - would evolution favor animals that are
attracted to light?

The secondary uses of such a strategy are clear enough. Once some deep-sea-dwellers emit
light, larger animals that predate on them might do better if attracted to light sources. 
But that presupposes the existence of other animals that already emit light, for other
reasons.

What are the primary uses of light? In a region where no other creatures emit light, here 
are some reasons why would might begin to do so:

1. To illuminate potential prey.
2. As a ward, to warn potential competitors that one is prepared to defend territory.
3. To attract complementary animals, either as symbiotes, or as mates.

In all these cases, the purpose of the light is to *reveal information*. In all but the
first case, it is to *share information with others, in order to enable cooperation*. 
Perhaps the purest version of this is the mating display. We can see this in the firefly, 
which uses its distinctive patterns of luminescent flashes to find mates.

The firefly has some information. It activates a beacon, in order to find someone with 
*complementary information*, in order to engage in *productive exchange*. Likewise for 
deep-sea fish who mate or find symbiotes by means of a light display.

The predation strategy of the anglerfish, properly generalized is a strategy that predates
on all information-seeking behavior, whether competitive or cooperative. The anglerfish
does not need to know that the animal that just swam in front of it is evaluating its 
mating display and finds it wanting, or is looking for a very different creature as a 
symbiote. So long as there are animals seeking illumination, the anglerfish only cares that
some calories and raw materials have been brought within reach of a single burst of 
swimming and the clamping shut of its great maw.

Typically, a predator has to be more sophisticated than the creatures on which it preys.
But the anglerfish follows a simple, information-poor strategy, that preys on sophisticated,
information-rich ones. It doesn’t have to be a particularly skilled mimic - it simply preys
on the fact that creatures seeking information will move towards beacons.
```

How does this analogize to subculture dilution?

```markdown
In David Chapman’s geeks, MOPs, and sociopaths, “geeks” are the originators of subcultures. 
They are persons of refined taste and discernment. They found subcultures by discovering or
creating something they believe to be of intrinsic value. The originators of this information
share it with others, and the first to respond enthusiastically will be other geeks, who can 
tell that the content of the message is valuable.

Eventually, enough geeks congregate together, and the thing they are creating together becomes
valuable enough, that people without the power to independently discern the source of value 
can tell that value is being created. These Chapman calls “Members Of the Public”, or “MOP”s. 
Geeks map roughly onto Aellagirl's possums, MOPs onto otters.

In the right ratios, MOPs and Geeks are symbiotes. The MOPs enjoy the benefits of the thing the
geeks created, and are generally happy to share their social capital, including money, with the
geeks.

But from another perspective MOPs are an exploitable resource, which the geeks have gathered in
one place but are neither efficiently exploiting, nor effectively defending. This attracts
people following a strategy of predating on such clusters of MOPs. These predators, whom Chapman
names “sociopaths,” do not care about the idiosyncratic value the geeks are busy creating. What 
they do care about, is the generic resources - attention, money, volunteer hours, social proof -
that the MOPs provide.

To summarize the above: Geeks build beacons. Initially these beacons are not very bright, but
they are sending out high-information signals which attract other geeks looking for that 
information. Eventually, enough geeks are contributing to the beacons that they become bright
enough to attract MOPs.

Chapman’s sociopaths can’t just waltz in and propose that everyone give them things for nothing.
After all, everyone in their feeding ground was attracted to it by something about it, something
that distinguishes it from other places in the culture. They need to look like a part of the 
scene. So they start by imitating, or proposing refinements to, the beacons the geeks have erected.

The geeks are only putting up a very particular kind of beacon. There are a lot of constraints 
on exactly what sort of signal they are willing to send. This is the same as saying that their 
beacons have a lot of information content. From the geeks’ perspective, the exchange of this 
information is the whole point of setting up beacons, and the presence of friendly MOPs is just a
happy side effect.

But from the sociopaths’ perspective, these information-bearing constraints are mere shibboleths.
Chapman’s sociopaths will follow whatever rules they have to in order to pass as contributors to
the subculture, but they won’t put independent effort into understanding why these rules are the 
ones they have to follow. Instead, their contribution is to **iteratively improve the beacons’ 
ability to attract prey.**

As sociopaths test out variations in their beacons, they will learn which variants are best at 
attracting people, by means of trial and error. Three things about this will reduce the relative 
proportion of geeks in the subculture, and therefore the geeks’ influence.

1. First, since MOPs are less sensitive to fine variations in signal than geeks are, random
mutations in beacon design are more likely to attract more MOPs than more geeks.

2. Second, as the overall process becomes better at attracting MOPs, more sociopaths will notice
that it is a promising feeding ground.

3. Finally, many changes that are neutral or beneficial for attracting MOPs, will, from the geeks’ 
perspective, seem like the introduction of errors. This will make the signal less attractive to
geeks who have not already invested in the subculture.

What does this process look like from the geeks’ perspective?

At first - people are coming into the geeks’ subculture, and trying to contribute to it. These
newcomers are putting a lot of energy into creating new content, but from time to time 
introduce perplexing errors. But, they are getting a lot of people interested in this wonderful
information you’ve created, so the geeks are not inclined to complain. The MOPs basically trust
the geeks’ implied endorsement, and accept the new contributors on the same footing as the old 
ones.

But now there are two forces at play affecting the content of the signals being sent. One is a
force correcting errors - the geeks’ desire to preserve, transmit, and develop the original 
information-content of the signal. The other force introduces errors: the sociopaths’ desire to
attract more MOPs. When the second force becomes stronger than the first, the sociopaths are now 
the dominant faction, and able to coordinate to suppress geek attempts to correct errors that 
make the message more popular.

At first, the MOPs’ acceptance of the sociopaths depended in part on the geeks’ tacit endorsement.
But once a sufficiently powerful faction of sociopaths has been given social proof, they can wield
the force of disendorsement against the geeks. The only meaningful constraint is that MOPs don’t
like conflict, so the sociopaths will want to avoid escalating to a point where the conflict
becomes overt.

From the sociopaths’ perspective, the geeks were inexplicably donating their time and energy to 
discovering a new signal to broadcast, that would attract a pool of MOPs to feed on. But the geeks
were - again incomprehensibly - neither exploiting nor defending that resource. The sociopath 
strategy invests in general understanding of social dynamics, but does not need to understand the 
specific content of what the geeks are trying to do. The sociopath need only know that some
attention, money, volunteer hours, and social proof have been brought within reach of a competent
marketing and sales effort.

From the sociopaths' perspective, they are not introducing errors - they are correcting them.

The paradigmatic predator is sufficiently smarter than its targets to anticipate and manipulate
their behavior. But Chapman’s sociopaths follow a simple, information-poor strategy, that preys on
sophisticated, information-rich ones. This strategy doesn’t have to understand the signal as well 
as the geeks do - the geeks will help it pass their tests (because geeks are usually guess culture,
and guess culture screens for trying to cooperate). It simply iterates empirically towards shining 
the most attractive beacon it can, of a kind that has already been selected to attract its prey.

The predation strategy of Chapman’s sociopath is a strategy that predates on all information-
seeking behavior, whether competitive or cooperative.
```

Note that sociopaths aren't "bad":

```markdown
Sociopaths are not necessarily universally bad or mean people. They just *don't care about your 
project*. This is fine. You don't care about most people's projects. Likewise, most people don't 
care about yours. The problem is when you let those people run your project.

As far as Chapman's sociopaths know, they are just doing what one does to beacons - trying to make
them more pleasing to more people. They are cooperating with the geeks as sincerely as they know 
how - as sincerely as the believe to be possible. In many cases they simply don’t understand that
the original signal had value. There's little point in being indignant about this.
```

It's the "geeks" who're most responsible for maintaining that subculture, and for the creation of community standards:

```markdown
The people who need to do something about the corruption of a message are the people who *care
the most about that message*: the geeks. In subcultures following this lifecycle, geeks have 
committed a key sin: trying to get something for nothing, by pretending to be more popular than
we are.

People playing sociopath strategies gain a foothold in subcultures, because they *bring in more
resources*, get more people involved, get attention from respectable people, raise money - since 
they are paying attention to how attractive their beacons are, not whether they are correct 
(from a geek perspective).

The obvious strategy to counter this is to speak up early and often when errors are being 
introduced. It is not a sin to be error-tolerant, in the sense of not immediately expelling people 
for making errors. But it is *always* a sin, in an otherwise-cooperative community, to *suppress the
calling-out of errors*, in order to avoid making a scene, scaring off the MOPs, harming morale and
momentum. If you are a geek in that sort of subculture, the MOPs are relying on your implied 
endorsement of the other content-creators. If you remain silent in the face of error, then you are
*betraying this trust*. There is no additional error-correction system that will save you - you were
supposed to be the error-correction system.

If you and your collaborators diligently follow this practice, then this will enable the creation 
of common knowledge when someone is reliably introducing errors, and either failing to correct them
or making the minimum possible correction. You will have shared knowledge of track records - who is
introducing information, and who is destroying it with noise. It is only with this knowledge that 
you can begin to have actual community standards.
```

<a name="#business-finance-entrepreneurship-management"></a>
## Business, finance, entrepreneurship, management
([overview](#overview))

<a name="#Real-work-and-bullshit-jobs"></a>
### Real work and bullshit jobs
([overview](#overview))

From Sarah Constantin's [How much work is real?](https://srconstantin.wordpress.com/2017/05/09/how-much-work-is-real/).

Productive work:

```markdown
Some work is clearly “productive.” If you plant things in a garden, you put in work,
and you get out plants.  If you cook a meal, your family gets fed. If you build a 
building where people want to live or work, they get shelter. If you treat a patient,
the patient gets better. If you carry goods to the place that they’re sold, people
get their stuff. If you invent a labor-saving machine, people get to free up their 
time for other things.

Productive work creates *value*, in the sense of “doingstuffness”, mana, usefulness-
to-humans, etc. It’s not just effort expended, or an accounting formalism like dollars, 
it’s an increase in the “real wealth” of humanity. That’s not a well-defined concept,
but it’s worth pointing at, so we know what we’re complaining about when we see 
deviations from this.

In the standard capitalist story, you get paid for work because you created value for
somebody; they wanted your stuff so much that they were willing to give you something
in exchange for it.

In this world, all productive work is honorable.  Work is *fair* — on average, you get
what it’s worth — and it’s a contribution, however small, to the wellbeing of
humankind, the fire that beats back against the blackness of hard vacuum.
```

Work that clearly fails to be productive:

```markdown
But there are ways that things called “work” can fail to be productive work.

Fraud or crime obviously are not productive. If you get money from people by tricking 
or terrifying them, you’re not getting it by providing them with value. You’re not a 
*maker*.

Enforced monopoly power is also not entirely productive. If people are required by law
— on pain of punishment — to buy your product, then at least some of your revenue is 
driven by fear, not desire.

Regulations can be a form of enforced monopoly power. If only people who meet certain
criteria are allowed to sell, then people are buying from you and not your competitors
not because they like you better, but because your competitors are driven out by fear
of punishment. Once again, you’re profiting partly off fear, not just desire.

A job that is funded by taxpayer money, or by the fact that the product sold is
mandatory to buy, or by the fact that nobody knows whether it would be illegal to get
rid of that job and they want to play it safe, doesn’t need to be useful at all.
```

Work that can indirectly fail to be productive:

```markdown
And there are still more indirect ways that a job can fail to be useful.  If you sell
to people who don’t do anything useful, then your job would not have been necessary in
a sensibly organized world, even if you do nothing dishonest yourself and genuinely add
value to your customers.

This is what it means to live in a “mixed economy.”  Not everything that everyone does 
for a living is genuinely useful.

If there are bullshit jobs, as anthropologist David Graeber claims, then that’s a shame,
from the perspective of human well-being. If we have enough real wealth, enough mana, to
support even people who aren’t making mana, then why not just allow leisure, instead of 
forcing people to go through the motions of dull and unnecessary work?

This is largely the position of left-libertarians like the people at Center for a
Stateless Society.  They make the empirical claim that most of the present economy in 
developed countries is coercive and unproductive, the result of crony capitalism and 
regulatory capture rather than honest, useful work.  As such, a “freed market” without 
such corruption would actually be more egalitarian than our current economy.   Since 
government promotes monopoly, Big Business wouldn’t be sustainable without coercion.  
Since highly regulated and positional goods like housing and education are essentially 
mandatory for participating in much of modern life, if those mandates were abolished, 
socioeconomic inequality would drop.

On the other hand, this empirical claim could be false. Nobody denies that some corruption
exists, but it might be the exception rather than the rule. We might not, in fact, be in
post-scarcity conditions.  So-called “bullshit” jobs may actually be valuable, just easy
to dismiss by outsiders like Graeber.  Growing wealth inequality may be largely the result
of winner-take-all phenomena, as Tyler Cowen thinks — in his model, the working rich 
really are more productive than ever, thanks to the amplifying effects of technology.  
Love it or hate it, says Cowen, capitalism works the way it says on the tin.
```

I was surprised by this particular example, and would like to know if the result replicates(?):

```markdown
Matt Rognlie’s paper shows that the long-term rise of capital’s share of wealth (compared
to labor’s) is almost entirely a result of increased housing prices — literal rents, kept
high by land-use restrictions.
```

Add to this Robin Hanson's claim that S&P 500 firms are now "5/6ths dark matter", or hard-to-copy intangibles (illegibles?):

```markdown
Imagine that you wanted to create a new firm to compete with one of these big established
firms. So you wanted to duplicate that firm’s products, employees, buildings, machines, 
land, trucks, etc. You’d hire away some key employees and copy their business process, at
least as much as you could see and were legally allowed to copy.

Forty years ago the cost to copy such a firm was about 5/6 of the total stock price of 
that firm. So 1/6 of that stock price represented the value of things you couldn’t easily
copy, like patents, customer goodwill, employee goodwill, regulator favoritism, and hard 
to see features of company methods and culture. Today it costs only 1/6 of the stock 
price to copy all a firm’s visible items and features that you can legally copy. So today
the other 5/6 of the stock price represents the value of all those things you can’t copy.
```

"Intangibles" isn't just "great culture", as people are wont to default to, it's also patents and rent-seeking favoritism:

```markdown
“Intangibles” would certainly include rent-seeking forms of favoritism.

It also includes patents, which arguably do not increase innovation on the margin 
(according to natural experiments between different countries with different patent regimes
or changes in patent laws.) Copyright and patent lengths have gotten longer in the US, and 
patent applications have grown at an accelerating rate; the growth of intellectual property
is another example of our economy becoming more monopolistic.
```

<a name="#Consulting"></a>
### Consulting
([overview](#overview))

<a name="#consultings-value-add"></a>
### Consulting's value-add
([overview](#overview))

Loku cofounder Raj Ramanan's [answer](https://qr.ae/TW1HiJ) to the Quora question 

```markdown
"What value do consulting firms (like McKinsey, Bain, et al.) really add to an 
operation? What exactly is it that they do that commands such premium compensation
and, more importantly, has created an entire sustainable and enduring industry?"
```

is apparently very well-received, including by a number of "top-shelf consultants" from the MBB firms themselves. So:

```markdown
I interned at Bain & Co., started my career at McKinsey & Co, worked in the operations
/turnaround group at KKR with ex-BCG consultants, worked at an internal consulting 
function at Bear Stearns, and was an independent consultant to small and medium
businesses before co-founding a company (Loku).

I first want to make the distinction between generalist strategy consulting shops 
(McKinsey, Bain, and BCG) vs. specialized or technical shops (Accenture, boutiques).
I can only speak credibly about the former (though, as a former consultant, a lack of
credibility has never stopped me from talking on any subject...).

Cynicism aside, the real **value a consulting firm provides is**:

1. **Political leverage**: CEOs that want or need to make an unpopular decision often 
bring in a consulting firm to help. This provides ammunition to recommend an unpopular
or risky decision to the board (expansion into a new business line or geography, or 
shutting down a plant). The CEO can also distance herself from an unpopular decision by
blaming the consultants. Finally, if things go wrong, consultants are a handy scapegoat. 

2. **Pool knowledge across functions**: Consultants are not part of the client's culture,
politics, or org culture. In the first month, as the firm builds a fact base, consultants 
usually interview people across functions. In large companies, cross-functional problem-
solving rarely happens. Just getting different functions in a room typically unlocks
creative problem solving.

3. **Pool knowledge across levels**: Similarly, consultants interview, watch, and tag
along with people down the org structure, often starting with customers and moving through
sales and line roles. CEOs and the exec team of large companies rarely do this (exception
being their largest customers). There are tremendous insights to be had by doing this.

4. **Deep focus on one problem**: The biggest value is that you have a dedicated team of
pretty smart people who are generally unbiased that can focus deeply on one particular 
problem. At a company, in any role, you have a day job and at best can focus only a 
portion of your time on a particular issue. Nowhere is this more true than at a startup, 
but that's another Quora post.

I want to point out that even though you could group all of this as "telling you what you
already know," all of this is real value that impacts the bottom line however you measure 
it, and that in the absence of strong leadership or a big, risky culture change, could not
have been accomplished without the consultant.

**What value does a consulting firm NOT provide?**

1. **Subject matter expertise**: The people who are doing the bulk of the work are fresh
out of college (like I was) or business school, often from a range of majors, with little-
to-no work experience. They don't know anything. They are really smart and very hard-
working and resourceful and well-trained, but they know absolutely nothing coming in about
your industry, company, or particular issue. More interestingly, the director or partner
on the project is not really a subject matter expert either. She has built her career in
a particular industry or practice area and has served a range of clients across a range
of problems. Her value is pattern recognition, but there is no way she can know more about
the client's industry, company, or problem then someone who lives it every single day for
years. And most partners and directors are career consultants, not industry vets (certainly
true at the big 3 shops).

2. **Executive coaching**: The firms love to provide long-term, ongoing, ambiguous projects 
on a retainer. If you are a CEO, get a therapist. Or some friends. Or a board that is 
actually helpful. Or a best friend. You don't want a partner at a firm who spends a half
hour a month thinking about you or your problem to bill you on a retainer. 

3. **Actual decision making**: Consultants are great at assembling facts from the outside 
world, bringing in perspective from all functions and levels of your company, finding
interesting patterns, and providing you with a point of view. They will jump in the line of
fire on controversial decisions and fall on the sword if things go wrong. But they can never
actually make the tough decision. No outsider can every truly understand the needs of the 
various stakeholders. No outsider has as much at stake personally and professionally. It is
so incredibly different to recommend layoffs for 10% of a plant (100 people) vs. actually 
firing a single person. I've done both and the former does not have the emotional or 
cultural consequences of the latter. Only the CEO or other leader at a company can actually
pull the trigger; the consultant can at best show you how to hold the gun and maybe give you
a few targets to aim at.


Would I ever hire a consultant?

I believe good leadership (promoting cross-functional decision-making, a flat or open org
structure, autonomy across roles; spending time with customers, talking to folks on the 
front line; and all the other stuff HBR publishes) can provide much more value than hiring 
an outside consultant can.

But, if I could do the following, in certain cases, I would hire a consultant:

1. **Impose limits**: Make sure it is a defined, narrow problem well before the consultants 
show up. Make sure it's a project of defined length, with a defined number and type of 
resource. Demand frequent updates and check-ins. Have the option of aborting the project if
you are not getting what you want or if the directional answer is good enough.

2. **Use a bid process**: The big 3 will lecture you and their individual differences and 
when I interviewed at the firms, I was obsessed about what made them different. They are the
same. They hire from the same talent pool. They are interchangeable. Put them through a 
competitive bid process (sealed bid or auction), with pre-set milestones. They will hate this.
They will tell you that they don't participate in auctions. They will try to wiggle out by 
defining the problem differently or changing the milestones or taking you out to dinner. Don't
do it. Force the process. They will comply. I've done it myself by sourcing consulting services
on behalf of Bear Stearns.

3. **Demand partner time**: It's your (or really your investors' or shareholders') money and
you are paying a fixed monthly fee. Demand partner time. Earlier objections notwithstanding,
I'd rather spend time with someone with 10 or 20 years of consulting experience who has at
least seen my problem in a different industry or knows anything about my industry than a 22 
year old with a fresh diploma who has never actually made a business decision. I know because
I was that kid. Best of intentions and a killer work ethic, but I'd want partner time.

I have a ton of respect for the firms I worked at and the colleagues I worked with. I learned 
a tremendous amount and made lifelong friends. Consulting as an industry and these three firms
in particular provide real business value; companies just need to know how to appropriately 
use these service providers.
```

<a name="#Management"></a>
### Management
([overview](#overview))

<a name="#comfort-with-uncertainty"></a>
### Comfort with uncertainty
([overview](#overview))

Sarah Constantin struggles with whether she wants to get better at management and leadership because of the possibility of losing her self in [Dwelling in possibility](https://srconstantin.wordpress.com/2017/05/18/dwelling-in-possibility/). There were a few quotes that weren't quite what the essay was about, but that struck me as worth remembering of their own regard for how they're articulated.

```markdown
One of the things I’ve noticed in people who are farther along in business or management
than I am, usually men with a “leaderly” mien, is a certain comfort with uncertainty or 
imperfection.

They can act relaxed even when their personal understanding of a situation is vague, when
the future is uncertain, when the optimal outcome is unlikely.  This doesn’t mean they’re
not motivated to get things done.  But they’re cool with a world in which a lot of things
remain nebulous and unresolved at any given moment.

They’re able to produce low-detail, high-level, positive patter for a general audience. 
They’re able to remain skeptical, expecting that most new ideas won’t work, without 
seeming *sad* about that.

Talking to someone like that, it feels like a smooth layer of butter has been spread over
the world, where everything is pretty much normal and fine most of the time — not a crisis,
not a victory, just normalcy.

Being okay with vagueness seems to be a prerequisite to managing other people — after all,
you can’t know every detail of everyone else’s job.  When I managed people, I struggled
with that a lot. I couldn’t be sure a thing was done right unless I checked it for myself.
I’m pretty good at holding large systems in my head, but eventually organizations defeat 
even the most heroic attempt to micromanage them.

Being okay with uncertainty also seems to be a prerequisite for managing a portfolio of 
anything high-risk and high-reward — investments, sales leads, technologies to adopt, etc.
If you are elated every time an opportunity appears, and dejected every time it doesn’t
work out, you’ll have a very hard time emotionally when dealing with a large volume of such
opportunities. (My husband is a salesman and he’s long since stopped telling me about leads
because I’ll get over-excited about every one of them.) ...

There’s a common thread between this notion, and people like Jonathan Haidt who believe in
worldview diversity and people at the Integral Center who believe that higher human 
developmental stages involve the ability to move fluidly between frames, and who sometimes
connect this to business through books like Tribal Leadership.

All of them share a view that the principled or systematic person — the person who believes
in one truth according to one set of principles — is weaker or less spiritually advanced than
the person who sees things through multiple points of view.

In particular, one idea I picked up from Tribal Leadership is that if you believe a particular
thing *as an individual*, you’ll be a weaker leader, because you’re just saying what you 
personally believe (which is selfish, in a sense, or at least private, and thus taken less 
seriously by others).  The leader has to be not just John Smith but the voice of Acme Corp. 
Expressing your own thoughts (speaking as John Smith) has value coming from an individual 
contributor, but there’s a different, more facilitator-like, skill where you try to encourage 
dialogue or distill a common thread between different people’s views, and encourage teamwork 
and unity — and that’s leadership.

I have definitely seen chillness coexist with strong technical skill; quite a few people with
that relaxed, leaderly affect are also top-notch at engineering or data science.  Accepting 
that some information “lives” in the “collective mind” of a group clearly doesn’t preclude 
knowing some things very well in your own mind and being able to execute well individually.

I’ve even seen a certain kind of chillness coexist with radical commitment. Rick Doblin, the 
founder of MAPS, has been steadily working for forty years on trying to promote research into
the therapeutic use of psychedelics.  He’s a pleasant, mild-mannered family man; despite his 
controversial mission, he seems to bear no ill will to anyone, including the regulators he’s
been trying to persuade to ease up drug restrictions.  He’s willing to collaborate with anyone,
from any perspective or background, if it’ll help psychedelic research.  He’s my role model 
for how someone can be profoundly committed to a cause without being an angry or rigid person.
His way is like water wearing down a stone.

But I definitely have heard people tell me that equanimity cost them something — that they lost
the chance to have a personal perspective and to want things for themselves when they learned 
to see things from all possible angles and be a facilitator for others.  I’ve seen people who 
are very good at sparking “interesting” conversations complain that they have a hard time
connecting personally rather than remaining a third-party observer.
```

Related to the above is what leadership coach Bryan Franklin [talks about](http://bryanfranklin.com/blog/evolving-leadership-through-paradox) w.r.t. "holding a paradox in your mind":

```markdown
Consider the common example of a leader who needs to convince her followers that, while 
the team is experiencing significant challenges and there is a very real risk of failure,
ultimately the team will prevail. There are two ways a lesser leader could falter in this
moment. The first is to simply pander to neg activism: agreeing with everyone’s feeling 
that the current situation is rough or hopeless, without offering any vision, possibility,
or credible plan. This would be a good display of empathy, but it won’t lead anyone to 
change. The second mistake would be to hold the opposite view, that the future is bright 
and the current setbacks are illusory or insignificant. This could be seen superficially 
as inspiring, but more likely it will backfire because it will be dismissed as being 
noncredible and unrelatable to the lived reality of the employees.

A superior leader learns how to hold paradox: to believe, at the same time, that the
situation is dire and hopeful, meeting employees where they’re at, but also convincing them
of the actions they can take that will lead to a brighter future. The evidence is that 
things are bad (anyone denying this will be seen as a Pollyanna); and also, the evidence is
that things are good (anyone denying this would be seen as a weak leader, lacking 
creativity to produce a positive way forward). Followers need to feel met in the reality 
that they are scared, yet they also need to be given a realistic expectation of future 
success.

When you’re confronted with a paradox, you are presented with a choice. You can either 
ignore it and take a side (believe one side of the statement is true while the other is 
false), or you can do what we call hold paradox, which is to believe both contradictory 
statements or implications simultaneously. It’s an expression of faith in a greater truth 
that is currently invisible to you, but resolves the paradox and allows for the truth of 
both sides to harmoniously coexist. This is what great leaders do.

Holding paradox is the ability to literally hold in your mind the truth and acknowledge, 
for example, your utter insignificance on a cosmic scale, and then without allowing that
experience to dissipate, add to it the unmistakable truth of your profound significance 
to those you love.
```

<a name="#High-standards"></a>
### High standards
([overview](#overview))

Jeff Bezos' [2017 annual letter to shareholders](https://blog.aboutamazon.com/company-news/2015-letter-to-shareholders) gives "high standards" as the answer to "how to keep ahead of customers' divine discontent" and talks a bit more about what high standards entail in practice. 

In summary: 

```markdown
The four elements of high standards as we see it: 

they are *teachable*, 
they are *domain specific*, 
you must *recognize* them, and 
you must explicitly coach realistic *scope*.
```

Jeff believes high standards are teachable:

```markdown
First, there’s a foundational question: are high standards intrinsic or teachable? If you 
take me on your basketball team, you can teach me many things, but you can’t teach me to be
taller. Do we first and foremost need to *select* for “high standards” people? If so, this 
letter would need to be mostly about hiring practices, but I don’t think so. I believe high
standards are teachable. In fact, people are pretty good at learning high standards simply 
through exposure. High standards are contagious. Bring a new person onto a high standards 
team, and they’ll quickly adapt. The opposite is also true. If low standards prevail, those
too will quickly spread. And though exposure works well to teach high standards, I believe 
you can accelerate that rate of learning by articulating a few core principles of high 
standards, which I hope to share in this letter.
```

Jeff also notes that high standards are domain-specific:

```markdown
Another important question is whether high standards are universal or domain specific. In 
other words, if you have high standards in one area, do you automatically have high standards
elsewhere? I believe high standards are domain specific, and that you have to learn high 
standards separately in every arena of interest. When I started Amazon, I had high standards
on inventing, on customer care, and (thankfully) on hiring. But I didn’t have high standards
on operational process: how to keep fixed problems fixed, how to eliminate defects at the 
root, how to inspect processes, and much more. I had to learn and develop high standards on 
all of that (my colleagues were my tutors).

Understanding this point is important because it keeps you humble. You can consider yourself
a person of high standards *in general* and still have debilitating blind spots. There can 
be whole arenas of endeavor where you may not even *know* that your standards are low or non
-existent, and certainly not world class. It’s critical to be open to that likelihood.
```

It's also important to recognize high standards, and know scope, i.e. realistic expectations for how hard it should be (how much work it will take) to achieve that result -- this isn't as obvious as you may think:

```markdown
First, you have to be able to *recognize* what good looks like in that domain.

A close friend recently decided to learn to do a perfect free-standing handstand. No leaning
against a wall. Not for just a few seconds. Instagram good. She decided to start her journey 
by taking a handstand workshop at her yoga studio. She then practiced for a while but wasn’t 
getting the results she wanted. So, she hired a handstand coach. Yes, I know what you’re 
thinking, but evidently this is an actual thing that exists. In the very first lesson, the 
coach gave her some wonderful advice. “Most people,” he said, “think that if they work hard,
they should be able to master a handstand in about two weeks. The reality is that it takes 
about six months of daily practice. If you think you should be able to do it in two weeks,
you’re just going to end up quitting.” Unrealistic beliefs on scope – often hidden and 
undiscussed – kill high standards. To achieve high standards yourself or as part of a team,
you need to form and proactively communicate realistic beliefs about how hard something is
going to be – something this coach understood well.
```

"Recognizing high standards" and "scope" also come into play together in Jeff's famous (or infamous) 6-page memos in lieu of PPT slides:

```markdown
We don’t do PowerPoint (or any other slide-oriented) presentations at Amazon. Instead, we 
write narratively structured six-page memos. We silently read one at the beginning of each
meeting in a kind of “study hall.” Not surprisingly, the quality of these memos varies widely.
Some have the clarity of angels singing. They are brilliant and thoughtful and set up the 
meeting for high-quality discussion. Sometimes they come in at the other end of the spectrum.

In the handstand example, it’s pretty straightforward to *recognize* high standards. It 
wouldn’t be difficult to lay out in detail the requirements of a well-executed handstand, 
and then you’re either doing it or you’re not. The writing example is very different. The 
difference between a great memo and an average one is much squishier. It would be extremely 
hard to write down the detailed requirements that make up a great memo. Nevertheless, I find
that much of the time, readers react to great memos very similarly. They know it when they 
see it. The standard is there, and it is real, even if it’s not easily describable.

Here’s what we’ve figured out. Often, when a memo isn’t great, it’s not the writer’s 
inability to *recognize* the high standard, but instead a wrong expectation on *scope*: they
mistakenly believe a high-standards, six-page memo can be written in one or two days or even
a few hours, when really it might take a week or more! They’re trying to perfect a handstand
in just two weeks, and we’re not coaching them right. The great memos are written and re-
written, shared with colleagues who are asked to improve the work, set aside for a couple of
days, and then edited again with a fresh mind. They simply can’t be done in a day or two. The
key point here is that you can improve results through the simple act of teaching scope – that
a great memo probably should take a week or more.
```

Note that nowhere does Jeff talk about needing "skill" for meeting high standards. He addresses this explicitly w.r.t. the memo writing example:

```markdown
Beyond recognizing the standard and having realistic expectations on scope, how about skill?
Surely to write a world class memo, you have to be an extremely skilled writer? Is it another
required element? In my view, not so much, at least not for the individual in the context of 
teams. The football coach doesn’t need to be able to throw, and a film director doesn’t need 
to be able to act. But they both do need to recognize high standards for those things and 
teach realistic expectations on scope. Even in the example of writing a six-page memo, that’s 
teamwork. Someone on the team needs to have the skill, but it doesn’t have to be you. (As a 
side note, by tradition at Amazon, authors’ names never appear on the memos – the memo is 
from the whole team.)
```

Benefits of high standards:

```markdown
1. Naturally and most obviously, you’re going to build better products and services for 
customers – this would be reason enough!

2. Perhaps a little less obvious: people are drawn to high standards – they help with 
recruiting and retention.

3. More subtle: a culture of high standards is protective of all the “invisible” but crucial 
work that goes on in every company. I’m talking about the work that no one sees. The work that
gets done when no one is watching. In a high standards culture, doing that work well is its own
reward – it’s part of what it means to be a professional.

4. And finally, high standards are fun! Once you’ve tasted high standards, there’s no going back.
```

<a name="#Business-success-measures"></a>
### Business success measures
([overview](#overview))

From Jeff Bezos' [first annual letter to shareholders](https://blog.aboutamazon.com/company-news/2017-letter-to-shareholders), way back in the dawn of time (1997, more exactly), where he talks about his "fundamental management and decision-making approach":

```markdown
We believe that a fundamental measure of our success will be the *shareholder value we create
over the long term*. This value will be a direct result of our ability to extend and solidify
our current market leadership position. The stronger our market leadership, the more powerful
our economic model. Market leadership can translate directly to higher revenue, higher 
profitability, greater capital velocity, and correspondingly stronger returns on invested 
capital.

Our decisions have consistently reflected this focus. We first measure ourselves in terms of 
the metrics most indicative of our market leadership: customer and revenue growth, the degree 
to which our customers continue to purchase from us on a repeat basis, and the strength of our
brand. We have invested and will continue to invest aggressively to expand and leverage our 
customer base, brand, and infrastructure as we move to establish an enduring franchise.

Because of our emphasis on the long term, we may make decisions and weigh tradeoffs differently
than some companies. Accordingly, we want to share with you our fundamental management and 
decision-making approach so that you, our shareholders, may confirm that it is consistent with 
your investment philosophy: 

1. We will continue to focus relentlessly on our customers.

2. We will continue to make investment decisions in light of long-term market leadership 
considerations rather than short-term profitability considerations or short-term Wall Street 
reactions.

3. We will continue to measure our programs and the effectiveness of our investments analytically,
to jettison those that do not provide acceptable returns, and to step up our investment in those
that work best. We will continue to learn from both our successes and our failures.

4. We will make bold rather than timid investment decisions where we see a sufficient probability
of gaining market leadership advantages. Some of these investments will pay off, others will not,
and we will have learned another valuable lesson in either case.

5. When forced to choose between optimizing the appearance of our GAAP accounting and maximizing
the present value of future cash flows, we’ll take the cash flows.

6. We will share our strategic thought processes with you when we make bold choices (to the extent
competitive pressures allow), so that you may evaluate for yourselves whether we are making 
rational long-term leadership investments.

7. We will work hard to spend wisely and maintain our lean culture. We understand the importance
of continually reinforcing a cost-conscious culture, particularly in a business incurring net losses.

8. We will balance our focus on growth with emphasis on long-term profitability and capital
management. At this stage, we choose to prioritize growth because we believe that scale is central 
to achieving the potential of our business model.

9. We will continue to focus on hiring and retaining versatile and talented employees, and continue 
to weight their compensation to stock options rather than cash. We know our success will be largely
affected by our ability to attract and retain a motivated employee base, each of whom must think
like, and therefore must actually be, an owner.

We aren’t so bold as to claim that the above is the “right” investment philosophy, but it’s ours,
and we would be remiss if we weren’t clear in the approach we have taken and will continue to take.
```

<a name="#always-day-1"></a>
### Always Day 1
([overview](#overview))

Jeff's [2016 annual letter to shareholders](https://blog.aboutamazon.com/company-news/2016-letter-to-shareholders) has some great quotes about how he resists organizational stasis at Amazon.

Responding to the question "Jeff, what does Day 2 look like?" at a recent all-hands meeting:

```markdown
    Day 2 is stasis. 
    Followed by irrelevance. 
    Followed by excruciating, painful decline. 
    Followed by death. 
    And that is why it is *always* Day 1.
```

The "starter pack" of "essentials for Day 1 defense:

```markdown
Such a question can’t have a simple answer. There will be many elements, multiple paths, 
and many traps. I don’t know the whole answer, but I may know bits of it: 

- customer obsession
- a skeptical view of proxies
- the eager adoption of external trends
- high-velocity decision making.
```

Or, as Jeff wraps up:

```markdown
So, have you settled only for decision quality, or are you mindful of decision velocity too?
Are the world’s trends tailwinds for you?
Are you falling prey to proxies, or do they serve you?
And most important of all, are you delighting customers?
```

Customer obsession matters most by far:

```markdown
There are many ways to center a business. You can be competitor focused, you can be product
focused, you can be technology focused, you can be business model focused, and there are more.
But in my view, obsessive customer focus is by far the most protective of Day 1 vitality.

Why? There are many advantages to a customer-centric approach, but here’s the big one: 

Customers are always beautifully, wonderfully dissatisfied, even when they report being happy
and business is great. Even when they don’t yet know it, customers want something better, and 
your desire to delight customers will drive you to invent on their behalf. No customer ever 
asked Amazon to create the Prime membership program, but it sure turns out they wanted it, and
I could give you many such examples.

Staying in Day 1 requires you to experiment patiently, accept failures, plant seeds, protect 
saplings, and double down when you see customer delight. A customer-obsessed culture best 
creates the conditions where all of that can happen.
```

This mirrors what he said in his very first annual letter:

```markdown
From the beginning, our focus has been on offering our customers compelling value. We realized
that the Web was, and still is, the World Wide Wait. Therefore, we set out to offer customers 
something they simply could not get any other way, and began serving them with books. We brought
them much more selection than was possible in a physical store (our store would now occupy 6
football fields), and presented it in a useful, easy-to-search, and easy-to-browse format in a 
store open 365 days a year, 24 hours a day. We maintained a dogged focus on improving the 
shopping experience, and in 1997 substantially enhanced our store. We now offer customers gift
certificates, 1-Click shopping, and vastly more reviews, content, browsing options, and 
recommendation features. We dramatically lowered prices, further increasing customer value. Word
of mouth remains the most powerful customer acquisition tool we have, and we are grateful for 
the trust our customers have placed in us. Repeat purchases and word of mouth have combined to 
make Amazon.com the market leader in online bookselling.
```

In the 2017 annual letter, Jeff again echoes this with the "divine discontent" quote (see also [high standards](#high-standards)):

```markdown
One thing I love about customers is that they are divinely discontent. Their expectations are
never static — they go up. It’s human nature. We didn’t ascend from our hunter-gatherer days by
being satisfied. People have a voracious appetite for a better way, and yesterday’s ‘wow’ quickly
becomes today’s ‘ordinary’. I see that cycle of improvement happening at a faster rate than ever
before. It may be because customers have such easy access to more information than ever before —
in only a few seconds and with a couple taps on their phones, customers can read reviews, compare
prices from multiple retailers, see whether something’s in stock, find out how fast it will ship
or be available for pick-up, and more. These examples are from retail, but I sense that the same
customer empowerment phenomenon is happening broadly across everything we do at Amazon and most 
other industries as well. You cannot rest on your laurels in this world. Customers won’t have it.

How do you stay ahead of ever-rising customer expectations? There’s no single way to do it – it’s
a combination of many things. But *high standards* (widely deployed and at all levels of detail) are
certainly a big part of it. We’ve had some successes over the years in our quest to meet the high
expectations of customers. We’ve also had billions of dollars’ worth of failures along the way. 
```

Specific product examples from Jeff's [previous year's annual letter](https://ir.aboutamazon.com/static-files/f124548c-5d0b-41a6-a670-d85bb191fcec):

```markdown
Many companies describe themselves as customer-focused, but few walk the walk. Most big technology
companies are competitor focused. They see what others are doing, and then work to fast follow. In contrast, 90
to 95% of what we build in AWS is driven by what customers tell us they want. A good example is our new
database engine, Amazon Aurora. Customers have been frustrated by the proprietary nature, high cost, and
licensing terms of traditional, commercial-grade database providers. And while many companies have started
moving toward more open engines like MySQL and Postgres, they often struggle to get the performance they
need. Customers asked us if we could eliminate that inconvenient trade-off, and that’s why we built Aurora. It
has commercial-grade durability and availability, is fully compatible with MySQL, has up to 5 times better
performance than the typical MySQL implementation, but is 1/10th the price of the traditional, commercial-grade
database engines. This has struck a resonant chord with customers, and Aurora is the fastest-growing service in
the history of AWS. Nearly this same story could be told about Redshift, our managed data warehouse service,
which is the second fastest growing service in AWS history – both small and large companies are moving their
data warehouses to Redshift.

Our approach to pricing is also driven by our customer-centric culture – we’ve dropped prices 51 times, in
many cases before there was any competitive pressure to do so. In addition to price reductions, we’ve also
continued to launch new lower cost services like Aurora, Redshift, QuickSight (our new Business Intelligence
service), EC2 Container Service (our new compute container service), and Lambda (our pioneering server-less
computing capability), while extending our services to offer a range of highly cost-effective options for running
just about every type of application or IT use case imaginable. We even roll out and continuously improve
services like Trusted Advisor, which alerts customers when they can save money – resulting in hundreds of
millions of dollars in savings for our customers. I’m pretty sure we’re the only IT vendor telling customers how
to stop spending money with us
```

The part where you experiment and accept failures and plant seeds etc in the first quote isn't just talk -- Brad Stone covers it in great detail w.r.t. how Prime was created in his 2013 book *The Everything Store*. Again Jeff also covers it in the [previous year's annual letter](https://ir.aboutamazon.com/static-files/f124548c-5d0b-41a6-a670-d85bb191fcec):

```markdown
One area where I think we are especially distinctive is failure. I believe we are the best 
place in the world to fail (we have plenty of practice!), and failure and invention are 
inseparable twins. To invent you have to experiment, and if you know in advance that it’s 
going to work, it’s not an experiment. Most large organizations embrace the idea of invention,
but are not willing to suffer the string of failed experiments necessary to get there. 
Outsized returns often come from betting against conventional wisdom, and conventional wisdom
is usually right. Given a ten percent chance of a 100 times payoff, you should take that bet 
every time. But you’re still going to be wrong nine times out of ten. We all know that if you
swing for the fences, you’re going to strike out a lot, but you’re also going to hit some home 
runs. The difference between baseball and business, however, is that baseball has a truncated 
outcome distribution. When you swing, no matter how well you connect with the ball, the most 
runs you can get is four. In business, every once in a while, when you step up to the plate, 
you can score 1,000 runs. This long-tailed distribution of returns is why it’s important to be 
bold. Big winners pay for so many experiments.

AWS, Marketplace and Prime are all examples of bold bets at Amazon that worked, and we’re 
fortunate to have those three big pillars. They have helped us grow into a large company, and 
there are certain things that only large companies can do. With a tip of the hat to our Seattle 
neighbors, no matter how good an entrepreneur you are, you’re not going to build an all-composite 
787 in your garage startup – not one you’d want to fly in anyway. Used well, our scale enables us 
to build services for customers that we could otherwise never even contemplate. But also, if we’re 
not vigilant and thoughtful, size could slow us down and diminish our inventiveness.
```

This segues naturally into Ben Thompson's article [Divine Discontent: Disruption’s Antidote](https://stratechery.com/2018/divine-discontent-disruptions-antidote/). There are two advantages to this obsessive customer focus, argues Ben:

```markdown
Critically, when it comes to Internet-based services, this customer focus does not come at the 
expense of a focus on infrastructure or distribution or suppliers: while those were the means to
customers in the analog world, in the online world controlling the customer relationship gives a
company power over its suppliers, the capital to build out infrastructure, and control over 
distribution. Bezos is not so much choosing to prioritize customers insomuch as he has unlocked the
key to controlling value chains in an era of aggregation.

Bezos’s letter, though, reveals another advantage of focusing on customers: it makes it impossible 
to overshoot. When I wrote that piece five years ago, I was thinking of the opportunity provided by
a focus on the user experience as if it were an asymptote: one could get ever closer to the ultimate
user experience, but never achieve it. ...

In fact, though, consumer expectations are not static: they are, as Bezos’ memorably states, 
“divinely discontent”. What is amazing today is table stakes tomorrow, and, perhaps surprisingly, 
that makes for a tremendous business opportunity: if your company is predicated on delivering the 
best possible experience for consumers, then your company will never achieve its goal.

In the case of Amazon, that this unattainable and ever-changing objective is embedded in the company’s
culture is, in conjunction with the company’s demonstrated ability to spin up new businesses on the 
profits of established ones, a sort of perpetual motion machine; I’m not sure that Amazon will beat 
Apple to $1 trillion, but they surely have the best shot at two.
```

Eugene also quotes Ben above, and [adds](https://www.eugenewei.com/blog/2018/5/21/invisible-asymptotes) that "you can't overserve on user experience":

```markdown
Pattern recognition is the default operation mode of much of Silicon Valley and other fields, but
it is almost always, by its very nature, backwards-looking. One can hardly blame most people for
resorting to it because it's a way of minimizing blame, and the economic returns of the Valley are
so amplified by the structural advantages of winners that even matching market beta makes for a 
comfortable living.

However, if consumer desires are shifting, it's always just a matter of time before pattern 
recognition leads to an invisible asymptote. One reason startups are often the tip of the spear for
innovation in technology is that they can't rely on market beta to just roll along. Achieving 
product-market fit for them is an existential challenge, and they have no backup plans. Imagine an 
investor who has to achieve alpha to even survive.

You can't overserve on user experience... as a product person, I'd argue, in parallel,
that it is difficult and likely impossible to understand your customer too deeply. Amazon's mission 
to the be the world's most customer-centric company is inherently a long-term strategy because it is
a one with an infinite time scale and no asymptote to its slope.
```

The next most important is to resist managing by proxy -- like process as proxy, or surveys as proxy for customers. This is a direct instance of Goodhart's law, and it's so important I'm thinking of giving it its own subheading, or slotting it under a high-level category called "Goodhart-type failures":

```markdown
As companies get larger and more complex, there’s a tendency to manage to proxies. This comes
in many shapes and sizes, and it’s dangerous, subtle, and very Day 2.

A common example is process as proxy. Good process serves you so you can serve customers. But
if you’re not watchful, the process can become the thing. This can happen very easily in large 
organizations. The process becomes the proxy for the result you want. You stop looking at 
outcomes and just make sure you’re doing the process right. Gulp. It’s not that rare to hear a 
junior leader defend a bad outcome with something like, “Well, we followed the process.” A more 
experienced leader will use it as an opportunity to investigate and improve the process. The 
process is not the thing. It’s always worth asking, do we own the process or does the process 
own us? In a Day 2 company, you might find it’s the second.

Another example: market research and customer surveys can become proxies for customers – 
something that’s especially dangerous when you’re inventing and designing products. “Fifty-five
percent of beta testers report being satisfied with this feature. That is up from 47% in the
first survey.” That’s hard to interpret and could unintentionally mislead.

Good inventors and designers *deeply* understand their customer. They spend tremendous energy 
developing that intuition. They study and understand many anecdotes rather than only the averages
you’ll find on surveys. They live with the design.

I’m not against beta testing or surveys. But you, the product or service owner, must understand
the customer, have a vision, and love the offering. Then, beta testing and research can help you
find your blind spots. A remarkable customer experience starts with heart, intuition, curiosity,
play, guts, taste. You won’t find any of it in a survey.
```

The third is to embrace "powerful external trends quickly", because "fighting them is fighting the future; embrace them and you have a tailwind" -- here Jeff focuses on ML+AI in particular:

```markdown
The outside world can push you into Day 2 if you won’t or can’t embrace powerful trends quickly. 
If you fight them, you’re probably fighting the future. Embrace them and you have a tailwind.

These big trends are not that hard to spot (they get talked and written about a lot), but they 
can be strangely hard for large organizations to embrace. We’re in the middle of an obvious one
right now: machine learning and artificial intelligence.

Over the past decades computers have broadly automated tasks that programmers could describe 
with clear rules and algorithms. Modern machine learning techniques now allow us to do the same
for tasks where describing the precise rules is much harder.

At Amazon, we’ve been engaged in the practical application of machine learning for many years
now. Some of this work is highly visible: our autonomous Prime Air delivery drones; the Amazon 
Go convenience store that uses machine vision to eliminate checkout lines; and Alexa,1 our 
cloud-based AI assistant. (We still struggle to keep Echo in stock, despite our best efforts. A
high-quality problem, but a problem. We’re working on it.)

But much of what we do with machine learning happens beneath the surface. Machine learning drives
our algorithms for demand forecasting, product search ranking, product and deals recommendations,
merchandising placements, fraud detection, translations, and much more. Though less visible, much
of the impact of machine learning will be of this type – quietly but meaningfully improving core
operations.

Inside AWS, we’re excited to lower the costs and barriers to machine learning and AI so
organizations of all sizes can take advantage of these advanced techniques.

Using our pre-packaged versions of popular deep learning frameworks running on P2 compute 
instances (optimized for this workload), customers are already developing powerful systems 
ranging everywhere from early disease detection to increasing crop yields. And we’ve also made
Amazon’s higher level services available in a convenient form. Amazon Lex (what’s inside Alexa),
Amazon Polly, and Amazon Rekognition remove the heavy lifting from natural language understanding, 
speech generation, and image analysis. They can be accessed with simple API calls – no machine 
learning expertise required. Watch this space. Much more to come.
```

The last one is high-velocity decision-making, or what Jeff memorably calls "disagree and commit" -- this is what startups do, but it doesn't scale naturally unless your org design explicitly deals with it:

```markdown
Day 2 companies make high-quality decisions, but they make high-quality decisions slowly. To
keep the energy and dynamism of Day 1, you have to somehow make high-quality, high-velocity 
decisions. Easy for start-ups and very challenging for large organizations. The senior team 
at Amazon is determined to keep our decision-making velocity high. Speed matters in business
– plus a high-velocity decision making environment is more fun too. We don’t know all the 
answers, but here are some thoughts.

First, never use a one-size-fits-all decision-making process. Many decisions are reversible, 
two-way doors. Those decisions can use a light-weight process. For those, so what if you’re
wrong? I wrote about this in more detail in last year’s letter.

Second, most decisions should probably be made with somewhere around 70% of the information you
wish you had. If you wait for 90%, in most cases, you’re probably being slow. Plus, either way,
you need to be good at quickly recognizing and correcting bad decisions. If you’re good at
course correcting, being wrong may be less costly than you think, whereas being slow is going
to be expensive for sure.

Third, use the phrase “disagree and commit.” This phrase will save a lot of time. If you have
conviction on a particular direction even though there’s no consensus, it’s helpful to say, 
“Look, I know we disagree on this but will you gamble with me on it? Disagree and commit?” By 
the time you’re at this point, no one can know the answer for sure, and you’ll probably get a 
quick yes.

This isn’t one way. If you’re the boss, you should do this too. I disagree and commit all the 
time. We recently greenlit a particular Amazon Studios original. I told the team my view: 
debatable whether it would be interesting enough, complicated to produce, the business terms 
aren’t that good, and we have lots of other opportunities. They had a completely different 
opinion and wanted to go ahead. I wrote back right away with “I disagree and commit and hope it
becomes the most watched thing we’ve ever made.” Consider how much slower this decision cycle
would have been if the team had actually had to convince me rather than simply get my commitment.

Note what this example is not: it’s not me thinking to myself “well, these guys are wrong and
missing the point, but this isn’t worth me chasing.” It’s a genuine disagreement of opinion, a
candid expression of my view, a chance for the team to weigh my view, and a quick, sincere 
commitment to go their way. And given that this team has already brought home 11 Emmys, 6 Golden
Globes, and 3 Oscars, I’m just glad they let me in the room at all!

Fourth, recognize true misalignment issues early and escalate them immediately. Sometimes teams
have different objectives and fundamentally different views. They are not aligned. No amount of
discussion, no number of meetings will resolve that deep misalignment. Without escalation, the 
default dispute resolution mechanism for this scenario is exhaustion. Whoever has more stamina 
carries the decision.

I’ve seen many examples of sincere misalignment at Amazon over the years. When we decided to 
invite third party sellers to compete directly against us on our own product detail pages – that
was a big one. Many smart, well-intentioned Amazonians were simply not at all aligned with the 
direction. The big decision set up hundreds of smaller decisions, many of which needed to be 
escalated to the senior team.

“You’ve worn me down” is an awful decision-making process. It’s slow and de-energizing. 
Go for quick escalation instead – it’s better.
```

Jeff expands upon the "never use a one-size-fits-all decision-making process" remark above in the previous annual letter, by distinguishing between two types of decisions:

```markdown
There are some subtle traps that even high-performing large organizations can fall into 
as a matter of course, and we’ll have to learn as an institution how to guard against 
them. One common pitfall for large organizations – one that hurts speed and inventiveness 
– is “one-size-fits-all” decision making.

Some decisions are consequential and irreversible or nearly irreversible – one-way doors
– and these decisions must be made methodically, carefully, slowly, with great 
deliberation and consultation. If you walk through and don’t like what you see on the
other side, you can’t get back to where you were before. We can call these Type 1 
decisions. But most decisions aren’t like that – they are changeable, reversible – they’re
two-way doors. If you’ve made a suboptimal Type 2 decision, you don’t have to live with 
the consequences for that long. You can reopen the door and go back through. Type 2
decisions can and should be made quickly by high judgment individuals or small groups.

As organizations get larger, there seems to be a tendency to use the heavy-weight Type 1 
decision-making *process* on most decisions, including many Type 2 decisions. The end result
of this is slowness, unthoughtful risk aversion, failure to experiment sufficiently, and 
consequently diminished invention.1 We’ll have to figure out how to fight that tendency.
```

<a name="#Dealing-with-message-distortion"></a>
### Dealing with message distortion
([overview](#overview))

Eugene Wei, whose blog [Remains of the Day](https://www.eugenewei.com/) is just great in general, has a post called [Compress to impress](https://www.eugenewei.com/blog/2017/5/11/jpeg-your-ideas) (URL: "JPEG your ideas") that pithily captures and expounds on a bunch of stuff I've vaguely thought about and seen elsewhere. 

The main issue:

```markdown
Even with modern communication infrastructure, however, any modern CEO deals with
amplification and distortion issues with any message. Humans learn about this problem 
very early on by playing telephone or operator, or what I just learned is more
canonically known outside the U.S. as Chinese whispers. One person whispers a message 
in another person's ear, and it's passed on down the line to see if the original phrase
can survive intact to the last person in the chain. Generally, errors accumulate along
the way and what makes it to the end is some shockingly defective copy of the original.

Despite learning this lesson early on, most people in leadership positions still 
underestimate just how pervasive this problem is. This is why any manager or executive 
is familiar with how much time they spend on communicating the same things to different
groups in the organization. It feels like it's all you do sometimes, and yet you still
encounter people who feel like they're in the dark.
```

Since Eugene worked at Amazon back in the day, he falls back to [Jeff Bezos](#jeff-bezos) as an example of a CEO who grokked this issue and came up with good ways to deal with it, in the process singling out the core problem of information flow as being what drives organizational design:

```markdown
I suspect that very early on in his career as CEO, Jeff noticed the Chinese whispers 
problem as the company scaled. Anyone who is lucky enough to lead a successful company 
very quickly senses the impossibility of scaling one's own time to all corners of the 
organization, but Jeff was laser focused on the more serious problem that presented, 
that of maintaining consistent strategy in all important decisions, many of which were
made outside his purview each day. At scale, maintaining strategic alignment feels like
an organizational design problem, but much of the impact of organizational design is
centered around how it impacts information flow.

This problem is made more vexing by not just the telephone game issue but by the human 
inability to carry around a whole lot of directives in their minds. Jeff could spend a 
ton of time in All Hands meetings or with his direct reports and other groups inside
Amazon, explaining his thinking in excruciating detail and hoping it sank in, but then 
he'd never have any time to do anything else.
```

I'm reminded of the term "Jeff bots" from Brad Stone's 2013 book *The Everything Store*, an almost compulsively-readable business book if there ever was one. 

How have people traditionally ensured message integrity when transmitted via the lossy media of oral tradition and hierarchical organizations? Distinctive message encoding:

```markdown
One of these is to encode you message in a very distinctive format. There are many 
rhetorical tricks that have stood the test of time, like alliteration or anadiplosis.
Perhaps supreme among these rhetorical forms is verse, especially when it rhymes. Both 
the rhythm and the rhyme (alliteration intentional) allow humans to compress and recall 
a message with greater accuracy than prose.

    Fe fi fo fum, I smell the blood of an Englishman.
 
It's thought that bards of old could recite epics like Homer's Odyssey entirely from
memory because the stories were in verse form (and through the use of memorization tricks
like memory palaces and visual encoding). I don't know many people who can recite any 
novels from memory, but I've occasionally run across someone who can recite a long poem by 
heart. That's the power of verse.
```

How Jeff encodes his messages distinctively -- Day 1 (more [here](#always-day-1)):

```markdown
I never chatted with Bezos about this, so I don't know if it was an explicit strategy on 
his part, but one of his great strengths as a communicator was the ability to encode the 
most important strategies for Amazon in very concise and memorable forms.

Take one example "Day 1." I don't know when he first said this to the company, but it was 
repeated endlessly all my years at Amazon. It's still Day 1. Jeff has even named one of the
Amazon buildings Day 1. In fact, I bet most of my readers know what Day 1 means, and Jeff 
doesn't even bother explaining what Day 1 is at the start of his letter to shareholders, so 
familiar is it to all followers of the company. Instead, he just jumps straight into talking
about how to fend off Day 2, which he doesn't even need to define because we all can probably
infer it from the structure of his formulation, but he does so anyway.

    Day 2 is stasis. 
    Followed by irrelevance. 
    Followed by excruciating, painful decline. 
    Followed by death. 
    And that is why it is always Day 1.
 
An entire philosophy, packed with ideas, compressed into two words. Day 1.
```

Another example of ideas codifed for maximum recall -- catchy annual themes to remind employees of that year's company goals:

```markdown
Go back even further, and there are dozens of examples of Bezos codifying key ideas for 
maximum recall. For example, every year I was at Amazon had a theme (reminiscent of how 
David Foster Wallace imagined in Infinite Jest that in the future corporate sponsors could
buy the rights to name years). These themes were concise and memorable ways to help 
everyone remember the most important goal of the company that year.

One year, when our primary goal was to grow our revenue and order volume as quickly as
possible to achieve the economies of scale that would capitalize on our high fixed cost
infrastructure investments and put wind into our flywheel, the theme was "Get Big Fast 
Baby." You can argue whether the "baby" at the end was necessary, but I think it's more 
memorable with it than without. Much easier to remember than "Grow revenues 80%" or 
"achieve economies of scale" or something like that.

Another time, as we looked out and saw the $1B revenue milestone approaching, one of Jeff's
chief concerns was whether our company's processes could scale to handle that volume of 
orders without breaking (I'll write another time about the $1B revenue scaling phenomenon). 
To head off any such stumbles, we set aside an entire year at the company for GOHIO. It 
stood for "Getting our house in order".

As the first analyst in the strategic planning group, I produced an order volume projection 
for $1B in revenue and also generated forecasts for other metrics of relevance for every 
group in the company. For example, the customer service department would have to handle a 
higher volume of customer contacts, and the website would have to handle a greater traffic
load.

Every group had that year of GOHIO to figure out how to scale to handle that volume without 
just linearly scaling its headcount and/or spending. If every group were just growing their
headcount and costs linearly with order volume, our business model wouldn't work. The 
exercise was intended to find those processes that would break at such theoretical load and
begin the work of finding where the economies of scale lay. An example was building customer
self-service mechanisms to offload the most common customer service inquiries like printing 
return labels.

I could continue on through the years, but what stands out is that I can recite these from 
memory even now, over a decade later, and so could probably everyone who worked at Amazon 
those years.

Here's a good test of how strategically aligned a company is. Walk up to anyone in the
company in the hallway and ask them if they know what their current top priority or mission 
is. Can they recite it from memory?

What Jeff understood was the power of rhetoric. Time spent coming up with the right words to
package a key concept in a memorable way was time well spent. People fret about what others 
say about them when they're not in the room, but Jeff was solving the issue of getting people 
to say what he'd say when he wasn't in the room.

It was so important to him that we even had company-wide contests to come up with the most 
memorable ways to name our annual themes. One year Jeff announced at an All Hands meeting 
that someone I knew, Barnaby Dorfman, had won the contest. Jeff said the prize was that he'd
buy something off the winner's Amazon wish list, but after pulling Barnaby's wish list up in
front of the whole company on the screen, he said he didn't think any of the items was good 
enough so instead he went over to the product page for image stabilized binoculars from Canon,
retailing for over $1000, and bought those instead.
```

Well-codified ideas need little maintenance. Jeff-relevant:

```markdown
One of the great advantages of identifying and codifying first principles is how little 
maintenance they need. Write once, remember forever. As testament to that, ever year, Bezos
ends his Letter to Shareholders the same way.

As always, I attach a copy of our original 1997 letter. It remains Day 1.
 
It's his annual mic drop. Shareholders must feel so secure with their Amazon shares. Bezos is
basically saying he figured out some enduring principles when he started his company, and 
they're so universal and stable that he doesn't have much to add some twenty years later
except to point people back at his first letter to shareholders.

Other CEO's and leaders I've encountered are gifted at this as well ("Lean in" "Yes we can" 
"Move fast and break things" "Innovation is saying no to a thousand things" "Just do it" "I 
have a dream") but I gravitate to those from Jeff because I saw them arise from distinct needs
in the moment, and not just for notoriety's sake. As such, it's a strategy applicable to more
than just philosophers and CEO's.

...

There will come a day when you'll come up with some brilliant theory or concept and want 
it to spread and stick. You want to lay claim to that idea. It's then that you'll want to
set aside some time to state it distinctively, even if you're not a gifted rhetorician. A
memorable turn of phrase need not incorporate sophisticated techniques like parataxis or 
polysyndeton. Most everyone in tech is familiar with Marc Andreessen's "software is eating
the world" and Stewart Brand's "information wants to be free." Often mere novelty is enough
to elevate the mundane. You've spent all that time cooking your idea, why not spend an 
extra few moments plating it?
```

Examples in other domains:

```markdown
Tyler Cowen named his latest book The Complacent Class. It's a really thought-provoking read,
but the alliteration in the title helps. Now economists everywhere are referring to a broad
set of phenomena by the term "complacent class." It wouldn't be nearly as memorable if called
Complacent People or The Dangers of Self-Satisfaction. Can you name the subtitle of the book?
It's "the self-defeating quest for the American Dream" but no one remembers that part.

Venkatesh Rao once wrote a memorable post about management principles encoded in the American
version of the TV show The Office. Anyone familiar with the post probably remembers it by the
first part of its title: "The Gervais Principle." Very few, I'd suspect, remember the rest of
the title—"Or The Office according to The Office"—though it does employ a clever bit of word 
repetition.

Whatever you think of Hillary Clinton as compared to Donald Trump as Presidential candidates,
I'd venture that more people can recite Trump's mantra—Make America Great Again—than Clinton's.
I don't know if she had a slogan, or if she did I don't remember what it was. Her most
memorable turn of phrase from the campaign trail was probably "then deal me in" at the end of 
a much longer phase, “if fighting for women's healthcare and paid family leave and equal pay is
playing the woman card, then deal me in." It's difficult to think of a phrase more emblematic 
of her problems in articulating what she stood for. The first part of the sentence is long and 
wonky, and I couldn't recall it from memory, and she never followed up on the second enough.

If she'd used it repeatedly in a speech, it could have been a form of epistrophe like Obama's 
"Yes we can" or Martin Luther King's "I have a dream." Imagine if she had an entire speech
where she kept hammering on what other cards she wanted to deal. "If ensuring that everyone in 
the country has an equal opportunity to reasonable healthcare is playing the [?] card then deal
me in. If ensuring everyone in this country has the right to a good education is playing the 
[?] card then deal me in." And so on. But she would only use it once in a while, or once in a 
speech, whereas Obama had entire speeches where he would circle back to "Yes we can" again and 
again. [Maybe there isn't an equivalent to "woman card" that makes this epistrophe scalable but 
the broad point about her weak use of rhetoric holds.]

That's not to say "Make American Great Again" is some slogan for the ages, but it is succinct
and has a loose bit of trochaic meter (MAKE ah-MERIC-uh GREAT a-GAIN) which grants it a sense
of emphatic energy which all political movements need. His supporters compressed it into #MAGA
which became a more liquid shorthand for social media. In general it seems the populist backlash
and the alt-right are stronger at such rhetorical tricks than the Democrats or the left, but 
perhaps it is bred of necessity from being the opposition party?
```

<a name="#Venture-capital"></a>
### Venture capital
([overview](#overview))

<a name="#Typical-route-to-becoming-VC"></a>
### Typical route to becoming VC
([overview](#overview))

From VC Jeff Bussgang of Flybridge Capital's [answer to What's the typical route to becoming a VC?](https://qr.ae/TW1ANA):

```markdown
Generally speaking, there are two on-ramps to the VC world.

One is what I’ll call “The Apprentice” model: go to a top college, get a few years of
working experience, go to a top business school, spend a few more years in a start-up
(typically in product marketing/management) and then join a firm in your late 20s/early
30s as an associate or principal and hope to be accepted as a junior partner into the 
partnership after 4-8 years. During that time, you will probably shadow a few of the
partners, join one or two boards and try to learn the trade from the experienced, senior
partners around you.

The challenge with VCs who follow this path is that the lack of deep operating experience
can potentially be viewed negatively by entrepreneurs. Some entrepreneurs ultimately 
conclude these types of VCs “don’t get it” because they’ve never walked in their shoes.
On the other hand, these “Apprentice” VCs are often very successful investors because 
they are incredibly broad in their range of expertise and analytical in their approaches
to selecting new investments.

The second on-ramp is what I’ll call the “ex-founder/MoneyMaker” model: work your way up 
the start-up ladder, become a VC-backed founder, navigate a successful exit or two and then
join one of the VC firms that backed you and with whom you’ve had a chance to build a
relationship (and make money for) over 5-10 years. This on-ramp sometimes begins with a 
“Venture Partner” title before becoming a full General Partner (i.e., the training wheels 
come off and you have your own checkbook, subject to partnership approval).

The challenge with VCs who follow this path is that they can be accused of viewing their 
VC careers as a lifestyle choice – “the back nine” – and never really go through the hard
work, long hours and long years to learn the trade. After all, this is a business where you
fund lifecycles are measured in decades. Although these types of VCs may have deep knowledge
in the particular domain where they had operating experience, they may not have the breadth
or analytical horsepower to productively invest in the fully broad range of opportunities
most general partners require to be successful. On the other hand, these “ex-founder/
MoneyMaker” VCs have great networks of former employees and business partners and an ability 
to bond with the next generation of young entrepreneurs for whom they can serve as valuable
mentors.

Which path is the more successful one? I have no idea – but I do know that numerous aspiring
VCs who can’t credibly follow one of these two paths have slim odds to entering the industry;
in a world where the odds are slim at any rate. And many LPs are looking for partnerships 
that blend the best of both sides into a single, holistic unit.
```

<a name="#Insurance"></a>
### Insurance
([overview](#overview))

<a name="#Main-function-of-insurance-sector-in-economy"></a>
### Main function of insurance sector in economy
([overview](#overview))

Allen Lobo's [answer to *Do you think insurance is really a waste of money in the long run?*](https://qr.ae/TW1Yi7) is bombastically written rhetoric as usual, so I've edited out the parts where he's flabbergasted at how stupid everyone is, and repeats himself for emphasis (unlike other writers I like, Allen's repetitions get on my nerves). 

```markdown
The primary function of the insurance sector in an economy is not your peace 
of mind.

That is certainly an important thing at the level of the individual consumer.
But it is merely a means to a far more important end.

And that end is the **liquidity of capital in the market.** Because **insurance
multiplies spending power in much the same manner as leverage.** And here’s but
a simple model of how it works.

(You’d hope that people would have learned a very painful lesson in the aftermath
of the 2008 financial crisis about what happens when capital freezes up in the 
economy. When people wondered “Why aren’t consumers and corporations spending all
that money, though they’re sitting on it!”)

...

Say you live in a colony which has a hundred families, each living in their own
house. And of the hundred houses, you know from historical experience that on 
average five of them are severely damaged each year due a combination of whatever
natural/man-made causes. Each house then takes $10,000 to repair. That’s an annual
cost of $50,000 every year. It works out to $500 per house on average per year.

Imagine now that there is no insurance of any kind and each family has to rely on 
charity at best, or just cross their fingers.

What do you think is the first thing which they will do?

**Each one will put aside $10,000 as quickly as they can (if they have any sense).**

Because while they know that there is just a 5% chance of it happening to them in 
any given year, over a twenty year period it will happen once to them (on average). 
So it makes sense to save that up.

But the rub of the deal is that they cannot spend or invest that money anymore, it
has to be kept ready at all times because nobody knows -

1. Which precise houses will be damaged each year.
2. Which houses may be damaged at higher or lower rates than average.

That is $10,000 by a hundred families each - one million dollars in capital.

**Locked down or off-limits for even any medium-term, let alone long-term 
investments.**

Oh and by the way, that’s a best-case scenario.

Remember that some of those homeowners after suffering damage the first time, will
go back to saving $10,000 again because providence does not say to them

“Okay, your house was damaged this year, you’ve had your turn.

You don’t need to worry now for the next 20 years, rest assured that misfortune will
hit only the other ones!”

You can now begin to see how terribly inefficient the system becomes in terms of 
**capital locked down purely based on the unpredictability.**

How on the one hand you will have houses that never get hit in that twenty year 
period but keep $10,000 locked up throughout and then on the other hand households
who get hit more than once in that period and then start setting aside even more 
than $10,000! In the real world, each household will actually set aside more than
$10,000 on average (to account for statistical variance in incidence of damage).

But let’s keep it simple for now to $1 million “under the mattress”.

On the other hand, if each family elects to put $500 each year into a ‘pot’ ($50,000
a year), they can rest assured that any untoward damage will be taken care of for 
the five damaged houses. That’s your “peace of mind” bit. Of course some houses will
get more than they put in (the ones who are damaged multiple times) and some may get
nothing back because they never get damaged. It’s how diversifying risk works.

But more importantly $9,500 is freed up for each family at the start of that 20-year
period and can now be put to use in whatever sense they individually need or want to,
whether that be consumption or investment.

That is effectively $950,000 dollars not extra in the system (that did not change) but
extra flowing around to be allocated to where capital can be put to productive use. 
Like I said, no amount of capital is of any use if it is frozen. 

...

Check out what happens in nations where there isn’t a robust insurance industry. How 
people at all income levels (but especially the middle class) keep as much money as 
they can ‘under the mattress’ metaphorically speaking (i.e. in cash, gold or short-
term liquid investments). That severely hobbles economic growth because there is a 
lot of capital alright but it is all ‘frozen’.

It’s not like those people are acting irrationally. No, it is that the absence of a 
advanced and sophisticated insurance sector forces them to act in a fashion which 
hurts economic growth by freezing capital.

Conversely, even though we don’t think of it as such, a system like the National 
Health Service (NHS) in the U.K. is effectively a health insurance firm. The state
says to the citizens “Pay an extra X% in tax every month and you can rest assured 
that you will be taken care of.” Same with unemployment insurance taxes in places 
like Germany.
```

This needs regulation:

```markdown
It is important not just that insurance fraud be punished severely but that insurers
who cheat their customers or misrepresent their products should be hung out to dry.
Because when people stop trusting that financial institutions will keep their 
promises, they go back to “storing money under the mattress” (as they did in the 
aftermath of 2008).Because if you buy an iPhone and Apple then tries to squirrel out
of their warranty, you can say “Screw them, I’ll buy Samsung from now on.” But if an
insurance company cheats you after you having paid those premiums for all of those 
years, it will financially wreck you!

And also why it is just as important that these institutions not be allowed to play 
around with other people’s money (unless it is that of the ultra-rich who have the
appetite and can afford high risk because that’s not money they’re depending on for
retirement).

That is also why as a man of finance, while I am hardly in favor of excessive 
regulations, it brings my blood to boil when I read of financial institutions
defrauding their customers. Whether that be outright or by soft means like cleverly
disguising the risks of financial instruments. It’s like how most policemen must
feel when they read about corrupt cops.

If the national economy were analogous to the human body, all of the other sectors 
are like individual organs. But the financial sector (retail and investment banks, 
insurance firms, mutual funds and pension funds) is the vascular system.

That is why the tech crash of 2000 stayed localized. Because the contagion was 
largely sequestered. If Amazon went bankrupt tomorrow, you’re not going to have it
rip off and take down entire chunks of the economy. But if AIG or Bank of America 
were to collapse on the other hand? It would send the markets into a complete state
of panic!
```

The answer is necessarily simplified lie-to-children style, but it does make me wonder why all the sophistication when the main function is supposed to be to gain the public's trust so they free up capital so that the market's capital is more liquid. The answer, I suppose, is profit maximization in a competitive and highly regulated industry (price differentiation too perhaps?), but I don't know much more about what the nature of the sophistication is for my speculations to be more than hot air.

<a name="#hiking-and-trails"></a>
## Hiking and trails
([overview](#overview))

<a name="#opinions-on-hiking"></a>
### Opinions on hiking
([overview](#overview))

<a name="#why-say-hello-to-fellow-hikers"></a>
### Why say hello to fellow hikers?
([overview](#overview))

Found this passage from the beginning of Sarah Constantin's essay [The face of the ice](https://srconstantin.wordpress.com/2017/05/30/the-face-of-the-ice/), which is about something else:

```markdown
My father was a serious alpinist in his youth, and taught us a few things about mountains
when we were kids.  For instance: you always say hello to people you pass on the mountain.

Why? Because the mountain can kill you.  “Hi” means “You are a human like me, and this 
mountain is our common threat. If you scream for help, I will come for you.”

There’s a certain solidarity between humans that emerges when we’re faced with a potentially
hostile natural world.  If we passed on the street, we’d be strangers. There’d be no bond
between us.  We might even engage in conflict, under the right circumstances. But on the 
mountain, all of that falls away.  By default, in the wilderness, a human face is a friendly 
face, a glad thing to see if you are lost or hurt.

If you are in the desert and you see someone obviously suffering from dehydration, you’ll
share your extra water with them. It won’t feel like some kind of altruism or charity, it
will feel *obvious*. It’s instructive, if you’re used to thinking of giving as an 
unpleasant duty, to experience some situations where it’s *natural* to be kind.  Kindness 
becomes practical and natural and obvious when the physical environment is hostile.  
Suddenly everything becomes simple: it’s human life against bitter nature, and nothing
else matters.  “All men are brothers” becomes a concrete reality.

There’s something clarifying about man vs. nature situations, even at the minimal level you
can experience by hiking a technical scramble alone.  I’ve found that a certain alertness
kicks in when I have to figure out where to put my hands and feet; I’m scared of falling, 
I have a heightened awareness of physical/spatial reality, and I don’t care at *all* about
looking foolish or getting dirt on my clothes, because *the important thing is to get off 
the damn mountain without any injuries*.  I also am much less lazy; “get to the top” or “get
to the bottom” make it feel natural to push a lot harder than “run X miles” or “lift X 
pounds,” almost as though reaching topographical milestones taps into some primal source of
motivation.  Mountains make life more *real* than it usually is in civilized life.

There’s a traditional overlap between mountain climbing and math; the Russians had their 
math camps in the Urals for decades. Alan Turing, Sophus Lie, Niels Abel , and many others
were avid hikers; a disturbing number of mathematicians have died in hiking accidents.  If
I can speculate about the connection, it might have something to do with love of solitude,
tolerance for pain and effort, or this heightened-reality effect from spatial problem-
solving.  I certainly get a disproportionate number of good ideas while on runs, bike rides,
hikes, or long walks alone.
```

<a name="#Great-trails"></a>
### Great trails
([overview](#overview))

<a name="#tuttle-creek-to-mt-langley"></a>
### Tuttle Creek to Mt Langley
([overview](#overview))

I can't believe I've never heard of this trail before stumbling upon [this SummitPost report](https://www.summitpost.org/tuttle-creek-to-mount-langley/314792). Cactus to Clouds has more gain of course, but it's also gentler. Even the steep initial portion of C2C, 8,000' in 11 miles, pales in comparison to TCL's 7,100' in a relentless, astonishing 4.5 miles -- and a lot of that includes gentle slopes, which means there are even steeper sections, needing Class 4 bouldering (although the hiker contends you can figure out a route around those, he didn't have to look because those sections were easy enough already). The hiker (can't figure out his name) took 7.5 hours to get to the top "at a fast pace" and 3.5 hours down. 

The overview is very understated. It just assumes you're a next-level hiker:

```markdown
This is a trail description that starts at Tuttle Creek trail head and goes to the peak of
Mount Langley. This trail is 4.5 miles (9 miles round trip). The trail head is at about 6,900'
and Mount Langley is 14,026', for a gain of over 7,100'. It is a very strenuous day hike or 
could be done as an overnight. This is an alternate route to the more popular route over New
Army Pass, it also gains more elevation, so is more difficult. This route starts at the Tuttle
Creek trail head and follows along Tuttle Creek, but just to the Ashram, after that it vears
away up an avalache chute.
```

I actually found out about it in an offhand remark by a commenter in the Mt Whitney FB Group on a post bitching about inexperienced hikers endangering themselves by attempting Mt Whitney and snagging permits from more experienced (and now disgruntled) hikers. The commenter said to try this route out, since (1) it doesn't require permits (2) the trailhead is right next to Lone Pine:

```markdown
Tuttle Creek trail head is just a few miles out of Lone Pine. Head up Whitney Portal and turn
South on Horseshoe Meadows Road. Turn West on Granite View Drive. You will now be looking up 
the beautiful and rugged Tuttle Creek Canyon, a rock climbers paradise. Pass south of the 
campground, after you pass a house right next to the South side of the road, start watching for 
a branch in the road on the North side that heads up toward the canyon. The road skirts along 
the canyon, high above the stream and is passable with two wheel drive. There is a turn around 
at the trail head. 
```

Apparently the couloir is 3 times the size of the chute on Mt Whitney's MR, per this report:

```markdown
Conditions were hot, clear, and dry. After departing the well-defined trail to the Ashram, 
I bushwhacked for an hour and only went about a quarter mile and 700 ft gain due to my poor
route-finding and trail-recognition skills. 

Finally getting back to the west side of the creek, there appeared no defined trail, however
navigation is easy from this point on - it's just a long sand slog for the next few miles and
5000 ft of elevation gain. 

I don't know an exact elevation, but there was no water in the creek once above 9000-10000 ft.
There was a small snowfield along the route around 11500 ft, which I used to refill my
reservoir both ascending and descending. 

Langley comes into view only once you gain the ridgeline around 12000 ft., the top of its 
characteristic eastern couloir still holding some snow. As I gain elevation and more of the
couloir comes into view, I am glad I chose the sand-slog - the couloir is mostly bare and the
talus scramble would've been no fun (this couloir is 3 times the size of the chute on Whitney's
Mountaineers Route, and that talus scramble is a pain).
```

[This](http://peaksforfreaks.blogspot.com/2013/05/tuttle-peak-mt-langley-via-northeast.html) is the best website/report I've found.

<a name="#games"></a>
## Games
([overview](#overview))

<a name="#board-games"></a>
### Board games
([overview](#overview))

Found a [Google+ post](https://groups.google.com/forum/?hl=en#!topic/fa.shogi/3aMMKIoYWhQ) by chess GM Larry Kaufman comparing various games in the chess/shogi family, apparently in response to a thread/interest in these comparisons that I can't find. 

His credentials:

```markdown
I am the only person in the world to
have earned a 2400 rating in both chess and shogi,  being an International
Master in the former and an Amateur 5 Dan in the latter. I was once
thought to be the strongest non-oriental player in the U.S. of Shang-chi
(Chinese chess), and have played roughly ten games each of Junk-ki (Korean
chess), Chu-Shogi, and Grand Chess (the modernized version of Capablanca's
10x10 chess), enough to have some feel for the good and bad points of each.
```

Very nice! Okay, so what are some key points to consider in this comparison?

```markdown
In my opinion the key points to consider in comparing the games are the

- frequency of draws in games between masters (less is better, though perhaps
a small percentage of draws may be preferred by some to none at all), 

- rough equality of chances of the two sides

- the importance of memorizing opening theory (less is better), 

- variety of play (a major objection to checkers and some might say to Go), 

- history and tradition (very desirable), 

- game length (not too short or too long, though this is subjective), 

- strategical principles (more are better), and 

- early interaction between the two sides (desirable, as if you can just do
your own thing without looking at the other player's moves, the game lacks interest).
```

On to the comparison then. Chess:

```markdown
Let's start with chess, the most widely played game (geographically) of
the family.  It ranks very highly on history and tradition, game length,
strategical principles, and early interaction.  Unfortunately the draw
percentage is too high (around 50% at high levels), and this is mostly due
to the nature of the game rather than to lack of fighting spirit.  The
chances of the two players are quite unequal, white winning about 5 games
for each 3 won by black at high level.  Memorized opening theory is way too
important at high level, though ideas like shuffle chess could solve this
problem.  Variety of play is not bad but could be much better.  So chess
gets 4 1/2 good grades out of 8.  Shuffle chess would score the same,
gaining a point on memorized theory but losing it back on history and
tradition, of which it has none.
```

Chinese chess and Korean chess:

```markdown
Now consider Chinese chess, the version of chess played by the largest
number of people world-wide, I believe.  It also ranks very highly on
history and tradition, game length, and early interaction.  I'll give it a
medium score on strategical principles (there's plenty of strategy, but less
than chess, I feel).  The draw percentage is perhaps a bit lower than in
chess, but still too high (the restriction of the elephants and ministers to
their own camp is the main reason for the draws, I believe).  The first
player has a substantial edge, though perhaps a bit less than in chess.
Memorized theory is a big problem, as in chess.  Variety of play is about
like in chess.  So I'll give Chinese chess the same 4 1/2 score as chess
got.

Korean chess is a relative of Chinese chess.  It scores a bit lower on
history and tradition, and a bit higher on the memorized theory problem,
with other scores about the same.  Let's also give it 4 1/2.
```

Chu-shogi and larger versions:

```markdown
Okay, how about Chu-shogi, the topic of much discussion on this list.
It certainly has history and tradition, though most of it is lost to us now,
so let's give it 1/2 for this.  I suspect that the percentage of draws among
masters would be very low, though I don't believe there are any masters in
the world now to test this hypothesis.  Similarly I cannot imaging that the
first move could be more than a trivial advantage, perhaps 51-49%.
Memorized opening theory is obviously not a problem; even if it existed, it
is very unlikely that this would ever be a decisive factor in such a complex
and long game.  Variety of play is obviously enormous; in fact I'll only
give it 1/2 credit because the variety of moves of the different promoted
and unpromoted pieces is far more than anyone would ever need to enjoy the
game, and simply serves to lower the standard of play by making it difficult
to ever become proficient with all the different pieces.  Game length is
much longer than most people would consider desirable, though the game is
certainly of playable length.  Early interaction certainly can occur, though
the space between the camps minimizes it, so I'll give Chu half credit here.
As for strategical principles, in my opinion there are not so many here, as
the tactical element seems to dominate the game, but I'll give it half
credit, mostly due to my not being expert enough to say for sure.  So I give
Chu 5 points, the best score so far, with the reservation that one would
have to devote an enormous amount of time to the game to acquire any real
proficiency.  I do enjoy playing the game on occasion, but since I have not
played enough to know the moves of the promoted pieces without reference to
the manual, both my skill and my enjoyment go way down late in the game.

As for the larger relatives of Chu, I must agree with  Colin Adams that
they are clearly less playable than Chu without offsetting advantages, and
so I find the constant discussion of these "games" to be rather silly.  I am
quite in agreement with George Hodges in the opinion that the really large
versions were not really meant to be played at all.  In particular versions
in which pieces demote on promotion would simply be drawish and boring.  Chu
shogi already has too many pieces, probably the reason it died out, so even
larger versions must simply be a joke.
```

Grand chess, the second-best of the lot by Kaufman's lights:

```markdown
As chu is to shogi, Grand chess is to chess.  The larger board and
extra pieces (bishop + knight and rook + knight) add a whole new dimension
to the game.  I'll have to give it a zero for history and tradition (a few
games by Capablanca don't qualify it here).  I believe the draw percentage
would be very low among masters (I haven't had one yet), and the advantage
of first move small enough.  Memorized theory doesn't exist, though it could
become a bit of a problem if the game became popular, so I'll give it 1/2
here.  Variety of play is good, more than chess without reaching the point
of overkill as with Chu, but perhaps still a bit less than I would like, so
I'll give it 3/4.  Game length seems about right to me, a bit more than
chess but nothing like Chu.  Early interaction is the same as in chess, and
the strategical principles should be similar.   So Grand chess, despite its
meager following, scores an amazing 6 1/4 out of 8 on my criteria, by far
the best so far.  It really is an excellent game and deserves a bigger
following.
```

Shogi, the best outright:

```markdown
Now for shogi, as it is currently played by millions of Japanese and a
few thousand Westerners.  History and tradition are there in abundance,
comparable to chess.  The draw % (about 2% in pro play, 1% in amateur) is
minimal (some might argue it's too low!).  The advantage of first move is
minimal (about 52-48%).  Variety of play is nearly ideal (ten piece types,
including promoted rook and bishop, versus six in chess).  Memorized theory
is a big problem, nearly as much as in chess, though the chances of turning
around a bad opening are better in shogi, so I'll give it 1/4 (maybe we need
shuffle-shogi !). Game length is ideal.  Early interaction is adequate,
though a bit less than in chess, so I'll give it 3/4.  Strategic principles
are quite ample, perhaps on a par with chess.  So shogi gets 7  out of 8,
making it clearly the winner of this "competition".

     Shogi is not a perfect game.  Some criticisms include the rather
arbitrary moves of some pieces, the occasional draw due to there being no
good way to start the fight in certain openings, the very unaesthetic need
to resolve impasse games by point count, and the fact that many games begin
with both sides moving into identical fortress formations before any
interaction occurs.  Also the strength of the Left Anaguma castle is felt by
many to be a spoiler in shogi, as for a while it seemed to relegate the
ranging rook openings  to the dustbin of history, though the recent success
of Fujii with his anti-Anaguma system seems to puncture a big hole in that
criticism.  Despite these criticisms, I think the evidence is strong that
shogi is the best game in the entire chess family, and with the risk of
offending Go players (a game which I also play and respect greatly), perhaps
the best game of all.
```

Michael Vaniver adds:

```markdown
Playing shogi is much more fun than playing chess for a novice.  The drop
rule permits one to create the position one wants in many cases, which
means that many tactical themes that occur only rarely in chess can be
"manufactured" using drops in shogi.  I also think that the fact that pawns
cannot capture forward in chess tends to create blockaded positions which
sometimes makes it hard to get any attack going.  What usually seems to
happen in these cases is that pieces get exchanged and before you know it
you're in the endgame.  Thus chess games often become a war of
attrition. This can't happen in shogi, and shogi endgames are vastly more
interesting than chess endgames by any reasonable comparison.

I agree with your assessment from my experience.  I think the only "flaw"
in shogi is the standardization of the openings, and that something like
"shuffle-shogi" will be necessary to keep the game from getting bogged down
by opening theory.  Has anyone played shuffle-shogi?  I must add that most
of what I've read on shogi seems to focus on tactics, giving the idea that
the game is primarily tactical.  I suppose this is true in the endgame, but
it would be interesting to read more about shogi strategy beyond analysis
of the opening.

I haven't played Chu (yet; I'm eager to try), but I find it hard to imagine
that a game played on a 12x12 board with 92 pieces could have less
strategical complexity than chess, played on an 8x8 board.  Perhaps you
could elaborate here?  Wayne Schmittberger has argued that the tactical
complexity of Chu is so great that, in fact, strategy dominates tactics
because it's hard to read many moves deep, and so the important thing for
Chu players is to learn to intuitively assess the merits of a position,
much as Go players have to.
```

<a name="#Game-complexity-measures"></a>
### Game complexity measures
([overview](#overview))

Found an interesting notion of "game depth" by renowned computer scientist and chess IM RJ Lipton in his post [The New Chess World Champion](https://rjlipton.wordpress.com/2014/12/28/the-new-chess-world-champion/). First the motivation/appetizer:

```markdown
GM Larry Kaufman and I have had closely similar chess ratings for four decades. However, in
the last game we played he gave me odds of rook and bishop and beat me handily. Then he told 
me that the world champion could probably beat him giving the same odds.

This was not Western chess, where I would be pretty confident of beating anyone given just an
extra bishop. It was Japanese chess, called Shogi. Shogi has no piece with the power of a queen,
and the armies have just one rook and bishop each, so the odds I received were maximal. The main
difference from Western chess is that captured pieces become their taker’s property and can be 
“paratrooped” back into the game. This prevents the odds receiver from winning by attrition 
through exchanges as prevails in chess, and throws upon the leader a burden of attack.

It also makes Shogi *deeper* than chess in a way that can be defined mathematically.
```

Ooh, but what is this? Say more words!

```markdown
Say two players are a "class unit" apart if the stronger expects to score 75% against the
weaker in a match. In chess, this corresponds to a difference of almost exactly 200 points
in the standard Elo rating system. László Mérő, in his 1990 book Ways of Thinking, called 
the number of class units from a typical beginning adult player to the human world champion
the "depth" of a game.

Tic-tac-toe may have a depth of 1: if you assume a beginner knows to block an immediate threat
of three-in-a-row but plays randomly otherwise, then you can score over 75% by taking a corner
when you go first and grifting a few games when you go second. Another school-recess game, 
dots-and-boxes, is evidently deeper. We don’t know its depth for sure because it doesn’t have 
a rating system and championship format like chess does.

Chess ratings in the US go all the way down to the formal floor of 100 among scholastic players,
but I concur with the estimate of Elo 600 for a young-adult beginner by a discussion of Mérő’s 
book which I saw in the 1990s but did not preserve. This gave chess a depth of 11 class units up
to 2800, which was world champion Garry Kasparov’s rating in 1990. If I recall correctly, 
checkers (8x8) and backgammon had depth 10 while bridge tied chess at 11, but Shogi scored 14 and
yet was dwarfed by Japan’s main head game, Go, at 25.
```

Whoa, *seriously*? Go's depth is 25?? 

How this relates to computer players:

```markdown
Although it is coming on 18 years since Deep Blue beat Kasparov, humans are still barely fending
off computers at shogi, while we retain some breathing room at Go. Since depth 14 translates to 
Elo 3400 on the chess scale, while Komodo 8 is scraping 3300 on several chess rating lists, This 
feels about right.

Ten years ago, each doubling of speed was thought to add 50 Elo points to strength. Now the 
estimate is closer to 30. Under the double-in-2-years version of Moore’s Law, using an average of
50 Elo gained per doubling since Kasparov was beaten, one gets 450 Elo over 18 years, which again
checks out.

To be sure, the gains in computer chess have come from better algorithms not just speed, and 
include nonlinear jumps, so Go should not count on a cushion of (25 – 14)*9 = 99 years.
```

Indeed it shouldn't. The post was written in 2014; AlphaGo beat the strongest player in the world only 3-4 years later.

Weakness of this metric:

```markdown
One weakness in the notion of depth is dependence on how contests are aggregated. For tennis, 
should we apply 75% expectation to games, pairs of games, sets, or matches—and how many games 
in a set or sets in a match? 

Another is that any game can be ‘reduced’ to depth 1 by flipping a coin; if heads the game is 
played as-usual; if tails a second coin flip defines the outcome. Then nobody ever has more
than 75% expectation. I regard both as beside the point for most board games, but both become
nontrivial for games like backgammon that involve chance and are played with match stakes.
```

<a name="#Doing-good"></a>
## Doing good
([overview](#overview))

<a name="#Effective-altruism"></a>
### Effective altruism
([overview](#overview))

<a name="#Weird-EA"></a>
### Weird EA
([overview](#overview))

The subheading for this section makes it sound like I'm making fun of the weirder parts of EA. Rest assured I'm not -- I am weird myself, and have always loved and sought out the weird. (That's what pretty much half this document *is*.)

The following are from Scott Alexander's [Fear and Loathing at EA Global 2017](https://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/), one of the more moving posts on EA he's written (and he's written a lot on this). 

Context:

```markdown
Effective altruism is the movement devoted to finding the highest-impact ways to help 
other people and the world. Philosopher William MacAskill described it as “doing for 
the pursuit of good what the Scientific Revolution did for the pursuit of truth”. They 
have an annual global conference to touch base and discuss strategy. This year it was 
in the Palace of Fine Arts in San Francisco, and I got a chance to check it out.

The official conference theme was “Doing Good Together”. The official conference 
interaction style was “earnest”. The official conference effectiveness level was “very”.
And it was impossible to walk away from some of the talks without being impressed. ...

The whole conference was flawlessly managed, from laser-fast registration to polished-
sounding speakers to friendly unobtrusive reminders to use the seventeen different apps
that would keep track of your conference-related affairs for you. And the of course the
venue, which really was amazing.
```

The "underbelly of the movement", where the weird stuff is:

```markdown
But walk a little bit outside of the perfectly-scheduled talks, or linger in the common
areas a little bit after the colorfully-arranged vegetarian lunches, and you run into the
shadow side of all of this, the hidden underbelly of the movement.

William MacAskill wanted a “scientific revolution in doing good”. But the Scientific
Revolution progressed from “I wonder why apples fall down” to “huh, every particle is in
an infinite number of places simultaneously, and also cats can be dead and alive at the 
same time”. The effective altruists’ revolution started with “I wonder if some charities
work better than others”. But even at this early stage, it’s gotten to some pretty weird
places.

I got to talk to some people from Wild Animal Suffering Research. They start with the 
standard EA animal rights argument – if you think animals have moral relevance, you can 
save zillions of them for almost no cost. A campaign for cage-free eggs, minimal in the 
grand scheme of things, got most major corporations to change their policies and gave two
hundred million chickens an improved quality of life. But WASR points out that even this
isn’t the most neglected cause. There are up to a trillion reptiles, ten quintillion
insects, and maybe a sextillion zooplankton. And as nasty as factory farms are, life in the
state of nature is nasty, brutish, short, and prone to having parasitic wasps paralyze you
so that their larvae can eat your organs from the inside out while you are still alive. 
WASR researches ways we can alleviate wild animal suffering, from euthanizing elderly 
elephants (probably not high-impact) to using more humane insecticides (recommended as an 
‘interim solution’) to neutralizing predator species in order to relieve the suffering of
prey (still has some thorny issues that need to be resolved).

Wild Animal Suffering Research was nowhere near the weirdest people at Effective Altruism
Global.

I got to talk to people from the Qualia Research Institute, who point out that everyone 
else is missing something big: the hedonic treadmill. People have a certain baseline 
amount of happiness. Fix their problems, and they’ll be happy for a while, then go back to 
baseline. The only solution is to hack consciousness directly, to figure out what exactly
happiness is – unpack what we’re looking for when we describe some mental states as having 
higher positive valence than others – and then add that on to every other mental state
directly. This isn’t quite the dreaded wireheading, the widely-feared technology that will
make everyone so doped up on techno-super-heroin (or direct electrical stimulation of the 
brain’s pleasure centers) that they never do anything else. It’s a rewiring of the brain
that creates a “perpetual but varied bliss” that “reengineers the network of transition 
probabilities between emotions” while retaining the capability to do economically useful 
work. Partly this last criteria is to prevent society from collapsing, but the ultimate
goal is:

    …the possibility of a full-fledged qualia economy: when people have spare
    resources and are interested in new states of consciousness, anyone good at
    mining the state-space for precious gems will have an economic advantage. In 
    principle the whole economy may eventually be entirely based on exploring the 
    state-space of consciousness and trading information about the most valuable
    contents discovered doing so.

The Qualia Research Institute was nowhere near the weirdest people at Effective Altruism 
Global.

I got to talk to some people from the Foundational Research Institute. They do a lot of 
research, and a lot of it is very good, but they’re most infamous within the community for
their particle work. It goes like this: the universe is really really big. So if suffering 
made up an important part of the structure of the universe, this would be so tremendously 
outrageously unconscionably bad that we can’t even conceive of how bad it could be. So the
most important cause might be to worry about whether fundamental physical particles are 
capable of suffering – and, if so, how to destroy physics. From their writeup:

    Speculative scenarios to change the long-run future of physics may dominate any
    concrete work to affect the welfare of intelligent computations — at least within 
    the fraction of our brain’s moral parliament that cares about fundamental physics. 
    The main value (or disvalue) of intelligence would be to explore physics further 
    and seek out tricks by which its long-term character could be transformed. 
    
    For instance, if false-vacuum decay did look beneficial with respect to reducing
    suffering in physics, civilization could wait until its lifetime was almost over 
    anyway (letting those who want to create lots of happy and meaningful intelligent 
    beings run their eudaimonic computations) and then try to ignite a false-vacuum 
    decay for the benefit of the remainder of the universe (assuming this wouldn’t 
    impinge on distant aliens whose time wasn’t yet up). Triggering such a decay might
    require extremely high-energy collisions — presumably more than a million times 
    those found in current particle accelerators — but it might be possible. On the 
    other hand, such decay may happen on its own within billions of years, suggesting
    little benefit to starting early relative to the cosmic scales at stake. 
    
    In any case, I’m not suggesting vacuum decay as the solution — just that there may be 
    many opportunities like it waiting to be found, and that these possibilities may
    dwarf anything else that happens with intelligent life.
```

Scott subscribes to the Lovecraftian "it was not meant that we should voyage far" from our "placid island of ignorance in the midst of black seas of infinity":

```markdown
We live on a placid island of ignorance in the midst of black seas of infinity, 
and it was not meant that we should voyage far. The sciences, each straining in 
its own direction, have hitherto harmed us little; but some day the piecing 
together of dissociated knowledge will open up such terrifying vistas of reality, 
and of our frightful position therein, that we shall either go mad from the 
revelation or flee from the deadly light into the peace and safety of a new dark age.
```

Scott adds:

```markdown
Morality wasn’t supposed to be like this. Most of the effective altruists I met were 
nonrealist utilitarians. They don’t believe in some objective moral law imposed by an 
outside Power. They just think that we should pursue our own human-parochial moral values
effectively. If there was ever a recipe for a safe and milquetoast ethical system, that
should be it. And yet once you start thinking about what morality is – really thinking,
the kind where you try to use mathematical models and formal logic – it opens up into 
these dark eldritch vistas of infinities and contradictions. The effective altruists
started out wanting to do good. And they did: whole nine-digit-sums worth of good,
spreadsheets full of lives saved and diseases cured and disasters averted. But if you 
really want to understand what you’re doing – get past the point where you can catch 
falling apples, to the point where you have a complete theory of gravitation – you end 
up with something as remote from normal human tenderheartedness as the conference lunches
were from normal human food.
```

And this is the money quote that made me start this section in the first place, because it touched me deeply (perhaps part of it is simply Scott's rhetoric working on me?):

```markdown
But I worry I’m painting a misleading picture here. It isn’t that effective altruism is
divided into two types of people: the boring effective suits, and the wacky explorers of
bizarre ethical theories. I mean, there’s always going to be *some* division. But by and 
large these were the same people, or at least you couldn’t predict who was who. They 
would go up and give a talk about curing river blindness in Nigeria, and then you’d catch 
them later and learn that they were worried that maybe the most effective thing was 
preventing synthetic biology from taking over the ecosystem. Or you would hear someone 
give their screed, think “what a weirdo”, and then learn they were a Harvard professor who 
served on a bunch of Fortune 500 company boards. Maybe the right analogy would be physics.
A lot of physicists work on practical things like solar panels and rechargeable batteries.
A tiny minority work on stranger things like wormholes and alternate universes. But it’s 
not like these are two different factions in physics that hate each other. And every so 
often a solar panel engineer might look into the math behind alternate universes, or a
wormhole theorist might have opinions on battery design. They’re doing really different 
stuff, but it’s within the same tradition.

The movement’s unofficial leader is William MacAskill. He’s a pretty typical overachiever
– became an Oxford philosophy professor at age 28 (!), founded three successful non-profits,
and goes around hobnobbing with rich people trying to get them to donate money (he himself
has pledged to give away everything he earns above $36,000). I had always assumed he was 
just a random dignified suit-wearing person who was slightly exasperated at having to put 
up with the rest of the movement. But I got a chance to talk to him – just for a few minutes,
before he had to run off and achieve something – and I was shocked at how much he knew about
all the weirdest aspects of the community, and how protective he felt of them. And in his 
closing speech, he urged the attendees to “keep EA weird”, giving examples of times when 
seemingly bizarre ideas won out and became accepted by the mainstream.

If it were just the senior research analysts at their spreadsheets, we could dismiss them as
the usual Ivy League lizard people and move on. If it were just the fringes ranting about 
cyber-neuro-metaphilosophy, we could dismiss them as loonies and forget about it. And if it 
were just the two groups, separate and doing their own thing, we could end National Geographic-
style, intoning in our best David Attenborough voice that “Effective Altruism truly is a land
of contrasts”. But it’s more than that. Some animating spirit gives rise to the whole thing, 
some unifying aesthetic that can switch to either pole and back again on a whim. After a lot
of thought, I have only one guess about what it might be.

I think the effective altruists are genuinely good people.

Over lunch, a friend told me about his meeting with an EA philosopher who hadn’t been able 
to make it to the conference. This friend had met the philosopher, and as they were walking,
the philosopher had stopped to pick up worms writhing on the sidewalk and put them back in 
the moist dirt.

And this story struck me, because I had taken a walk with one of the speakers earlier, and 
seen her do the same thing. She had been apologetic, said she knew it was a waste of her 
time and mine. She’d wondered if it was pathological, whether maybe she needed to be checked
for obsessive compulsive disorder. But when I asked her whether she wanted to stop doing it,
she’d thought about it a little, and then – finally – saved the worm.

And there was a story about the late great moral philosopher Derek Parfit, himself a member
of the effective altruist movement. This is from Larissa MacFarquhar:

    As for his various eccentricities, I don’t think they add anything to an understanding 
    of his philosophy, but I find him very moving as a person. When I was interviewing him 
    for the first time, for instance, we were in the middle of a conversation and suddenly 
    he burst into tears. It was completely unexpected, because we were not talking about 
    anything emotional or personal, as I would define those things. I was quite startled, 
    and as he cried I sat there rewinding our conversation in my head, trying to figure out
    what had upset him. Later, I asked him about it. It turned out that what had made him 
    cry was the idea of suffering. We had been talking about suffering in the abstract. I 
    found that very striking.

    Now, I don’t think any professional philosopher is going to make this mistake, but 
    nonprofessionals might think that utilitarianism, for instance (Parfit is a utilitarian),
    or certain other philosophical ways of think about morality, are quite unemotional, 
    quite calculating, quite cold; and so because as I am writing mostly for nonphilosophers,
    it seemed like a good corrective to know that for someone like Parfit these issues are 
    extremely emotional, even in the abstract.

    The weird thing was that the same thing happened again with a philosophy graduate student
    whom I was interviewing some months later. Now you’re going to start thinking it’s me, 
    but I was interviewing a philosophy graduate student who, like Parfit, had a very
    unemotional demeanor; we started talking about suffering in the abstract, and he burst 
    into tears. I don’t quite know what to make of all this but I do think that insofar as 
    one is interested in the relationship of ideas to people who think about them, and not 
    just in the ideas themselves, those small events are moving and important.
    
I imagine some of those effective altruists, picking up worms, and I can see them here too. 
I can see them sitting down and crying at the idea of suffering, at allowing it to exist.

Larissa MacFarquhar says she doesn’t know what to make of this. I think I sort of do. I’m not
much of an effective altruist – at least, I’ve managed to evade the 80,000 Hours coaches long
enough to stay in medicine. But every so often, I can see the world as they have to. Where 
the very existence of suffering, any suffering at all, is an immense cosmic wrongness, an 
intolerable gash in the world, distressing and enraging. Where a single human lifetime seems
frighteningly inadequate compared to the magnitude of the problem. Where all the normal 
interpersonal squabbles look trivial in the face of a colossal war against suffering itself, 
one that requires a soldier’s discipline and a general’s eye for strategy.

All of these Effecting Effective Effectiveness people don’t obsess over efficiency out of 
bloodlessness. They obsess because the struggle is so desperate, and the resources so few. 
Their efficiency is military efficiency. Their cooperation is military discipline. Their unity 
is the unity of people facing a common enemy. And they are winning. Very slowly, WWI trench-
warfare-style. But they really are.
```

<a name="#Amazing-people"></a>
## Amazing people
([overview](#overview))

<a name="#donald-trump"></a>
### Donald Trump
([overview](#overview))

(I don't think DJT is amazing -- precisely the opposite. I just couldn't resist including the quotes below, and the only main category I have that's remotely related to people is this one.)

From Scott Alexander's [book review](https://slatestarcodex.com/2016/03/19/book-review-the-art-of-the-deal/) of DJT's *The Art of the Deal*, written in March 2016 before he became president. 

This quote contrasts DJT with Robert Cialdini of *Influence*:

```markdown
Trump is no psychology expert, but after a few months of attributing his victories to 
blind luck, most people have accepted Scott Adams’ hypothesis that he’s really a “master
persuader”. Salon, Daily Caller, Bill Maher, and the Economist all use the word “genius”.
The less you respect Trump’s substance – and I respect it very little – the more you’re 
forced to admire whatever combination of charisma, persuasion, and showmanship he uses 
to succeed without having any.
```

DJT thinks deal-making is an innate ability:

```markdown
More than anything else, I think deal-making is an ability you’re born with. It’s in the
genes…unlike the real estate evangelists you see all over television these days, I can’t
promise you that by following the precepts I’m about to offer you’ll become a millionaire 
overnight. Unfortunately, life rarely works that way, and most people who try to get rich
quick end up going broke instead.
```

DJT's "truthful hyperbole" tactic:

```markdown
One thing I’ve learned about the press is that they’re always hungry for a good story,
and the more sensational the better. It’s in the nature of the job, and I understand that.
The point is that if you are a little different, or a little outrageous, or if you do 
things that are bold or controversial, the press is going to write about you…

The funny thing is that even a critical story, which may be hurtful personally, can be 
very valuable to your business. [When I announced my plans to build Television City to 
the press], not all of them liked the idea of the world’s tallest building. But the point
is that we got a lot of attention, and that alone creates value.

The other thing I do when I talk with reporters is to be straight. I try not to deceive 
them or to be defensive, because those are precisely the ways most people get themselves
into trouble with the press. Instead, when a reporter asks me a tough question, I try to
frame a positive answer, even if that means shifting the ground. For example, if someone 
asks me what negative effects the world’s tallest building might have on the West Side, 
I turn the tables and talk about how New Yorkers deserve the world’s tallest building, 
and what a boost it will give the city to have it again. When a reporter asks why I build
only for the rich, I note that the rich aren’t the only ones who benefit from my buildings.
I explain that I put thousands of people to work who might otherwise be collecting
unemployment, and that I add to the city’s tax base every time I build a new project. I
also point out that buildings like Trump Tower have helped spark New York’s renaissance.

The final key to the way I promote is bravado. I play to people’s fantasies. People may 
not always think big themselves, but they can still get very excited by those who do. 
That’s why a little hyperbole never hurts. People want to believe that something is the 
biggest and the greatest and the most spectacular.

I call it truthful hyperbole. It’s an innocent form of exaggeration – and a very effective
form of promotion.
```

DJT's tenth rule of deal-making is "having fun":

```markdown
I don’t kid myself. Life is very fragile, and success doesn’t change that. If anything,
success makes it more fragile. Anything can change, without warning, and that’s why I 
try not to take any of what’s happened too seriously. Money was never a big motivation 
for me, except as a way to keep score. The real excitement is playing the game. I don’t
spend a lot of time worrying about what I should have done differently, or what’s going
to happen next. If you ask me exactly what the deals I’m about to describe all add up to
in the end, I’m not sure I have a very good answer. Except that I’ve had a very good 
time making them.
```

DJT's late-career-stage plans apparently involve giving back by having skin in the game:

```markdown
What’s next? Fortunately, I don’t know the answer, because if I did, that would take all
the fun out of it. This much I do know: it won’t be more of the same.

I’ve spent the first twenty years of my working life building, accumulating, and
accomplishing things that many said could not be done. The biggest challenge I see over 
the next twenty years is to figure out some creative ways to give back some of what I’ve 
gotten.

I don’t just mean money, although that’s part of it. It’s easy to be generous when you’ve 
got a lot, and anyone who does, should be. But what I admire most are people who put
themselves directly on the line. I’ve never been terribly interested in why people give, 
because their motivation is rarely what it seems to be, and it’s almost never pure altruism. 
To me, what matters is the doing, and giving time is far more valuable than just giving 
money.

In my life, there are two things I’ve found I’m very good at: overcoming obstacles and 
motivating good people to do their best work. One of the challenges ahead is how to use 
those skills as successfully in the service of others as I’ve done, up to now, on my own 
behalf.

Don’t get me wrong. I also plan to keep making deals, big deals, and right around the clock.
```

And this is the money quote that make me create this subsection in the first place -- DJT as autistic child with special interest in real estate development:

```markdown
So much for seventeen pages of business advice. The other three hundred forty-eight
pages are Trump gushing about the minutiae all of the interesting deals he’s been a 
part of.

“GUYS, YOU’RE NOT GOING TO BELIEVE THIS, THERE WAS THIS ONE SKYSCRAPER THAT WAS SUPPOSED 
TO HAVE A FLOOR TO AREA RATIO OF 6, BUT THEN I BEAT HILTON IN NEGOTIATING THE AIR RIGHTS 
FROM THE COMPANY NEXT DOOR, AND ACTIVATED AN OPTION TO BUY A PROPERTY ON THE OTHER SIDE 
OF IT, AND ALL OF THAT LANDED ME A PARTNERSHIP WITH ONE OF THE BIG BANKS, AND THEN THE 
PLANNING BOARD TOTALLY CHANGED THE FLOOR AREA RATIO! CAN YOU BELIEVE IT, GUYS??!”

Overall the effect was that of an infodump from an autistic child with a special interest
in real estate development, which was both oddly endearing and not-so-oddly very boring.
```

combined with this -- an answer to "what do real estate developers do?" The answer being "coordination, often via blatant lies":

```markdown
I started the book with the question: what exactly do real estate developers do? They
don’t design buildings; they hire an architect for that part. They don’t construct the 
buildings; they hire a construction company for that part. They don’t manage the buildings; 
they hire a management company for that part. They’re not even the capitalist who funds the 
whole thing; they get a loan from a bank for that. So what do they do? Why don’t you or I 
take out a $100 million loan from a bank, hire a company to build a $100 million skyscraper,
and then rent it out for somewhat more than $100 million and become rich?

As best I can tell, the developer’s job is coordination. This often means *blatant lies*.
The usual process goes like this: the bank would be happy to lend you the money as long as
you have guaranteed renters. The renters would be happy to sign up as long as you show them
a design. The architect would be happy to design the building as long as you tell them what
the government’s allowing. The government would be happy to give you your permit as long as 
you have a construction company lined up. And the construction company would be happy to 
sign on with you as long as you have the money from the bank in your pocket. Or some kind
of complicated multi-step catch-22 like that. The solution – or at least Trump’s solution – 
is to tell everybody that all the other players have agreed and the deal is completely done 
except for their signature. The trick is to lie to the right people in the right order, so 
that by the time somebody checks to see whether they’ve been conned, you actually do have 
the signatures you told them that you had. The whole thing sounds very stressful.

The developer’s other job is dealing with regulations. The way Trump tells it, there are so 
many regulations on development in New York City in particular and America in general that
erecting anything larger than a folding chair requires the full resources of a multibillion
dollar company and half the law firms in Manhattan. Once the government grants approval it’s 
likely to add on new conditions when you’re halfway done building the skyscraper, insist on
bizarre provisions that gain it nothing but completely ruin your chance of making a profit,
or just stonewall you for the heck of it if you didn’t donate to the right people’s campaigns
last year. Reading about the system makes me both grateful and astonished that any structures 
have ever been erected in the United States at all, and somewhat worried that if anything
ever happens to Donald Trump and a few of his close friends, the country will lose the 
ability to legally construct artificial shelter and we will all have to go back to living in 
caves.
```

The first part reminds me of this oft-forwarded joke (via Sharon Xu):

```markdown
I told my son, “You will marry the girl I choose.”
He said, “NO!”
I told him, “She is Bill Gates’ daughter.”
He said, “OK.”
I called Bill Gates and said, “I want your daughter to marry my son.”
Bill Gates said, “NO.”
I told Bill Gates, My son is the CEO of World Bank.”
Bill Gates said, “OK.”
I called the President of World Bank and asked him to make my son the CEO.
He said, “NO.”
I told him, “My son is Bill Gates’ son-in-law.”
He said, “OK.”
This is exactly how politics works...
```

The ice-rink success story:

```markdown
Trump’s greatest pride is his ability to construct things on time and under budget. He 
gives the story of an ice rink that New York City was trying to renovate in Central Park.
After six years and $13 million, the city had completely failed to renovate it and just 
made things worse. Trump offered as a charitable gesture to do it himself, and the mayor,
who was a political enemy, refused. The press hounded the mayor, Trump eventually was 
allowed to try, and he finished it in four months for only $2.5 million. He boasted that 
he finished fixing the rink in less time than it took the city to complete their study on
why their rink-fixing project had failed.
```

Analogizing DJT to sportsplayers where you can't (aren't interested in?) changing the rules, you just do your best within them:

```markdown
He had a couple more stories like this – but throughout all of it, there was a feeling of 
something missing. Here is a guy whose job is cutting through bureaucracy, and who is 
apparently quite good at it. Yet throughout the book – and for that matter, throughout his
campaign for the nomination of a party that makes cutting bureaucracy a big part of their 
platform – he doesn’t devote a lot of energy to expressing discontent with the system. 
There is no libertarian streak to Trump – in the process of successfully navigating all of
these terrible rules, he rarely takes a step back and wonders about a better world where 
these rules don’t exist. Despite having way more ability to change the system than most 
people, he seems to regard it as a given, not worth debating. I think back to his
description of how it’s all just a big game to him. Most star basketball players are too 
busy shooting hoops to imagine whether the game might be more interesting if a three-
pointer was worth five points, or whatever. Trump seems to have the same attitude – the 
rules are there; his job is to make the best deal he can within those rules.

Maybe I’m imagining things, but I feel like this explains a lot about his presidential 
campaign. People ask him something like “How would you fix Medicare?”, and he gives some
vapid answer like “There are tremendous problems with Medicare, but I’m going to hire the
best people. I know all of the best doctors and health care executives, and we’re going 
to cut some amazing deals and have the best Medicare in the world.” And yeah, he did say
in his business tips that you should change the frame to avoid being negative to reporters.
But this isn’t a negative or a gotcha question. At some point you’d expect Trump to do his
homework and get some kind of Medicare plan or other. Instead he just goes off on the same
few tangents. This thing about hiring the best people, for example, seems almost like an 
obsession in the book. But it works for him. When somebody sues him (which seems like an 
hourly occurrence in real estate development no matter how careful you are) his response is
to find the best lawyer, hire them, and throw them at the problem. When he needs a hotel 
managed, he hires the best hotel managers and tells them to knock themselves out. Even his
much-mocked tendency to talk about all the people he knows comes from this being a big part
of his real estate strategy – one of the reasons he can outcompete other tycoons is because
he knows people on the planning board, knows people in the banks, knows people in all the 
companies he works with. It’s a huge advantage for him.

These strategies have always worked for him before, and floating off into some intellectual
ideal-system-design effort has never worked for him before. So when he says that he’s going
to solve Medicare by hiring great managers and knowing all the right people, I don’t think 
this is some vapid way of avoiding the question. I think it’s the honest output of a mind 
that works very differently from mine. I’ve been designing ideal systems of government for 
the heck of it ever since I was old enough to realize what a government was. Trump is at 
serious risk of actually taking over a government, and such design still doesn’t appeal to
him. The best he can do is say that other people are bad at governing, but he’s going to be
good at governing, on account of his deal-making skill. I think he honestly believes this. 
It makes perfect sense in real estate, where some people are good businesspeople, others are
bad businesspeople, and the goal is to game the system rather than change it. But in politics,
it’s easy to interpret as authoritarianism – “Forget about policy issues, I’m just going to
steamroll through this whole thing by being personally strong and talented.”
```

"The world is taken as a given; it contains deals" -- DJT a la narrow-but-powerful optimization process:

```markdown
I said it before, but it bears repeating – this book has a really good ghostwriter. Yeah, it
comes across as narcissistic; there’s probably no way to avoid that in a Trump autobiography.
But Donald Trump’s interest in Donald Trump pales beside his blazing hot interest in the sheer
awesomeness of hotel property deals. And part of me wants to say that people with obsessive 
interests in bizarre things are My Kind Of People.

But there’s still something alien about Trump here, even moreso than with the populist 
demagogue of the campaign trail. Trump the demagogue is attacked as anti-intellectual. I get
anti-intellectualism because – like all isms – it’s an intellectual idea, and I tend to think
in those terms. But Trump of the book is more a-intellectual, in the same way some people are
amoral or asexual. The world is taken as a given. It contains deals. Some people make the 
deals well, and they are winners. Other people make the deals poorly, and they are losers.
Trump does not need more than this. There will be no civilization of philosopher-Trumps asking
where the first deal came from, or whether a deal is a deal only by virtue of its participation
in some primordial deal beyond material existence. Trump’s world is so narrow it’s hard to fit
your head inside it, so narrow that on contact with any wider world it seems strange and 
attenuated, a broken record of deals and connections and hirings expanding to fill the space 
available.
```

<a name="#Jeremy-bentham"></a>
### Jeremy Bentham
([overview](#overview))

I found [this FB post by Rikk Hill](https://www.facebook.com/rikkhill/posts/10155063688827423) entirely at random, made the *very* lucky step of saving it all in a Word doc somewhere within the bowels of my laptop, and then forgot nearly everything about it except the strong sense that Jeremy Bentham was important to utilitarianism and weird EA because of his thinking style. I forgot *so much* about it that I couldn't find it back after concerted searching, all my search terms being wrong; in fact, it was only today that I realized I even had the Word doc, from which I could later retrieve the original source. That was a deeply frustrating experience. Not all of these experiences have a similarly happy ending; indeed this was an exception. This class of experiences is in fact one of *the* main drivers behind the existence of this entire sprawling document. 

But I digress. Anyway:

It was really, deeply important that Jeremy Bentham's thinking was weird, as a side effect of being utterly, undeviatingly principled; in that sense he was the first weird EAist, decades and centuries ahead of his time. (For more on weird EA see [here](#weird-EA)). Also, rereading Rikk's post made me think of [Scott on Robin Hanson](#robin-hanson). Robin may well be the Bentham of our time. 

Last point before we dive in: Bentham, like Hanson, and like myself (except for the part where I don't have any intellectual output), is an inveterate [sequence thinker](https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/); most people are cluster thinkers, and there is a lot of value in that as well (chiefly sandboxing weird conclusions from dominating via calculations).

Here's Rikk's post in full.

```markdown
I’d like to talk a little bit about moral philosopher Jeremy Bentham, and why he has a 
weird level of celebrity status among people who think like I do.

Jeremy Bentham lived in the late 18th and early 19th Centuries, in what we think of as the 
mid-late Georgian era. That’s important. This is the period in which Jane Austen wrote all 
her novels. The global policy issue of the era was motivating the abolition of the 
international slave trade. 3% of the population of England and Wales had the right to vote. 
Over this period, the Industrial and Agricultural revolutions in Britain mean that instead 
of 45% of the labour force working in agriculture, a mere 22% of the labour force was
required for feeding the nation. At the time, using just under a quarter of your working
population to produce enough food was seen as nothing short of a miracle. Today, this number
is less than 1%.

He lived in a very different time, is what I’m saying. I can’t stress this enough. It’s 
desperately important to remember this when thinking about Jeremy Bentham, because his 
intellectual output is principally characterised by being 

(a) utterly batshit, and 
(b) shockingly, eerily, uncannily ahead of its time.

Let’s deal with the batshit stuff first. Jeremy Bentham was a wildly eccentric dude, perhaps
best demonstrated by the fact that I used to walk past his corpse every day. It sits in a 
glass case in University College London. He instructed that after his death, his body should
be dissected for medical science, and afterwards, his taxidermied corpse should be put on 
display. He called this an “auto-icon”; why build a statue to someone when you can just 
exhibit their body? If you’re around Fitzrovia on a weekday, you can pop in and see it.

He thought the social mores and taboos of his time were arbitrary or unhelpful or outright 
wrong, and the social mores and taboos around death and the dead were just one example of this.
In later life, he became fixated on the role of bodies after death, and suggested decorating
public spaces with mummified corpses instead of trees. He also shared his bed with the pig he
kept as a house pet, pioneered a form of jogging (which he called “ante-prandial 
circumgyration”), and maintained largely-unsolicited correspondence with different heads of 
state from all over the world on the subjects of legal and constitutional reform.

Some of this is to be expected. If you reject the norms of your time and try to re-derive the 
social order from first principles, you might end up spooning a pig and writing to the President
of the United States about codifying the common law. What you might *also* end up doing is
independently inventing much of 21st Century Western liberalism, which is what Bentham also did.

There is a sense in which Jeremy Bentham literally invented a lot of the concepts we take for
granted as the founder of utilitarianism and a prolific Enlightenment thinker, but there is 
another sense in which, almost as a side-effect, he came to a variety of conclusions about the
social order which wouldn’t gain widespread traction until decades or even centuries after his
death. Universal suffrage, sexual equality, decriminalisation of homosexuality, abolition of 
the death penalty, animal welfare, no-fault divorce… the list of stuff which he essentially 
inferred from first principles yet ran strongly counter to the prevailing cultural norms is 
striking.

He was notoriously bad at getting his work published, and almost all of what he wrote had to be
collated and published posthumously by friends and proteges. A lot of editorial work went into 
making these works acceptable to the public, to the point where his defence of homosexuality 
didn’t come to light until 1931, over a century after he wrote it.

Many of the ideas he advocated have been part of the society we all grew up in for so long that
we don’t even recognise them as active choices of how our society is organised, but others are 
weirdly specific contemporary progressive ideas. For example, he very specifically drew attention
to sexual promiscuity of women as being judged disproportionately harshly compared to that of men.
He claimed there were other norms, such as sexual fidelity, that undercut any harms that might
result from sexual promiscuity, and shaming women on the basis of their sexual appetite just 
harmed everybody. This is a point that contemporary feminists are still having to make two 
centuries later. With a few small tweaks, a lot of what he wrote on sexuality could have come 
straight out of a contemporary discussion on sex-positivity. A reminder: he was writing this at 
about the same time Jane Austen was writing Mansfield Park.

I am going somewhere with this. I’m not just yammering on about how cool I think Jeremy Bentham 
is. So here we go. Here we get to the point of this little rant.

Treating everything you know about the world as suspect and trying to reason about it from as 
small a set of assumptions as possible is, to my mind, one of the fundamental tools of analytical
thought. Not everyone agrees with me on this. There is a particularly annoying strand of post-
cultural-turn thought which rears its head whenever I try this in public. “What’s that?” it’ll
say, “you’re trying to infer objectively-grounded facts about the universe? Well you *can’t*, you
naive silly sausage, because whether you like it or not, you’re in a *culture*, and that culture 
permeates your entire conception of reality, and you can’t ever really know anything, so there!” 
Then it sits there, like a cat that’s inordinately smug about what it’s just dropped in the litter
box.

I’m not *entirely* unsympathetic to this idea. We are a product of the culture we’re raised in.
It would be silly to think that if I grew up 12th Century Saxony or early Imperial China that I
would have the same moral and political sensibilities that I do now. I would probably believe 
what most other people in those times and places believed. Given that, maybe it is unreasonable
to think that I can somehow discard the biases of my own culture.

But then I look at Jeremy Bentham, who, at a time when the morality of chattel-slavery was still
a hotly-debated topic, was saying that It’s Okay to Be Gay and we shouldn’t slut-shame. Throw in
a Belle and Sebastian album and an animated gif of a puppy fighting it’s own reflection, and he 
could be on Tumblr.

Jeremy Bentham’s method is my method. My justification for progressive liberalism comes from
starting at the same premises he started at and playing them forward. It’s phenomenally easier for
me to do that than it was for Jeremy Bentham, because we’ve had 200 years of progress and I’m 
surrounded by other people who have the same set of object-level beliefs as I do. Those people 
are happy to support this method when it’s advocating LGBT+ rights and sex-positivity, but if it
delivers anything from left-field, that’s when the knives come out.

Here is a radical proposition: Jeremy Bentham wasn’t just ahead of his time — he was ahead of *our*
time. This was definitely true as recently as 1967 when homosexuality was decriminalised in the UK.
Maybe in another hundred years we’ll all be taking our pigs for an ante-prandial circumgyration 
along a row of mummified corpses. Taking a less facetious tack, over half of his writings have 
never been published. Who knows what he wrote which we would find bizarre today but which our great-
grandchildren wouldn’t bat an eyelid at.

More generally, maybe you can’t have the visionary foresight without the eccentricity. Even among
progressive people, who pay a lot of lip-service to celebrating diversity, there is a surprising 
amount of hostility to weird nerds re-deriving the social order from first principles. When we’re
judging people for doing this, maybe we should remember Jeremy Bentham. Perhaps this method has 
more value than meets the eye.
```

<a name="#Srinivasa-Ramanujan"></a>
### Srinivasa Ramanujan
([overview](#overview))

From JE Littlewood's review of “The Collected Papers of Srinivasa Ramanujan”:

```markdown
…the most important of [Ramanujan’s methods] were completely original. His intuition worked in
analogies, sometimes, remote, and to an astonishing extent by empirical induction from particular
numerical cases. …his most important weapon seems to have been a highly elaborate technique of 
transformation by means of divergent series and integrals … He had no strict logical justification
for his operations. He was not interested in rigour, which for that matter is not of first-rate 
importance in analysis beyond the undergraduate stage, and can be supplied, given a real idea, by
any competent professional. The clear-cut idea of what is *meant* by a proof, nowadays so familiar
as to be taken for granted, he perhaps did not possess at all. If a significant piece of reasoning
occurred somewhere, and the total mixture of evidence and intuition gave him certainty, he looked 
no further. It is a minor indication of his quality that he can never have *missed* Cauchy’s
theorem. With it he could have arrived more rapidly and conveniently at certain of his results, 
but his own methods enabled him to survey the field with an equal comprehensiveness and as sure a 
grasp.
```

<a name="#Richard-Feynman"></a>
### Richard Feynman
([overview](#overview))

A lot of great quotes from Danny Hillis' retrospective of [his experiences with Dick](http://longnow.org/essays/richard-feynman-connection-machine/). I'll try not to quote the entire essay. 

How they met, and how Dick agreed to help Danny & Co. work on the million-processor Connection Machine:

```markdown
One day when I was having lunch with Richard Feynman, I mentioned to him that I was planning
to start a company to build a parallel computer with a million processors. His reaction was 
unequivocal, "That is positively the dopiest idea I ever heard." For Richard a crazy idea was
an opportunity to either prove it wrong or prove it right. Either way, he was interested. By 
the end of lunch he had agreed to spend the summer working at the company.

Richard's interest in computing went back to his days at Los Alamos, where he supervised the 
"computers," that is, the people who operated the mechanical calculators. There he was
instrumental in setting up some of the first plug-programmable tabulating machines for physical
simulation. His interest in the field was heightened in the late 1970's when his son, Carl,
began studying computers at MIT.

I got to know Richard through his son. I was a graduate student at the MIT Artificial Intelligence
Lab and Carl was one of the undergraduates helping me with my thesis project. I was trying to
design a computer fast enough to solve common sense reasoning problems. The machine, as we 
envisioned it, would contain a million tiny computers, all connected by a communications network.
We called it a "Connection Machine." Richard, always interested in his son's activities, followed
the project closely. He was skeptical about the idea, but whenever we met at a conference or I 
visited CalTech, we would stay up until the early hours of the morning discussing details of the
planned machine. The first time he ever seemed to believe that we were really going to try to build
it was the lunchtime meeting.
```

Funny anecdote:

```markdown
Richard arrived in Boston the day after the company was incorporated. We had been busy raising the 
money, finding a place to rent, issuing stock, etc. We set up in an old mansion just outside of the
city, and when Richard showed up we were still recovering from the shock of having the first few 
million dollars in the bank. No one had thought about anything technical for several months. We were
arguing about what the name of the company should be when Richard walked in, saluted, and said, 
"Richard Feynman reporting for duty. OK, boss, what's my assignment?" The assembled group of not-
quite-graduated MIT students was astounded.

After a hurried private discussion ("I don't know, you hired him..."), we informed Richard that his
assignment would be to advise on the application of parallel processing to scientific problems.

"That sounds like a bunch of baloney," he said. "Give me something real to do."

So we sent him out to buy some office supplies. While he was gone, we decided that the part of the
machine that we were most worried about was the router that delivered messages from one processor 
to another. We were not sure that our design was going to work. When Richard returned from buying 
pencils, we gave him the assignment of analyzing the router.
```

The router problem:

```markdown
The router of the Connection Machine was the part of the hardware that allowed the processors to
communicate. It was a complicated device; by comparison, the processors themselves were simple. 
Connecting a separate communication wire between each pair of processors was impractical since a 
million processors would require $10^{12}$ wires. Instead, we planned to connect the processors in
a 20-dimensional hypercube so that each processor would only need to talk to 20 others directly. 
Because many processors had to communicate simultaneously, many messages would contend for the same
wires. The router's job was to find a free path through this 20-dimensional traffic jam or, if it 
couldn't, to hold onto the message in a buffer until a path became free. Our question to Richard 
Feynman was whether we had allowed enough buffers for the router to operate efficiently.

During those first few months, Richard began studying the router circuit diagrams as if they were
objects of nature. He was willing to listen to explanations of how and why things worked, but 
fundamentally he preferred to figure out everything himself by simulating the action of each of 
the circuits with pencil and paper. ...

The technical side of the project was definitely stretching our capacities. We had decided to 
simplify things by starting with only 64,000 processors, but even then the amount of work to do
was overwhelming. We had to design our own silicon integrated circuits, with processors and a
router. We also had to invent packaging and cooling mechanisms, write compilers and assemblers, 
devise ways of testing processors simultaneously, and so on. Even simple problems like wiring the
boards together took on a whole new meaning when working with tens of thousands of processors. In
retrospect, if we had had any understanding of how complicated the project was going to be, we
never would have started.
```

Solving the router problem the physicist's way -- PDEs instead of discrete math:

```markdown
By the end of that summer of 1983, Richard had completed his analysis of the behavior of the 
router, and much to our surprise and amusement, he presented his answer in the form of a set of 
partial differential equations. To a physicist this may seem natural, but to a computer designer,
treating a set of boolean circuits as a continuous, differentiable system is a bit strange. 
Feynman's router equations were in terms of variables representing continuous quantities such as
"the average number of 1 bits in a message address." I was much more accustomed to seeing analysis
in terms of inductive proof and case analysis than taking the derivative of "the number of 1's" 
with respect to time. Our discrete analysis said we needed seven buffers per chip; Feynman's
equations suggested that we only needed five. We decided to play it safe and ignore Feynman.

The decision to ignore Feynman's analysis was made in September, but by next spring we were up
against a wall. The chips that we had designed were slightly too big to manufacture and the only way 
to solve the problem was to cut the number of buffers per chip back to five. Since Feynman's 
equations claimed we could do this safely, his unconventional methods of analysis started looking
better and better to us. We decided to go ahead and make the chips with the smaller number of buffers.

Fortunately, he was right. When we put together the chips the machine worked. The first program
run on the machine in April of 1985 was Conway's game of Life.
```

Dick's "Los Alamos" Great Reference Point, and "Let's Get Organized":

```markdown
I had never managed a large group before and I was clearly in over my head. Richard volunteered 
to help out. "We've got to get these guys organized," he told me. "Let me tell you how we did it
at Los Alamos."

Every great man that I have known has had a certain time and place in their life that they use 
as a reference point; a time when things worked as they were supposed to and great things were 
accomplished. For Richard, that time was at Los Alamos during the Manhattan Project. Whenever 
things got "cockeyed," Richard would look back and try to understand how now was different than
then. Using this approach, Richard decided we should pick an expert in each area of importance
in the machine, such as software or packaging or electronics, to become the "group leader" in 
this area, analogous to the group leaders at Los Alamos.

Part Two of Feynman's "Let's Get Organized" campaign was that we should begin a regular seminar 
series of invited speakers who might have interesting things to do with our machine. Richard's
idea was that we should concentrate on people with new applications, because they would be less
conservative about what kind of computer they would use. For our first seminar he invited John 
Hopfield, a friend of his from CalTech, to give us a talk on his scheme for building neural 
networks. In 1983, studying neural networks was about as fashionable as studying ESP, so some 
people considered John Hopfield a little bit crazy. Richard was certain he would fit right in at
Thinking Machines Corporation.

What Hopfield had invented was a way of constructing an [associative memory], a device for 
remembering patterns. To use an associative memory, one trains it on a series of patterns, such 
as pictures of the letters of the alphabet. Later, when the memory is shown a new pattern it is 
able to recall a similar pattern that it has seen in the past. A new picture of the letter "A" 
will "remind" the memory of another "A" that it has seen previously. Hopfield had figured out how
such a memory could be built from devices that were similar to biological neurons.

Not only did Hopfield's method seem to work, but it seemed to work well on the Connection Machine.
Feynman figured out the details of how to use one processor to simulate each of Hopfield's neurons,
with the strength of the connections represented as numbers in the processors' memory. Because of 
the parallel nature of Hopfield's algorithm, all of the processors could be used concurrently with
100% efficiency, so the Connection Machine would be hundreds of times faster than any conventional 
computer.
```

Dick's love for detail:

```markdown
Concentrating on the algorithm for a basic arithmetic operation was typical of Richard's approach.
He loved the details. In studying the router, he paid attention to the action of each individual
gate and in writing a program he insisted on understanding the implementation of every instruction.
He distrusted abstractions that could not be directly related to the facts. When several years 
later I wrote a general interest article on the Connection Machine for [Scientific American], he
was disappointed that it left out too many details. He asked, "How is anyone supposed to know that
this isn't just a bunch of crap?"

Feynman's insistence on looking at the details helped us discover the potential of the machine for
numerical computing and physical simulation. We had convinced ourselves at the time that the 
Connection Machine would not be efficient at "number-crunching," because the first prototype had no
special hardware for vectors or floating point arithmetic. Both of these were "known" to be
requirements for number-crunching. Feynman decided to test this assumption on a problem that he was
familiar with in detail: quantum chromodynamics.

Quantum chromodynamics is a theory of the internal workings of atomic particles such as protons.
Using this theory it is possible, in principle, to compute the values of measurable physical 
quantities, such as a proton's mass. In practice, such a computation requires so much arithmetic
that it could keep the fastest computers in the world busy for years. One way to do this calculation
is to use a discrete four-dimensional lattice to model a section of space-time. Finding the solution
involves adding up the contributions of all of the possible configurations of certain matrices on 
the links of the lattice, or at least some large representative sample. (This is essentially a 
Feynman path integral.) The thing that makes this so difficult is that calculating the contribution
of even a single configuration involves multiplying the matrices around every little loop in the 
lattice, and the number of loops grows as the fourth power of the lattice size. Since all of these
multiplications can take place concurrently, there is plenty of opportunity to keep all 64,000 
processors busy.

To find out how well this would work in practice, Feynman had to write a computer program for QCD.
Since the only computer language Richard was really familiar with was Basic, he made up a parallel
version of Basic in which he wrote the program and then simulated it by hand to estimate how fast 
it would run on the Connection Machine.

He was excited by the results. "Hey Danny, you're not going to believe this, but that machine of
yours can actually do something [useful]!" According to Feynman's calculations, the Connection 
Machine, even without any special hardware for floating point arithmetic, would outperform a machine
that CalTech was building for doing QCD calculations. From that point on, Richard pushed us more and
more toward looking at numerical applications of the machine.
```

Dick's attitude towards explanations:

```markdown
In the meantime, we were having a lot of trouble explaining to people what we were doing with
cellular automata. Eyes tended to glaze over when we started talking about state transition diagrams
and finite state machines. Finally Feynman told us to explain it like this,

"We have noticed in nature that the behavior of a fluid depends very little on the nature of the
individual particles in that fluid. For example, the flow of sand is very similar to the flow of
water or the flow of a pile of ball bearings. We have therefore taken advantage of this fact to 
invent a type of imaginary particle that is especially simple for us to simulate. This particle is
a perfect ball bearing that can move at a single speed in one of six directions. The flow of these
particles on a large enough scale is very similar to the flow of natural fluids."

This was a typical Richard Feynman explanation. On the one hand, it infuriated the experts who had
worked on the problem because it neglected to even mention all of the clever problems that they had
solved. On the other hand, it delighted the listeners since they could walk away from it with a real
understanding of the phenomenon and how it was connected to physical reality.

We tried to take advantage of Richard's talent for clarity by getting him to critique the technical
presentations that we made in our product introductions. Before the commercial announcement of the
Connection Machine CM-1 and all of our future products, Richard would give a sentence-by-sentence 
critique of the planned presentation. "Don't say `reflected acoustic wave.' Say [echo]." Or, "Forget
all that `local minima' stuff. Just say there's a bubble caught in the crystal and you have to shake
it out." Nothing made him angrier than making something simple sound complicated.
```

Dick hated being asked for advice, and would often say "not my department":

```markdown
Getting Richard to give advice like that was sometimes tricky. He pretended not to like working on 
any problem that was outside his claimed area of expertise. Often, at Thinking Machines when he was
asked for advice he would gruffly refuse with "That's not my department." I could never figure out 
just what his department was, but it did not matter anyway, since he spent most of his time working
on those "not-my-department" problems. Sometimes he really would give up, but more often than not he
would come back a few days after his refusal and remark, "I've been thinking about what you asked the
other day and it seems to me..." This worked best if you were careful not to expect it.

I do not mean to imply that Richard was hesitant to do the "dirty work." In fact, he was always
volunteering for it. Many a visitor at Thinking Machines was shocked to see that we had a Nobel 
Laureate soldering circuit boards or painting walls. But what Richard hated, or at least pretended to
hate, was being asked to give advice. So why were people always asking him for it? Because even when 
Richard didn't understand, he always seemed to understand better than the rest of us. And whatever he
understood, he could make others understand as well. Richard made people feel like a child does, when
a grown-up first treats him as an adult. He was never afraid of telling the truth, and however foolish 
your question was, he never made you feel like a fool.
```

I do this too! "What's the simplest example", etc, albeit at a lot lower level:

```markdown
As it turned out, building a big computer is a good excuse to talk to people who are working on some of
the most exciting problems in science. We started working with physicists, astronomers, geologists, 
biologists, chemists --- everyone of them trying to solve some problem that it had never been possible
to solve before. Figuring out how to do these calculations on a parallel machine requires understanding
of the details of the application, which was exactly the kind of thing that Richard loved to do.

For Richard, figuring out these problems was a kind of a game. He always started by asking very basic
questions like, "What is the simplest example?" or "How can you tell if the answer is right?" He asked
questions until he reduced the problem to some essential puzzle that he thought he would be able to solve.
Then he would set to work, scribbling on a pad of paper and staring at the results. While he was in the
middle of this kind of puzzle solving he was impossible to interrupt. "Don't bug me. I'm busy," he would
say without even looking up. Eventually he would either decide the problem was too hard (in which case he
lost interest), or he would find a solution (in which case he spent the next day or two explaining it to
anyone who listened). In this way he worked on problems in database searches, geophysical modeling, 
protein folding, analyzing images, and reading insurance forms.
```

Retracing experts as amateurs, a quote I also really relate to -- this one on punctuated equilibrium:

```markdown
The last project that I worked on with Richard was in simulated evolution. I had written a program that
simulated the evolution of populations of sexually reproducing creatures over hundreds of thousands of 
generations. The results were surprising in that the fitness of the population made progress in sudden
leaps rather than by the expected steady improvement. The fossil record shows some evidence that real 
biological evolution might also exhibit such "punctuated equilibrium," so Richard and I decided to look
more closely at why it happened. He was feeling ill by that time, so I went out and spent the week with
him in Pasadena, and we worked out a model of evolution of finite populations based on the Fokker Planck
equations. When I got back to Boston I went to the library and discovered a book by Kimura on the 
subject, and much to my disappointment, all of our "discoveries" were covered in the first few pages.
When I called back and told Richard what I had found, he was elated. "Hey, we got it right!" he said.
"Not bad for amateurs."

In retrospect I realize that in almost everything that we worked on together, we were both amateurs. 
In digital physics, neural networks, even parallel computing, we never really knew what we were doing.
But the things that we studied were so new that no one else knew exactly what they were doing either. 
It was amateurs who made the progress.
```

<a name="#Robin-Hanson"></a>
### Robin Hanson
([overview](#overview))

Robin may well be [the Bentham of our time](#jeremy-bentham).

Scott Alexander, writing about Robin in his book review [Age of Em](https://slatestarcodex.com/2016/05/28/book-review-age-of-em/):

```markdown
There are some people who are destined to become adjectives. Pick up a David Hume book you’ve 
never read before and it’s easy to recognize the ideas and style as Humean. Everything Tolkien
wrote is Tolkienesque in a non-tautological sense. This isn’t meant to denounce either writer 
as boring. Quite the opposite. They produced a range of brilliant and diverse ideas. But there
was a hard-to-define and very consistent ethos at the foundation of both. Both authors were 
*very much like themselves.*

Robin Hanson is more like himself than anybody else I know. He’s obviously brilliant – a PhD in
economics, a masters in physics, work for DARPA, Lockheed, NASA, George Mason, and the Future of
Humanity Institute. But his greatest aptitude is in being really, really Hansonian.

Bryan Caplan describes it as well as anybody:

*When the typical economist tells me about his latest research, my standard reaction is ‘Eh, maybe.’
Then I forget about it. When Robin Hanson tells me about his latest research, my standard reaction 
is ‘No way! Impossible!’ Then I think about it for years.*

This is my experience too. I think I said my first “No way! Impossible!” sometime around 2008 after
reading his blog Overcoming Bias. Since then he’s influenced my thinking more than almost anyone
else I’ve ever read. When I heard he was writing a book, I was – well, I couldn’t even imagine a
book by Robin Hanson. When you read a thousand word blog post by Robin Hanson, you have to sit down 
and think about it and wait for it to digest and try not to lose too much sleep worrying about it.
A whole book would be *something.*
```

Redditor and English professor u/werttrew's [A Robin Hanson primer](https://www.reddit.com/r/slatestarcodex/comments/3sjtar/a_robin_hanson_primer/) has a nice summary of his main ideas. He has this flattering remark:

```markdown
I think Hanson is important not necessarily because he’s always right but because he provides some
startling and fresh ways of looking how the world works. I can probably safely attest that my 
thinking of the world is heavily influenced by his ideas. (I am an English professor, and I find
myself talking about Hanson’s ideas in my classes more often Weber or Foucault or Freud any other
thinker).
```

u/werttrew talks about Robin's "inductive style":

```markdown
Robin tends to work inductively—big idea speculated first, then thought-experiments as to how it 
would reveal itself. Like many economists, for better or worse Robin tends to work with interpretive
frameworks that cannot be falsified empirically. This means that economic models do not always have
the epistemic status of models in the physical sciences, which can be falsified through experiments.
```

<a name="#Scott-alexander"></a>
### Scott Alexander
([overview](#overview))

TheZvi commenting on Scott's post [Does age bring wisdom?](https://slatestarcodex.com/2017/11/07/does-age-bring-wisdom/):

```markdown
Happy birthday, indeed. Since no one else seems to have pointed it out yet, let me state
what seems to me to be completely obvious, which is that #2 is a load of bull.

You are special. You can and do change the world.

I’m not saying that to the proverbial/general you; this isn’t a “everyone is beautiful in
every single way” thing, or a “anyone can change the world” pep talk. This is a “You are
Scott Alexander and you have a voice and platform that actually matters” pep talk. People
really do actually listen to you. People who matter. Your opinions filter in to the general
discussion. I’m not talking only about rationalists or people in Berkeley, I’m talking
about people like Tyler Cohen and Ezra Klein. If I had to list “blogs/people everyone in
my office reads that aren’t explicitly about our work” you’d be on the list at #3 behind
Tyler Cohen and Matt Levine.

Maybe 10 years ago it would have been reasonable to say that you (probably) weren’t special
and couldn’t change the world. But why would you believe that less now rather than more? 
You’ve actually already changed it, and mostly for the better. So when you notice you’re 
doing things that quash truth, or worse encourage others to quash or disregard truth for 
other values (as you’ve done recently and explicitly), you’re not just fitting in. You’re
doing real, sizable harm. Snap out of it.

Act like you matter. Because you do.

In general, it seems like you’re making the mistake of thinking that because there are
hidden/complex reasons behind the things people do, and you’ve seen some of those reasons
but not others, and at least some of those reasons are good reasons, that this means that 
there’s a good reason for most (seemingly dumb) things people do and believe. No. A reason,
or even worse an explanation, does not mean a good reason!

Anyway, that’s what I have time for right now, except the reminder that yes Facebook is
ruining everything and seriously you have all the information and know full well that 
it’s true, and implying that it isn’t true by associating it with low-status in the form 
of an old-person stereotype really isn’t helping.
```

#2 is [this point](#wisdom) (under Scott's "does age bring wisdom" musings).

<a name="#Jeff-Bezos"></a>
### Jeff Bezos
([overview](#overview))

As we should, we'll start with the legendary Steve Yegge [giant brain post](https://plus.google.com/110981030061712822816/posts/AaygmbzVeRq) on Google+ (which sadly no longer exists):

```markdown
**Amazon War Story #1:  Jeff Bezos**

Over the years I watched people give presentations to Jeff Bezos and come back bruised: 
emotionally, intellectually, often career-ily.  If you came back with a nod or a signoff,
you were jumping for joy.  Presenting to Jeff is a gauntlet that tends to send people 
back to the cave to lick their wounds and stay out of the sunlight for a while.

I say “presentations” and you probably think PowerPoint, but no:  he outlawed PowerPoint
there many years ago.  It’s not allowed on the campus.  If you present to Jeff, you write
it as prose.

One day it came time for me to present to Jeff.  It felt like... I don’t know, maybe how
they swarm around you when you’re going to meet the President.  People giving you last-
minute advice, wishing you luck, ushering you past regiments of admins and security guards.
It’s like you’re in a movie.  A gladiator movie.

Fortunately I’d spent years watching Jeff in action before my turn came, and I had prepared
in an unusual way.  My presentation -- which, roughly speaking was about the core skills a
generalist engineer ought to know -- was a resounding success.  He loved it.  Afterwards 
everyone was patting me on the back and congratulating me like I’d just completed a game-
winning hail-mary pass or something.  One VP told me privately:  “Presentations with Jeff
never go that well.”

But here’s the thing:  I had already suspected Jeff was going to like my presentation. 
You see, I had noticed two things about him, watching him over the years, that others had
either not caught on to, or else they had not figured out how to make the knowledge actionable.

Here is how I prepared.  Amazon people, take note.  This will help you.  I am dead serious.

To prepare a presentation for Jeff, first make damn sure you know everything there is to know
about the subject. Then write a prose narrative explaining the problem and solution(s).   
Write it exactly the way you would write it for a leading professor or industry expert on the
subject.

That is:  assume he already knows *everything* about it.  Assume he knows more than you do
about it.  Even if you have groundbreakingly original ideas in your material, just pretend it’s
old hat for him.  Write your prose in the succinct, direct, no-explanations way that you would 
write for a world-leading expert on the material.

You’re almost done.  The last step before you’re ready to present to him is this:  
*Delete every third paragraph.*

Now you’re ready to present!

Back in the mid-1800s there was this famous-ish composer/pianist named Franz Liszt.  He is
widely thought to have been the greatest sight-reader who ever lived.  He could sight-read
*anything* you gave him, including crazy stuff not even written for piano, like opera scores.
He was so staggeringly good at sight-reading that his brain was only fully engaged on the first
run-through.  After that he’d get bored and start embellishing with his own additions.

Bezos is so goddamned smart that you have to turn it into a game for him or he’ll be bored and
annoyed with you.  That was my first realization about him.  Who knows how smart he was before 
he became a billionaire -- let’s just assume it was “really frigging smart”, since he did build 
Amazon from scratch.  But for years he’s had armies of people taking care of *everything* for
him.  He doesn’t have to do anything at all except dress himself in the morning and read 
presentations all day long.  So he’s really, REALLY good at reading presentations.  He’s like the
Franz Liszt of sight-reading presentations.

So you have to start tearing out whole paragraphs, or even pages, to make it interesting for him.
He will fill in the gaps himself without missing a beat.  And his brain will have less time to
get annoyed with the slow pace of your brain.

I mean, imagine what it would be like to start off as an incredibly smart person, arguably a first-
class genius, and then somehow wind up in a situation where you have a general’s view of the 
industry battlefield for ten years.  Not only do you have more time than anyone else, and access
to more information than anyone else, you also have this long-term eagle-eye perspective that only
a handful of people in the world enjoy.

In some sense you wouldn’t even be human anymore.  People like Jeff are better regarded as hyper-
intelligent aliens with a tangential interest in human affairs.

But how do you prepare a presentation for a giant-brained alien?  Well, here’s my second realization:
He *will* outsmart you.  Knowing everything about your subject is only a first-line defense for you.
It’s like armor that he’ll eat through in the first few minutes.  He is going to have at least one 
deep insight about the subject, right there on the spot, and it’s going to make you look like a 
complete buffoon.

Trust me folks, I saw this happen time and again, for *years*.  Jeff Bezos has all these incredibly
intelligent, experienced domain experts surrounding him at huge meetings, and on a daily basis he 
thinks of shit that they never saw coming.  It’s a guaranteed facepalm fest.

So I knew he was going to think of something that I hadn’t.  I didn’t know what it might be, because
I’d spent weeks trying to think of everything.  I had reviewed the material with *dozens* of people.
But it didn’t matter.  I knew he was going to blindside me, because that’s what happens when you 
present to Jeff.

If you assume it’s coming, then it’s not going to catch you quite as off-guard.

And of course it happened.  I forgot Data Mining.  Wasn’t in the list.  He asked me point-blank, 
very nicely:  “Why aren’t Data Mining and Machine Learning in this list?”  And I laughed right in 
his face, which sent a shock wave through the stone-faced jury of VPs who had been listening in 
silence, waiting for a cue from Jeff as to whether he was going to be happy or I was headed for the
salt mines.

I laughed because I was *delighted*.  He’d caught me with my pants down around my ankles, right in 
front of everyone, despite all my excruciating weeks of preparation.  I had even deleted about a third
of the exposition just to keep his giant brain busy, but it didn’t matter.  He’d done it again, and I
looked like a total ass-clown in front of everyone.  It was frigging awesome.

So yeah, of course I couldn’t help laughing.  And I said:  “Yup, you got me.  I don’t know why it’s not
in there.  It should be.  I’m a dork.  I’ll add it.”  And he laughed, and we moved on, and everything 
was great.  Even the VPs started smiling.  It annoyed the hell out of me that they’d had to wait for a 
cue, but whatever.  Life was good.

You have to understand:  most people were scared around Bezos because they were waaaay too worried about
trying to keep their jobs.  People in high-level positions sometimes have a little too much personal 
self-esteem invested in their success.  Can you imagine how annoying it must be for him to be around 
timid people all day long?  But me -- well, I thought I was going to get fired every single day.  So 
fuck timid.  Might as well aim high and go out in a ball of flame.

That’s where the “Dread Pirate Bezos” line came from.  I worked hard and had fun, but every day I honestly
worried they might fire me in the morning.  Sure, it was a kind of paranoia.  But it was sort of healthy
in a way.  I kept my resume up to date, and I kept my skills up to date, and I never worried about saying
something stupid and ruining my career.  Because hey, they were most likely going to fire me in the morning.
```

And this one is from his legendary [platforms rant](https://gist.github.com/chitchcock/1281611):

```markdown
But there's one thing they (Amazon) do really really well that pretty much makes up for ALL 
of their political, philosophical and technical screw-ups.

Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's
retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous
and respected human-computer interaction expert in the entire world, and then ignored every
goddamn thing Larry said for three years until Larry finally -- wisely -- left the company.
Larry would do these big usability studies and demonstrate beyond any shred of doubt that 
nobody can understand that frigging website, but Bezos just couldn't let go of those pixels,
all those millions of semantics-packed pixels on the landing page. They were like millions of
his own precious children. So they're all still there, and Larry is not.

Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, 
yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm 
just trying to set the context here, to help you understand what happened. We're talking about
a guy who in all seriousness has said on many public occasions that people should be paying 
him to work at Amazon. He hands out little yellow stickies with his name on them, reminding 
people "who runs the company" when they disagree with him. The guy is a regular... well, Steve
Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me
wrong. He just makes ordinary control freaks look like stoned hippies.
```

Khe Hy is [far more reverent](https://radreads.co/weis-wisdom-jeff-bezos-on-leadership-and-business-strategy/) of Bezos. The following all come from Eugene Wei, whose blog [Remains of the Day](https://www.eugenewei.com/) is also pretty great. 

Compression as a form of business leverage:

```markdown
He understood that you had to do a good job of encapsulating company strategy in a succinct 
and memorable way because the company was just so large and sprawling that it was so easy for
the message to dissipate as it traveled from person to person. He did a good job of encoding
the message and using rituals to ensure that the message was always emphasized.
```

Encoding a message:

```markdown
Amazon had different themes for different years [like in Infinite Jest]. One year, we had studied
different companies in history and discovered that at $1 Bn in revenue companies go sideways or 
topple over due to scaling. Jeff was conscious of heading that off. We had a year called “Getting 
our House in Order” and the acronym was G.O.H.I.O. There was a contest to name the year and this 
was the most memorable name. You ask anyone at Amazon who was there at that year and they’ll say 
“Oh there was that year of G.O.H.I.O. with a concrete set of projects.” You go to most companies 
today and ask them, what’s our theme this year and they’ll have no idea. It seems silly to have a
contest to name a theme, but it really hit home with every group of the company. You’d have that 
repeated at every meeting.
```

The power of rituals:

```markdown
An example of a ritual was that Jeff was worried that in organizations, as they get larger, they 
get slower moving. People spot problems but don’t feel the same impulse to fix a problem as they 
did when it was a small company. We had an award called the “Just Do It” Award and he’d announce 
it at every all-hands meetings, he’d give out any number of awards. And you could nominate people 
for this. And the rule was that it was something that you did on your own, without asking for 
permission, it didn’t have to work. You just spotted a problem, you tried to tackle it in some
way. And you would get a single Nike shoe on a plaque. That was just reinforcing the idea that
“Hey, we’re a culture that constantly tries to spot and fix problems.”
```

Bezos separating the company from the business:

```markdown
He was very good at separating the company from the business. The company is Amazon, a bunch 
of capabilities, people, organizations and processes and then there’s Amazon.com the retail 
business and he knew that these were two separate things. You had to work at the organism of
the organization and the culture, processes. And sometimes you had to think about basic business
strategy at Amazon.com. We’re very attached to a model of the tech CEO as a “product visionary,”
the Steve Jobs model of rolling up your sleeves, nitpicking the product details; we probably
underestimate Steve Jobs the company-builder.
```

Eugene Wei, whose blog [Remains of the Day](https://www.eugenewei.com/) is just great in general, has a post called [Compress to impress](https://www.eugenewei.com/blog/2017/5/11/jpeg-your-ideas) (URL: "JPEG your ideas") that pithily captures and expounds on a bunch of stuff I've vaguely thought about and seen elsewhere. I covered the quotables [here](#Dealing-with-message-distortion); this is just about Jeff himself. 

Eugene:

```markdown
Jeff understood the power of rhetoric. Time spent coming up with the right words to package 
a key concept in a memorable way was time well spent. People fret about what others say about
them when they're not in the room, but Jeff was solving the issue of getting people to say 
what he'd say when he wasn't in the room.

It was so important to him that we even had company-wide contests to come up with the most 
memorable ways to name our annual themes. ...

I have a list of dozens of Jeff sayings filed away in memory, and I'm not alone. It's one 
reason he's one of the world's most effective CEO's. What's particularly impressive is that
Jeff is so brilliant that it would be easy for him to state his thinking in complex ways that
us mere mortals wouldn't grok. But true genius is stating the complex simply.

Ironically, Jeff employs the reverse of this for his own information inflows. It's well known
that he banned Powerpoint at Amazon because he was increasingly frustrated at the lossy nature
that medium. As Edward Tufte has long railed against, Powerpoint encourage people to reduce 
their thinking to a series of bullet points. Whenever someone would stand up in front of Jeff
to present, Jeff would have rifled through to the end of the presentation before they would've 
finished a handful of slides, and Jeff would just jump in and start asking questions about 
slide 35 when someone was still talking to slide 3.

As a hyper intelligent person, Jeff didn't want lossy compression or lazy thinking, he wanted
the raw feed in a structured form, and so we all shifted to writing our arguments out as essays
that he'd read silently in meetings. Written language is a lossy format, too, but it has the 
advantage of being less forgiving of broken logic flows than slide decks.

To summarize, Jeff's outbound feedback was carefully encoded and compressed for maximum fidelity
of transmission across hundreds of thousands of employees all over the world, but his inbound 
data feed was raw and minimally compressed. In structure, this pattern resembles what a great 
designer or photographer does. Find the most elegant and stable output from a complex universe 
of inputs.
```

This agrees with Steve Yegge's funny quotes above.

<a name="#Andrey-Kolmogorov"></a>
### Andrey Kolmogorov
([overview](#overview))

Andrey Kolmogorov was the (only slightly) poorer man's version of [Johnny von Neumann](#johnny-von-neumann). He was born in 1903, eight months earlier than von Neumann, but lived thirty years longer. Von Neumann was only nineteen when he published two major math papers, the second of which gave the modern definition of ordinal numbers, superseding Cantor’s definition; that same year Kolmogorov wrote a paper on operations on sets which was a major generalization of results obtained by Suslin, then gained international prominence for constructing a Fourier series that diverges almost everywhere, something experts considered wholly unexpected. 

Everything he touched seemed to turn to gold:

```markdown
Almost simultaneously [Kolmogorov] exhibited his interest in a number of other areas of classical 
analysis: in problems of differentiation and integration, in measures of sets etc. In every one of his
papers, dealing with such a variety of topics, he introduced an element of originality, a breadth of 
approach, and a depth of thought.
```

He published eight papers in 1925 alone, all written while he was still an undergrad at MSU, and by the time he received his doctorate in 1929 Kolmogorov already had 18 papers to his name.

The 1930s were the mathematical heydays of both von Neumann and Kolmogorov. The latter in particular published over sixty papers during that decade alone in at least *eleven* different fields:

```markdown
… probability theory, projective geometry, mathematical statistics, the theory of functions of a real 
variable, topology, mathematical logic, mathematical biology, philosophy and the history of mathematics.
```

He was only thirty when he published Foundations of the Theory of Probability, laying the modern axiomatic foundations of probability theory and establishing his reputation as the world's leading expert in this field, and in the process bringing respectability to a discipline suffering from lack of foundations.

Like that of von Neumann, Kolmogorov’s work covered every area of mathematics except number theory. A non-exhaustive list of areas he enriched by his fundamental research:

```markdown
The theory of trigonometric series, measure theory, set theory, the theory of integration, constructive
logic (intuitionism), topology, approximation theory, probability theory, the theory of random processes, 
information theory, mathematical statistics, dynamical systems, automata theory, theory of algorithms, 
mathematical linguistics, turbulence theory, celestial mechanics, differential equations, Hilbert's 6th 
and 13th problems, ballistics, and applications of mathematics to problems of biology, geology, and the
crystallization of metals.
```

Kolmogorov was clearly a giant of a mathematical scientist in the highest sense of the word, so much so that I wonder why he didn’t have von Neumann’s reputation for brains. My best guess is that he wasn’t a mental calculator like von Neumann. The easiest way to *viscerally* demonstrate “wow brains!” is to do the sort of party tricks von Neumann used to regale his audience with—out-calculating (early) computers, instantly summing infinite series, translating books by heart “with no diminution in speech rate”, memorizing entire pages from phone books on command and suchlike—which Kolmogorov probably didn’t do.

<a name="#charlie-fefferman"></a>
### Charlie Fefferman
([overview](#overview))

I've known about Charles Fefferman for a few years now, but never really bothered to check out his background. I vaguely thought of him as Terry Tao before Terry Tao: both work in analysis, both are extremely accomplished and productive (Tao probably more so, since he seems to explicitly optimize for productivity to a degree I find unusual even in top-tier mathematicians), both garnered a reputation very early on for consistently solving really tough problems (the kind more naturally associated with high IQ than artistic creativity, more Gauss than Grothendieck if that makes sense), both have an impressive string of awards to their name, and both growing up were prodigies – in the most rarefied sense of the word. 

You know Tao's story already. Here's a quick run-through from memory: scored 760 on the Math portion of the SAT (99th percentile score) at 8 on his first try (only one other 8-year-old ever scored more than 700, Lenhard Ng, and it was his fifth try), youngest IMO medalist ever at 10 (bronze), youngest IMO silver ever (year after), youngest IMO gold ever (year after), graduated at 16, Ph.D. from Princeton at 20 under the renowned analyst Elias Stein, same year published his first (four) papers, full professor at UCLA at 24, Fields Medal at 31. 

Fefferman's story, interestingly, parallels and predates Tao's by about 26 years or so. In fact he was slightly ahead even. A summary: started college at 12, published his first paper at 15 (!), graduated at 17, Ph.D. also from Princeton at 20 under Elias Stein (!), full professor at University of Chicago at 22 (youngest at major university in US history), Fields Medal at 29 (youngest ever, since surpassed).

Although I'm a fan of Tao, I honestly think that some of the effusive praise surrounding him is a bit overmuch -- see for instance [the University of St Andrew's bio on Terry](#terry-tao). But even *that* doesn't compare to how Charlie was lauded back in the day. When I dug through old news articles on thirty-year-old Fefferman, I found the kind of praise that puts the above to shame: "perhaps the greatest mathematical genius of this century" and its variants popped up in the first two articles I found! Also others, like "certainly every mathematician in this country knows Charlie Fefferman". 

The anecdotes on young Fefferman strike me as representative of someone who wasn't just very good at math, but very smart in general. When he was four, the local auto mechanic at Silver Springs told his mom that "anytime the little kid wants a job here he's hired", since *by the age of four* he was already "better than any mechanic [he'd] ever met" (!).Later he took an interest in dinosaurs, and got so obsessed with them that his impromptu "lectures" on fossil exhibits at the Smithsonian to his younger brother reputedly drew throngs of followers. Tall tales, all of them, each as fun to read as the next.

Incidentally, Fefferman wrote the [laudatio](http://www.icm2006.org/proceedings/Vol_I/7.pdf) summarizing Tao's work leading to the latter winning the Fields. I like to see it as the passing of a torch from a Fields prodigy to another. Winning the Fields is rare enough. Winning it before the age of 32 – this sounds somewhat arbitrary, but by that I mean two cycles early, meaning you had two more chances to win it when older people past 36 have none, so you'd have to be really deserving of the award to take a spot from one of them – is much rarer still. I only know three. Two of them are Fefferman and Tao.

There's a quote by Fefferman of Tao that goes: "He's wonderful. There are a few in a generation, and he's one of the few." So are you, Charlie, so are you.

<a name="#johnny-von-neumann"></a>
### Johnny von Neumann
([overview](#overview))

From Cosma Shalizi's [notebook on Johnny](http://bactra.org/notebooks/von-neumann.html):

```markdown
Johnny, as it seems everyone called him, was one of those people who are so bright it's hard to believe
they were human. 

(Maybe he wasn't. There's an old joke about the Fermi Paradox, a problem which occured to Enrico Fermi 
one day at Los Alamos: where are They? If there are intelligent aliens out there in the universe, why
aren't they here yet? A million years is nothing, as the universe reckons things, but, judging from our
own track-record, a species only that much older than us would have technology which would blow our minds,
pretty close to limits set by physical laws. Leo Szilard is supposed to have answered Fermi: "Maybe they
're already here, and you just call them Hungarians.")

About the only large current of the natural sciences in this century which von Neumann's work has *not* 
added to is molecular biology. Almost everything else of any signficance he touched: mathematical logic; 
pure math; quantum physics; computing (which, as we know it, is largely his invention), cybernetics and 
automata theory; the Bomb; turbulence; game theory (another invention) and so economics, evolutionary 
biology, and the theory of war and conflict; artificial life, cellular automata (a third invention), the
theory of self-reproduction (which, with molecular biology, finally killed off any last lingering hopes 
for vitalism) and artificial evolution. What many of us like to think of as new and profound changes in 
the way science works, brought about by computer modelling and simulation, were forseen and called for by
von Neumman in the '40s. If any one person can be said to be the intellectual ancestor of complexity and
all that travels alongside it, it was Johnny. His only real rival for the honor is Norbert Wiener, a better
man but a less overwhelming scientist.
```

All this, in thirty-odd years. Von Neumann died relatively young, aged fifty-three, from either bone or pancreatic cancer. I’ve often wondered what else he would have accomplished had he lived longer, how much more he might have done. 

(I think I know now the answer: Johnny von Neumann would have probably been a richer man’s version of Andrey Nikolaevich Kolmogorov, and I’m not so sure about the “richer” part. See [here](#Andrey-Kolmogorov).)

The ever-entertaining Steve Yegge on Johnny, from his essay [Math every day](https://sites.google.com/site/steveyegge2/math-every-day):

```markdown
Von Neumann was originally "just" a mathematician (and chemical engineer), but he made lasting and
often central contributions to fields other than pure mathematics. Yes, he invented the computer and
computer programming as we know it, which is a fine thing to have on your resume, but it's only a 
tiny part of his work. The list is so long that the book I read couldn't begin to mention it all, 
let alone discuss it. I'll list a handful of his accomplishments here, but they don't begin to paint
the full picture.

Johnny co-invented game theory, made critical contributions to the field of economics, and extended 
his game theory to formalize the theory of linear programming, which is now a staple optimization 
method for many disciplines, including some that we employ at Amazon (e.g. operations research).

He singlehandedly invented the theory of cellular automata -- you know, the one that madman Stephen 
Wolfram tries to take credit for in A New Kind of Science -- and was one of the pioneers of methods
for understanding the human brain, which spun off any number of sub-fields.

He also did pioneering work in the theory of building large, reliable systems comprised of unreliable
components.

He was one of the key contributors to the atomic bomb on the Manhattan Project. He's the one who 
suggested the implosion device for achieving reliable detonation (they had been trying to use a 
gun/projectile method), and he was instrumental in working out the hydrodynamic and thermodynamic 
problems in understanding the shock waves and blast behavior. Regardless of your feelings about nuclear
weapons, you have to appreciate his contributions in a critical wartime initiative.

He made many major contributions to the field of numerical approximation. Many real-world problems are 
non-tractable to mathematical analysis; e.g. they're only describable by systems of high-order partial 
differential equations. Johnny tackled these problems with gusto, inventing or extending a huge number
of different approaches for doing approximations or probabilistic analyses that yielded results where
rigorous analysis had failed.

This is the primary reason that he went to all the effort to invent the modern computer. He wasn't 
satisfied with simply designing and building it; he used it to solve a huge variety of real-life problems.

For instance -- and this is just one example -- he turned weather forecasting into a science. Before
Johnny decided to tackle the problem (and he knew nothing of meteorology; he had to learn it in order 
to solve it), it wasn't possible to get accurate 24-hour weather predictions, and the experts in the 
field had given up hope of ever making scientific weather predictions. You now get accurate weather 
forecasts because Johnny decided it was an interesting problem.
```

Yegge On "what computers really are":

```markdown
Another realization I had while reading the book (John von Neumann and the Origins of Modern Computing) 
is that just about every course I took in my CS degree was either invented by Johnny von Neumann, or 
it's building on his work in mostly unintelligent ways.

Where to start? Before von Neumann, the only electronic computing devices were calculators. He invented
the modern computer, effectively simulating a Universal Turing Machine because he felt a sequential 
device would be cheaper and faster to manufacture than a parallel one. I'd say at least 80% of what we
learned in our undergrad machine-architecture course was straight out of his first report on designing
a programmable computer. It really hasn't changed much.

He created a sequential-instruction device with a fast calculation unit but limited memory and slow 
data transfer (known as the infamous "von Neumann bottleneck", as if he's somehow responsible for
everyone else being too stupid in the past 60 years to come up with something better. In fact, Johnny was
well on his way to coming up with a working parallel computer based on neuron-like cellular automata; he 
probably would have had one in production by 1965 if he hadn't tragically died of cancer in 1957, at age 54.)

Von Neumann knew well the limitations of his sequential computer, but needed to solve real problems with
it, so he invented everything you'd need to do so: encoding machine instructions as numbers, fixed-point
arithmetic, conditional branching, iteration and program flow control, subroutines, debugging and error 
checking (both hardware and software), algorithms for converting binary to decimal and back, and mathematical
and logical systems for modelling problems so they could be solved (or approximated) on his computing machine.

He did much of the hardware and materials engineering himself, working closely with engineers to come up
with solutions for memory, secondary storage, input and output (including hooking up an oscilloscope to the
computer to visually check results that were too complex for punch cards). He secured the funding for building
the computer and computing lab from government and other sources, developed training programs to teach people
how to program computers, and worked with domain experts to find problems that were too hard for mathematical
analysis but tractable to numerical solutions on his computer.

Of course he knew what was computable on his machine, because he worked out new definitions of elegance and
efficiency, came up with the mathematical models for analyzing the complexity of algorithms being executed on
his device, and with his staff, ran hundreds of experiments to learn what worked well and what didn't.

John von Neumann invented our universe.

Then he died at the depressingly early age of 54, robbing the world of perhaps the greatest genius of the 
20th century. "Those who know" generally seem to rank Albert Einstein ahead of von Neumann, but Johnny
always gets a solid #2 vote. Frankly, though, I think Johnny had a far bigger impact on my life, and not 
just because I'm a programmer. What did Albert do, really? Dashed all our hopes of faster-than-light travel,
that's what he did. Whined a lot about not agreeing with quantum mechanics, that's what he did. To the best
of my knowledge, Einstein didn't even know EJB, which according to many Amazon folks makes him a retard.

When I say that von Neumann invented our universe, I'm not trying to be poetic or rhetorical. What I'm saying 
is that his *first* attempt at a computing machine, one that he didn't really like all that much and considered
mostly a prototype, is still the one sitting on my desk today. That means we're a bunch of frigging boneheads.
You, me, everyone.
```

More of the same from Yegge:

```markdown
Discrete math, data structures, algorithms: they've all been refined since he died, sure, but he started
it all. Virtually every discipline that we think of as "computer science" is like Euclidean geometry: 
useful, sure, but far from the only kind out there.

Operating systems, threads, and processes are just a pathetic attempt at fooling you into thinking you have
a parallel computer, right? Gosh, computers are so darn fast that you can have the processor zing around
like Feyman's hypothetical "only electron in the universe", and it almost looks as if we're smart. But we're
not. All the world at large truly understands is serial execution, which is precisely why we're so lost in
the whole distributed computing space. Everyone talks about agents and crawlers and web services and all
this other crap we don't understand; Johnny would have taken one look at it and invented tractable
mathematical solutions.

Compilers: now there's one discipline where Johnny clearly didn't have much influence. He was a big old-
school proponent of doing everything in machine code. He might have changed his mind if he'd lived another 
few decades; hard to say. But ordinary mortals realized they needed shortcuts: higher-level languages that
would be translated into Johnny's machine instructions.

So people came up with a bunch of crap-ass languages that still had the exact same abstractions as the
underlying machine: a global memory that you update by issuing statements or instructions, expressions
that can be computed by the arithmetic-logic unit, conditional branching and loops, subroutines.
Everything you need to be "Turing-complete", which is equivalent to von Neumann-complete.

The new languages simply added various shortcuts: named storage locations ("variables"), syntax for dealing
with memory addresses, "types" for telling the compiler that a variable comes from a particular set of valid
values and has a particular set of legal operations, "scopes" and "namespaces" for organizing your code a
little better, "objects" for anthropomorphizing your data so it can be happy or sad (mostly sad), and other
cruft.

Bu it's all crap. Why? Because it's all just sugaring for the capabilities of assembly language on von
Neumann serial machines, plus a smattering of support for calling into the operating system code so you
can pretend your program is performing truly parallel operations. And none of that parallel stuff works
very well, since we don't understand it, because we don't know math.

What about data structures, you ask? Surely that's one island of purity, something that exists outside of
the von Neumann universe? After all, we worked so hard to understand them. (Well, some of us did. Plenty 
of folks didn't even do that; they just call an API and it all works like magic.)

Sorry to disappoint you, but most of our data structures are fundamentally based on Johnny's sequential 
machine. Think of all those pointers: they're just memory addresses. The best you can do for a sorting 
algorithm, complexity-wise, assuming radix sort isn't possible, is n*logn. Or so you thought; there are
parallel algorithms that run in linear time. All your cherished data structures are simply the best that
clever people could do at organizing data on a Turing Machine. If someone created a set of data structures
whose pointers were URLs, that would be a step away from von Neumann.

What about SMP or NUMA? Surely adding multiple processors is a huge step towards parallel computing? Nope,
sorry. Well, not much of one. There's still a von Neumann bottleneck; the channel has just been made a bit 
shorter. We need it to be made infinitely wider by creating a truly parallel computer that doesn't consist
of CPUs all trying to update the same global store.

Face it: Computer Science was a misnomer. It should have been called Johnny's First Universe.
```

Yegge on Johnny "making intractable problems tractable":

```markdown
Mathematicians have traditionally avoided problems that they considered "intractable" to analysis. 
They've known about numerical methods for centuries; even Newton devised a few, as did the ancient Greeks. 
But brute-force methods have traditionally been met with some disdain, because the results of large-scale
human computations rarely justify the cost of the undertaking.

Clairaut's work in 1757, in which he and two other people labored for 5 months to trace the orbit of 
Halley's Comet to compute its perihelion, garnered widespread criticism after his team missed the true 
perihelion by thirty-one days. Larger-scale computing groups have met with similar results: you can perform
large, expensive computations, e.g. to create trigonometric tables, but humans make errors, and human 
computers generate unreliable output that becomes worse the harder they push to get it done faster.

Before von Neumann's programmable computer, mathematicians had few other options. They pushed pure analysis
to its limits, but some physical and natural phenomena can only be described with complex systems of 
equations that cannot be solved analytically. Many problems had to be solved informally.

Von Neumann changed all that; he embraced numerical methods and invented entire new branches of math for use
with the computers he was inventing -- the sequential computer for the most part, but also his planned 
parallel computers. And he was able to perform larger and larger computing tasks with small-ish teams. When
a task grew large enough, requiring more than, say, 20 people, it was considered intractable. Rather than 
trying to secure funding for more people, he and his colleagues went to work on creating intrinsically more 
powerful mathematical models to make the problem tractable.

And don't you think for a moment that his problems were "easier" than the ones Amazon faces today. Looking
at the speed at which he was pioneering the development of parallel computational models and research on
intelligent, self-reproducing, self-repairing systems in the last few years of his life, it's pretty clear
that he would have been successful. Johnny's approach was to make intractable tasks tractable.
```

This quote isn't that relevant, but it touched me. Steve Yegge in [Moore's law is crap](http://steve-yegge.blogspot.com/2006/03/moores-law-is-crap.html):

```markdown
You *do* realize that John von Neumann spent the last 10 years of his life singlehandedly developing a 
theory of computing based on cellular automata? The computer you're reading this blog rant on was his
frigging prototype! He was going to throw it out and make a better one! And then he died of cancer, just
like my brother Dave did, just like so many people with so much more to give and so much more life to live.
And we're not making headway on cancer, either, because our computers and languages are such miserable crap.
```

Wolfram on von Neumann, from [John von Neumann's 100th birthday](http://blog.stephenwolfram.com/2003/12/john-von-neumanns-100th-birthday/):

```markdown
Some scientists (such as myself) spend most of their lives pursuing their own grand programs, ultimately in a fairly isolated way. John von Neumann was instead someone who always liked to interact with the latest popular issues—and the people around them—and then contribute to them in his own characteristic way.

He worked hard, often on many projects at once, and always seemed to have fun. In retrospect, he chose most of his topics remarkably well. He studied each of them with a definite practical mathematical style. And partly by being the first person to try applying serious mathematical methods in various areas, he was able to make important and unique contributions.
```

More from Steve -- he's interesting to read, because it's by one of the few people on the planet whose sheer mental horsepower was within striking distance of Johnny.

```markdown
Today (December 28, 2003) would have been John von Neumann’s 100th birthday—if he had
not died at age 54 in 1957. I’ve been interested in von Neumann for many years—not least
because his work touched on some of my most favorite topics. He is mentioned in 12 
separate places in my book—second in number only to Alan Turing, who appears 19 times.

I always feel that one can appreciate people’s work better if one understands the people
themselves better. And from talking to many people who knew him, I think I’ve gradually 
built up a decent picture of John von Neumann as a man.

He would have been fun to meet. He knew a lot, was very quick, always impressed people,
and was lively, social and funny.

One video clip of him has survived. In 1955 he was on a television show called Youth Wants
to Know, which today seems painfully hokey. Surrounded by teenage kids, he is introduced
as a commissioner of the Atomic Energy Commission—which in those days was a big deal. He
is asked about an exhibit of equipment. He says very seriously that it’s mostly radiation
detectors. But then a twinkle comes into his eye, and he points to another item, and says
deadpan, “Except this, which is a carrying case.” And that’s the end of the only video 
record of John von Neumann that exists.

Some scientists (such as myself) spend most of their lives pursuing their own grand programs,
ultimately in a fairly isolated way. John von Neumann was instead someone who always liked 
to interact with the latest popular issues—and the people around them—and then contribute
to them in his own characteristic way.

He worked hard, often on many projects at once, and always seemed to have fun. In retrospect,
he chose most of his topics remarkably well. He studied each of them with a definite 
practical mathematical style. And partly by being the first person to try applying serious
mathematical methods in various areas, he was able to make important and unique contributions.

But I’ve been told that he was never completely happy with his achievements because he thought
he missed some great discoveries. And indeed he was close to a remarkable number of important
mathematics-related discoveries of the twentieth century: Godel’s theorem, Bell’s inequalities,
information theory, Turing machines, computer languages—as well as my own more recent favorite
core NKS discovery of complexity from simple rules.

But somehow he never quite made the conceptual shifts that were needed for any of these 
discoveries.

There were, I think, two basic reasons for this. First, he was so good at getting new results 
by the mathematical methods he knew that he was always going off to get more results, and never 
had a reason to pause and see whether some different conceptual framework should be considered. 
And second, he was not particularly one to buck the system: he liked the social milieu of 
science and always seemed to take both intellectual and other authority seriously. ...

By the 1930s von Neumann was publishing several papers a year, on a variety of popular topics
in mainstream mathematics, often in collaboration with contemporaries of significant later 
reputation (Wigner, Koopman, Jordan, Veblen, Birkhoff, Kuratowski, Halmos, Chandrasekhar, etc.).
Von Neumann’s work was unquestionably good and innovative, though very much in the flow of
development of the mathematics of its time.

Despite von Neumann’s early interest in logic and the foundations of math, he (like most of the
math community) moved away from this by the mid-1930s. In Cambridge and then in Princeton he
encountered the young Alan Turing—even offering him a job as an assistant in 1938. But he 
apparently paid little attention to Turing’s classic 1936 paper on Turing machines and the
concept of universal computation, writing in a recommendation letter on June 1, 1937 that
“[Turing] has done good work on … theory of almost periodic functions and theory of continuous
groups”.

As it did for many scientists, von Neumann’s work on the Manhattan Project appears to have 
broadened his horizons, and seems to have spurred his efforts to apply his mathematical prowess
to problems of all sorts—not just in traditional mathematics. His pure mathematical colleagues 
seem to have viewed such activities as a peculiar and somewhat suspect hobby, but one that could
generally be tolerated in view of his respectable mathematical credentials.

Among several of von Neumann’s interests outside of mainstream pure mathematics was his attempt 
to develop a mathematical theory of biology and life (see the NKS book, page 876). In the 
mid-1940s there had begun to be—particularly from wartime work on electronic control systems—
quite a bit of discussion about analogies between “natural and artificial automata”, and 
“cybernetics”. And von Neumann decided to apply his mathematical methods to this. I’ve been told
he was particularly impressed by the work of McCullough and Pitts on formal models of the analogy
between brains and electronics (see the NKS book, page 1099). (There were undoubtedly other 
influences too: John McCarthy told me that around 1948 he visited von Neumann, and told him about
applying information theory ideas to thinking about the brain as an automaton; von Neumann’s main
response at the time was just, “Write it up!”)

Von Neumann was in many ways a traditional mathematician, who (like Turing) believed he needed to
turn to partial differential equations in describing natural systems. I’ve been told that at Los 
Alamos von Neumann was very taken with electrically stimulated jellyfish, which he appears to have
viewed as doing some kind of continuous analog of the information processing of an electronic
circuit. In any case, by about 1947, he had conceived the idea of using partial differential 
equations to model a kind of factory that could reproduce itself, like a living organism.

Von Neumann always seems to have been very taken with children, and I am told that it was in playing
with an erector set owned by the son of his game-theory collaborator Oskar Morgenstern that von
Neumann realized that his self-reproducing factory could actually be built out of discrete robotic-
like parts. (There was already something of a tradition of building computers out of Meccano—and 
indeed for example some of Hartree’s early articles on analog computers appeared in Meccano Magazine.)

An electrical engineer named Julian Bigelow, who worked on von Neumann’s IAS computer project,
pointed out that 3D parts were not necessary, and that 2D would work just as well. (When I was 
at the Institute in the early 1980s Bigelow was still there, though unfortunately viewed as a 
slightly peculiar relic of von Neumann’s project.)

Stan Ulam told me that he had independently thought about making mathematical models of biology,
but in any case, around 1951 he appears to have suggested to von Neumann that one should be able
to use a simplified, essentially combinatorial model—based on something like the infinite matrices
that Ulam had encountered in the so-called Scottish Book of math problems (named after a café in 
Poland) to which he had contributed.

The result of all this was a model that was formally a two-dimensional cellular automaton. Systems
equivalent to two-dimensional cellular automata were arising in several other contexts around the
same time (see the NKS book, page 876). Von Neumann seems to have viewed his version as a convenient
framework in which to construct a mathematical system that could emulate engineered computer systems—
especially the EDVAC on which von Neumann worked.

In the period 1952–53 von Neumann sketched an outline of a proof that it was possible for a formal 
system to support self reproduction. Whenever he needed a different kind of component (wire, 
oscillator, logic element, etc.) he just added it as a new state of his cellular automaton, with 
new rules. He ended up with a 29-state system, and a 200,000-cell configuration that could reproduce
itself. (Von Neumann himself did not complete the construction. This was done in the early 1960s by
a former assistant of von Neumann’s named Arthur Burks, who had left the IAS computer project to
concentrate on his interests in philosophy, though who maintains even today an interest in cellular
automata.)

From the point of view of NKS, von Neumann’s system now seems almost grotesquely complicated. But
von Neumann’s intuition told him that one could not expect a simpler system to show something as 
sophisticated and biological as self reproduction. What he said was that he thought that below a
certain level of complexity, systems would always be “degenerative”, and always generate what 
amounts to behavior simpler than their rules. But then, from seeing the example of biology, and 
of systems like Turing machines, he believed that above some level, there should be an “explosive”
increase in complexity, with systems able to generate other systems more complex than themselves. 
But he said that he thought the threshold for this would be systems with millions of parts.

Twenty-five years ago I might not have disagreed too strongly with that. And certainly for me it 
took several years of computer experimentation to understand that in fact it takes only very simple 
rules to produce even the most complex behavior. So I do not think it surprising—or unimpressive—
that von Neumann failed to realize that simple rules were enough. ...

I have asked many people who knew him why von Neumann never considered simpler rules. Marvin Minsky
told me that he actually asked von Neumann about this directly, but that von Neumann had been 
somewhat confused by the question. It would have been much more Ulam’s style than von Neumann’s to
have come up with simpler rules, and Ulam indeed did try making a one-dimensional analog of 2D 
cellular automata, but came up not with 1D cellular automata, but with a curious number-theoretical
system (see the NKS book, page 908).

In the last ten years of his life, von Neumann got involved in an impressive array of issues.
Some of his colleagues seem to have felt that he spent too little time on each one, but still 
his contributions were usually substantial—sometimes directly in terms of content, and usually 
at least in terms of lending his credibility to emerging areas.

He made mistakes, of course. He thought that each logical step in computation would necessarily
dissipate a certain amount of heat, whereas in fact reversible computation is in principle 
possible. He thought that the unreliability of components would be a major issue in building large
computer systems; he apparently did not have an idea like error-correcting codes. He is reputed to 
have said that no computer program would ever be more than a few thousand lines long. He was 
probably thinking about proofs of theorems—but did not think about subroutines, the analog of lemmas.

Von Neumann was a great believer in the efficacy of mathematical methods and models, perhaps 
implemented by computers. In 1950 he was optimistic that accurate numerical weather forecasting
would soon be possible (see the NKS book page 1132). In addition, he believed that with methods
like game theory it should be possible to understand much of economics and other forms of human 
behavior (see the NKS book page 1135).

Von Neumann was always quite a believer in using the latest methods and tools (I’m sure he would 
have been a big Mathematica user today). He typically worked directly with one or two collaborators,
sometimes peers, sometimes assistants, though he maintained contact with a large network of
scientists. (A typical communication was a letter he wrote to Alan Turing in 1949, in which he asks,
“What are the problems on which you are working now, and what is your program for the immediate 
future?”) In his later years he often operated as a distinguished consultant, brought in by the
government, or other large organizations. His work was then often presented as a report, that was 
accorded particular weight because of his distinguished consultant status. (It was also often a good
and clear piece of work.) He was often viewed a little ambivalently as an outsider in the fields he 
entered—positively because he brought his distinction to the field, negatively because he was not in
the clique of experts in the field.

Particularly in the early 1950s, von Neumann became deeply involved in military consulting, and 
I wonder how much of the intellectual style of Cold War US military strategic thinking actually
originated with him. He seems to have been quite flattered that he was called upon to do this
consulting, and he certainly treated the government with considerably more respect than many 
other scientists of his day. Except sometimes in his exuberance to demonstrate his mathematical 
and calculational prowess, he seems to have always been quite mature and diplomatic. The transcript
of his testimony at the Oppenheimer security hearing certainly for example bears this out.

Nevertheless, von Neumann’s military consulting involvements left some factions quite negative
about him. It’s sometimes said, for example, that von Neumann might have been the model for the
sinister Dr. Strangelove character in Stanley Kubrick’s movie of that name (and indeed von Neumann
was in a wheelchair for the last year of his life). And vague negative feelings about von Neumann
surface for example in a typical statement I heard recently from a science historian of the period—
that “somehow I don’t like von Neumann, though I can’t remember exactly why”.

I recently met von Neumann’s only child—his daughter Marina, who herself has had a distinguished
career, mostly at General Motors. She reinforced my impression that until his unpleasant final 
illness, John von Neumann was a happy and energetic man, working long hours on mathematical topics,
and always having fun.
```

The fullest version of the Dantzig-meets-von Neumann story I’ve seen comes from the [IFORS Operational Research Hall of Fame article](http://www.cs.xu.edu/~neyer/Math/NumberTheory/Research/VonNeumann.pdf) on the latter:

```markdown
Looking at von Neumann’s game theory mathematical results in terms of matrix and linear relationships,
one can see how and why von Neumann reacted to George Dantzig’s description of the newly formulated 
linear-programming model when they first met in 1947. Here is that story as told by Dantzig 
(1982, p. 45):

On October 3, 1947, I visited him (von Neumann) for the first time at the Institute for Advanced Study 
at Princeton. I remember trying to describe to von Neumann, as I would to an ordinary mortal, the Air
Force problem. I began with the formulation of the linear programming model in terms of activities and
items, etc. Von Neumann did something which I believe was uncharacteristic of him. ‘‘Get to the point,’’
he said impatiently. Having at times a somewhat low kindling-point, I said to myself ‘‘O.K., if he wants
a quicky, then that’s what he will get.’’ In under one minute I slapped the geometric and algebraic
version of the problem on the blackboard. Von Neumann stood up and said ‘‘Oh that!’’ Then for the next 
hour and a half, he proceeded to give me a lecture on the mathematical theory of linear programs.

At one point seeing me sitting there with my eyes popping and my mouth open (after I had searched the 
literature and found nothing), von Neumann said: ‘‘I don’t want you to think I am pulling all this out of
my sleeve at the spur of the moment like a magician. I have just recently completed a book with Oscar
[sic] Morgenstern on the theory of games. What I am doing is conjecturing that the two problems are 
equivalent. The theory that I am outlining for your problem is an analogue to the one we have developed 
for games.’’ Thus I learned about Farkas’ Lemma, and about duality for the first time.
```

This certainly sounds more reasonable / realistic than the version perpetuated in folklore, which has von Neumann (completely out of the blue, as it were) developing the theory in a 1.5-hour lecture to a flabbergasted Dantzig after the latter had just begun describing the simplex method to vN. JvN was (to quote Trump on Kim Jong Un) “a pretty smart cookie”, or (paraphrasing user ‘mathwonk’ on Ed Witten) “a little smarter than the average bear”, but the standard folklore version makes him out to be completely superhuman.



<a name="#Butler-Lampson"></a>
### Butler Lampson
([overview](#overview))

Alan Kay, answering [I don't want be a specialist. How can I be a great generalist in computer science?](https://www.quora.com/I-dont-want-be-a-specialist-How-can-I-be-a-great-generalist-in-computer-science):

```markdown
Every once in a while I’m asked “what does it mean to be a computer scientist?” My answer is “I 
don’t know exactly, but take a look at Butler Lampson. He is certainly the quintessential Computer
Scientist if ever there was one”. He must have been the easiest Turing Award winner to choose in 
1992 (the top recognition in computing)… Certainly in the top few of the most impressive people 
I’ve ever met, and especially to have had the fun of working with.
```

I found this really intriguing for three reasons. The first reason is that I’d been reading a lot of Alan Kay’s answers on Quora recently, and I liked that he was a dreamer of a pioneer who was good enough to make headway on those dreams — good enough to win him the Turing Award himself, in fact; Alan Kay’s one of the few people I wouldn’t hesitate to apply the overused word visionary to — and it took a lot to impress him, so getting singled out for such glowing praise from Kay (over the likes of Donald Knuth!) was certainly something. It also helped that Kay praises Lampson for being a generalist, and portrays him as being a true “end-to-end” contributor, which I liked since I’ve repeatedly said on [my blog](https://mosstuff.quora.com) that generalists and subfield-cross-pollinators and their kin are My Kind Of People.

The second reason is that it hasn’t been that long since I read [these two](https://sites.google.com/site/steveyegge2/math-every-day) [blog posts](http://steve-yegge.blogspot.my/2006/03/moores-law-is-crap.html) by Steve Yegge, and thought sure, JvN probably had more raw mental horsepower than anyone else in the 20th century, but he also had the advantage of being the first person to ‘mathematize’ fields that hadn’t yet been ‘mathematized’, so that there were low-hanging fruit by the bushel; what might he have done if he were born a couple decades late? At least if we confine ourselves to computer science (and since we’re using Alan Kay’s definition of computer science as the study of systems and processes very broadly construed, this isn’t even much of a constraint), the answer, judging from the way his peers all talk about him, in addition to his own [ridiculously long list of systems he’s worked on](http://bwlampson.site/Systems.htm), seems to be Butler Lampson. Seriously, go skim through that list and tell me you don’t feel the same way you do in going through JvN’s ‘known for’ list on Wikipedia.

The third reason was that on a lark, I decided to skim through some of his [lecture notes](http://bwlampson.site/48-POCScourse/48-POCS2006Abstract.html) for the Principles of Computer Systems course he taught at MIT, and I was struck by their no-nonsense near-incompressible density — I tend to be sensitive to fluff in technical reads, because I lose the forest for the trees more easily than most, which is why I make it a habit to summarize the heck out of lecture note-style reads online on a topic I get interested in; Lampson’s lecture notes seemed like they were already summarized, which gave the impression of a ton of mental horsepower.

(Also, see this: [Turing100 Event Report: Work of Butler Lampson - Systems](http://punetech.com/turing100-event-report-work-of-butler-lampson-systems/))

The quotes themselves are pretty fun — maybe not in the same ballpark as the almost-whimsical ones about von Neumann and Ed Witten, but not too far behind. 

Eric Schmidt, Google executive chairman, in an [opening statement](http://news.mit.edu/2014/mit-professor-made-much-of-our-world-possible-0218) at [LampsonFest](https://www.microsoft.com/en-us/research/event/lampsonfest/?from=http://research.microsoft.com/en-us/events/lampsonfest/default.aspx) which commemorates Lampson’s contributions to computing:

```markdown
Butler is probably the broadest and smartest computer scientist today. We all just tried to keep
up with him — and almost always fell behind. His contributions made much of our world possible,
and I am beyond grateful.
```

Eric Metcalfe, Ethernet inventor, answering [What was working at Xerox PARC in the early 1970s like?](https://www.quora.com/What-was-working-at-Xerox-PARC-in-the-early-1970s-like):

```markdown
Heaven on Earth. We worked until we got tired, slept until we woke up, no alarm clocks. Cost me a 
marriage, but I got to invent Ethernet, pioneer the Internet. Butler Lampson would win all the 
arguments, even when he was wrong -- a lesson there. By the way, Butler is right a lot.
```

Stan Hanks, answering [Who should come on Software Engineering Daily for an interview?](https://www.quora.com/Who-should-come-on-Software-Engineering-Daily-for-an-interview):

```markdown
From the Real World, I'd like to see Butler Lampson who was part of so many vast projects with
lasting impact that it boggles the mind, but no one knows who he is, hardly.
```

Wenbin Fang, answering [Who are some of the greatest computer scientists, past and present?](https://www.quora.com/Who-are-some-of-the-greatest-computer-scientists-past-and-present):

```markdown
Butler Lampson: You'll find many "classic" software engineering books just copied ideas from this
guy's paper Hints for Computer System Design
```

Various Turing Award winners talking about Butler Lampson at LampsonFest in much the same way Nobel Prize winners used to talk about Johnny von Neumann:

```markdown
Butler Lampson has as good a claim as anyone to the title of “father of the modern PC.” …

Lampson turned 70 in December; last week, a group of computer science luminaries gathered at 
Microsoft Research New England, at the edge of the MIT campus, for a daylong conference celebrating
his achievements.

Two major themes emerged from the morning sessions. The first was the sheer range of Lampson’s 
technical innovation. The other theme of the morning sessions was just how formidable — and, as
Metcalfe put it, “fast” — Lampson is in debate.

Sproull mentioned a unit of measure used in computer science circles, which indicates the “speed
of delivery of technical information” and is known as the lampson. “Most of us could ourselves only
achieve millilampsons,” Sproull said.
```

From [Cool jargon of the day](http://www.jargon.net/jargon/cool/):

```markdown
milliLampson /mil'*-lamp`sn/ /n./ A unit of talking speed, abbreviated mL. Most people run about 
200 milliLampsons. The eponymous Butler Lampson (a CS theorist and systems implementor highly regarded 
among hackers) goes at 1000. A few people speak faster.

This unit is sometimes used to compare the (sometimes widely disparate) rates at which people can 
generate ideas and actually emit them in speech. For example, noted computer architect C. Gordon Bell
(designer of the PDP-11) is said, with some awe, to think at about 1200 mL but only talk at about 300; 
he is frequently reduced to fragments of sentences as his mouth tries to keep up with his speeding brain.
```

More from Alan Kay:

```markdown
We find in Butler someone who covers in knowledge and skills a very wide range in computing and 
software engineering, who has not just designed/invented but also built operating systems, programming
languages, networking and internetworking systems, desktop media, specification languages, atomic 
transaction protocols, and much more, and hardware architectures and detailed designs for making hardware
... And more. A partial summary is here.

He must have been the easiest Turing Award winner to choose in 1992 (the top recognition in computing),
and is one of the few ever to have also been awarded the top engineering award, the IEEE John von Neumann
Award. He was inducted into both the National Academy of Science and the National Academy of Engineering...

Once in an ARPAnet design meeting in the 60s, Butler was supposed to give a one hour talk at the end of 
the day, but the meeting dragged on. When it was his turn, he looked at his watch and said “I’ve got a plane
I have to catch”, and proceeded to give the -entire- talk in 20 minutes, in perfect understandable English 
at great speed! It was shockingly impressive even from him. The person next to me said “It’s a great 
privilege to just know someone like Butler”. That talk — on a capability based operating system design — 
covered the content in a paper you can read.

Butler is also one of the clearest writers in our field. Here is a partial list of his papers over the years.
And here is the first page of his website with a few tips on stuff to look at that has been most popular with 
readers.

Certainly in the top few of the most impressive people I’ve ever met, and especially to have had the fun of
working with.

I think that — besides Butler’s obvious brilliance — there are a few keys to his range and depth, and many
of these revolve around -systems and processes- rather than hardware and software, or programming languages
and operating systems. Dealing with whole systems was in the zeitgeist of the ARPA community back then, and
it found Butler while on his way — after an undergrad degree in Physics from Harvard — to grad school in 
Physics at Berkeley. As he puts it “I walked in the wrong door and found people trying to debug code for a
new computer. And never found the doorway out”.

Butler was and is interested in -anything- that has dynamic relationships — systems and processes — (and
note the analogies with what is studied in physics). I think looking at the world this way — all systems
and processes (including physical and biological) — will round you into a “generalist in computer scientist” 
who can be a vital contributor not just a dabbler. There are only a few people around like Butler, but how
he has gone about his art can be and is an inspiration for the rest of us.
```

Leslie Lamport [talking about](https://www.microsoft.com/en-us/research/publication/part-time-parliament/?from=http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf) how Butler Lampson was the only person to understand the significance of his Paxos algorithm:

```markdown
I thought, and still think, that Paxos is an important algorithm. Inspired by my success at popularizing the consensus problem by describing it with Byzantine generals, I decided to cast the algorithm in terms of a parliament on an ancient Greek island. Leo Guibas suggested the name Paxos for the island. I gave the Greek legislators the names of computer scientists working in the field, transliterated with Guibas’s help into a bogus Greek dialect. (Peter Ladkin suggested the title.) Writing about a lost civilization allowed me to eliminate uninteresting details and indicate generalizations by saying that some details of the parliamentary protocol had been lost. To carry the image further, I gave a few lectures in the persona of an Indiana-Jones-style archaeologist, replete with Stetson hat and hip flask.

My attempt at inserting some humor into the subject was a dismal failure. People who attended my lecture remembered Indiana Jones, but not the algorithm. People reading the paper apparently got so distracted by the Greek parable that they didn’t understand the algorithm. Among the people I sent the paper to, and who claimed to have read it, were Nancy Lynch, Vassos Hadzilacos, and Phil Bernstein. A couple of months later I emailed them the following question:

Can you implement a distributed database that can tolerate the failure of any number of its processes (possibly all of them) without losing consistency, and that will resume normal behavior when more than half the processes are again working properly?

None of them noticed any connection between this question and the Paxos algorithm.

I submitted the paper to TOCS in 1990. All three referees said that the paper was mildly interesting, though not very important, but that all the Paxos stuff had to be removed. I was quite annoyed at how humorless everyone working in the field seemed to be, so I did nothing with the paper. A number of years later, a couple of people at SRC needed algorithms for distributed systems they were building, and Paxos provided just what they needed. …

So, I thought that maybe the time had come to try publishing it again.

Meanwhile, the one exception in this dismal tale was Butler Lampson, who immediately understood the algorithm’s significance. He mentioned it in lectures and in a paper, and he interested Nancy Lynch in it. De Prisco, Lynch, and Lampson published their version of a specification and proof. Their papers made it more obvious that it was time for me to publish my paper. …
```

<a name="#grigori-perelman"></a>
### Grigori Perelman
([overview](#overview))

From Masha Gessen's biography *Perfect Rigor*, the "universal compactor" quote:

```markdown
Golovanov, who studied and occasionally competed alongside Perelman for more than ten years,
tagged him as an unambiguous geometer: Perelman had a geometry problem solved in the time it 
took Golovanov to grasp the question. This was because Golovanov was an algebraist. Sudakov, 
who spent about six years studying and occasionally competing with Perelman, claimed Perelman
reduced every problem to a formula. This, it appears, was because Sudakov was a geometer: his
favorite proof of the classic theorem above was an entirely graphical one, requiring no 
formulas and no language to demonstrate. 

In other words, each of them was convinced Perelman’s mind was profoundly different from his 
own. Neither had any hard evidence. Perelman did his thinking almost entirely inside his head,
neither writing nor sketching on scrap paper. He did a lot of other things—he hummed, moaned, 
threw a Ping-Pong ball against the desk, rocked back and forth, knocked out a rhythm on the 
desk with his pen, rubbed his thighs until his pant legs shone, and then rubbed his hands 
together—a sign that the solution would now be written down, fully formed. 

For the rest of his career, even after he chose to work with shapes, he never dazzled colleagues
with his geometric imagination, but he almost never failed to impress them with the single-
minded precision with which he plowed through problems. His brain seemed to be a universal math 
compactor, capable of compressing problems to their essence. Club mates eventually dubbed whatever
it was he had inside his head the “Perelman stick”—a very large imaginary instrument with which 
he sat quietly before striking an always-fatal blow.
```

<a name="#Alexander-Grothendieck"></a>
### Alexander Grothendieck
([overview](#overview))

From Barry Mazur's 2016 retrospective [Thinking about Grothendieck](http://www.math.harvard.edu/~mazur/papers/Thinking.about.Grothendieck%285%29.pdf):

```markdown
During the early 60’s his conversations had a secure calmness. He would offer mathematical ideas
with a smile that always had an expanse of generosity in it. Firm feet on the ground; sometimes
barefoot. Transparency: his feelings towards people, towards things, were straightforwardly felt,
straightforwardly expressed — often garnished with a sprig of morality. 

But perhaps the word ‘morality’ doesn’t set the right tone: one expects a dour or dire music to 
accompany any moral message. Grothendieck’s opinions, observations, would be delivered with an 
upbeat, an optimism, a sense that “nothing could be easier in the world” than to view things as 
he did. In fact, as many people have mentioned, Grothendieck didn’t butt against obstacles, but 
rather he arranged for obstacles to be dissolved even before he approached them. The mathematical 
road, he would seem to say, shows itself to be ‘the correct way’ by how easy it is to travel along
it. This is, of course, a vastly different ‘ease’ than what was an intellectual abomination to 
Grothendieck: something he called, with horror, “tourner la manivelle” (or ‘cranking it out’).

...

The mathematical talks I had with him—as I remember them now—were largely, perhaps only, about 
viewpoint, never about specifics (with the exception of a conversation about differential 
structures on conjugate complexifications of an algebraic variety over a number field). 
Grothendieck’s message was clear throughout: that everything important will follow easily, will
flow, from the right vantage. It was principally ‘the right vantage,’ a way of seeing mathematics,
that he sought, and perhaps only on a lesser level, its by-products.
```

His simplicity and hospitality:

```markdown
Simplicity was a great virtue for him, in ideas, in material possessions, in food. The main objects
in his living-room when he lived in an apartment at R´esidence Gratien, in Bures-sur-Yvette, were
a wrought-iron statue of a goat, a large urn filled with oil-cured black olives, a small somewhat
rickety table on which perched his typewriter (his work-space). You could meet him on the way
from market, during the weekly market-day in Bures, carrying only one (ample) bag of grapes,
eating them as he walked and offering them to you.

His hospitality was startling. Later, when he lived near the RER stop Massy-Verri`ere he once
invited an entire family who needed lodging, to stay in his basement and to bring with them their
in-laws. He helped them install a taramasalata machine there to give them some economic activity.
```

His attitude towards shopping malls and non-math objects in general:

```markdown
In encountering a shopping mall when he visited Cambridge (USA) his only utterance was an
Elizabethan “Let us flee.” How sparing he was in any activity other than mathematics during the
sixties. As a result, some of his non-mathematical experiences at that time had revelatory force
for him. He returned to Bures from Paris one day (this was probably the late sixties) saying that
he’d just seen the first movie he had seen in 12 years (Butch Cassidy and the Sundance Kid)
and was struck by its moral complexity. The one non-mathematical book I know he was reading
with intense respect at that epoch was a volume entitled ”History of the Jews” (I’ve forgotten its
author, or the language in which Grothendieck was reading it). John Tate writes that Moby Dick
was Grothendieck’s favorite novel.
```

Leila Schneps' [Mathematics and creativity](http://people.math.jussieu.fr/~leila/grothendieckcircle/chap1.pdf) contains the following passage:

```markdown
Pierre Cartier observed that when Grothendieck took interest in some mathematical domain that he 
had not considered up till then, finding a whole collection of theorems, results and concepts 
already developed by others, he would continue building on this work ‘by turning it upside down’.
Michel Demazure described his approach as ‘turning the problem into its own solution’. In fact, 
Grothendieck’s spontaneous reaction to whatever appeared to be causing a difficulty - nilpotent 
elements when taking spectra or rings, curve automorphisms for construction of moduli spaces - was
to adopt and embrace the very phenomenon that was problematic, weaving it in as an integral 
feature of the structure he was studying, and thus transforming it from a difficulty into a 
clarifying feature of the situation. (p. 8)
```

Rene Thom, 1958 Fields Medalist:

```markdown
His technical superiority was crushing,” Thom wrote. “His seminar attracted the whole of Parisian 
mathematics, whereas I had nothing new to offer.
```

Pierre Deligne, 1978 Fields Medalist:

```markdown
When I was in in Paris as a student, I would go to Grothendieck's seminar at IHES... I enoyed the 
atmosphere around him very much ... we did not care much about priority because Grothendieck had the
ideas that we were working on and priority would have meant nothing.
```

Mikhail Gromov, 2010 Abel Prize Winner:

```markdown
(The IHES) is a remarkable place.. I knew about it before I came there; it was a legendary place
because of Grothendieck. He was kind of a god in mathematics.
```

Ngo Bau Chau, 2010 Fields Medalist:

```markdown
On arriving at the IHES, we ordinary mathematicians share the same feeling that Muslims experience
on a pilgrimage to Mecca. Here is the place were, for a dozen or so years, Grothendieck relentlessly
explained the holy word to his apostles. Of that saga, only the apocrypha reached us in the form of 
big, yellow, boring-looking books edited by Springer. These dozens of volumes...are still our most 
precious working companion.
```

Grothendieck on himself, cf dumb ox/sleight of hand:

```markdown
I've had the chance...to meet quite a number of people, both among my "elders" and among young 
people in my general age group, who were much more brilliant, much more "gifted" than I was. I
admired the facility with which they picked up, as if at play, new ideas, juggling them as if 
familiar with them from the cradle - while for myself I felt clumsy. even oafish, wandering 
painfully up a arduous track, like a dumb ox faced with an amorphous mountain of things that I 
had to learn ( so I was assured), things I felt incapable of understanding the essentials or 
following through to the end. Indeed, there was little about me that identified the kind of bright
student who wins at prestigious competitions or assimilates, almost by sleight of hand, the most
forbidding subjects.
```

<a name="#terry-tao"></a>
### Terry Tao
([overview](#overview))

The University of St Andrew's [biography](http://www-history.mcs.st-andrews.ac.uk/Biographies/Tao.html) of Terry has some of the more effusive (overmuch, honestly) praise I've ever seen for an active mathematician. I mean, look at these sentences, pretty much randomly picked:

```markdown
It is very difficult to write a biography of someone who is at the height of their creative
powers as Tao is. Anything that one writes about his research contributions will be quickly 
outdated as he is contributing major results in such a wide range of different areas.

[He] has produced such a fantastic collection of results, leading to the award of all the top
prizes in mathematics, that one must try to at least give a vague picture of the work of this
remarkable mathematician.

To dismiss this fantastic achievement in a single sentence seems silly, but there is so much
more to say.

Before looking at his contributions we note the prizes and awards he has received (although 
again this list is bound to become rapidly outdated as he continues to receive awards).

One might imagine that with his remarkable output of research papers, Tao would not find time 
to write books. However, this would be entirely wrong since he has produced both research
monographs and undergraduate texts.

But, amazingly, this still does not complete the list of Tao's 2006 books...

It will come as no surprise to learn that Tao, who is such an innovator in everything he does,
has created a new style of book. ...
```

Tim Gowers on Terry, in a review:

```markdown
Textbooks and popular science are still the two obvious niches for mathematics in the book market,
but the advent of the Internet has brought about a sudden change in the possibilities for mathematical
exposition, because now anybody can put anything they like on the Web. As a result, there has been a
rapid rise in a form of mathematical exposition that is too technical for the layperson, but much 
easier to read and enjoy for mathematicians than a textbook. A medium that is particularly well 
suited to this is the blog, and the undisputed king of all mathematics blogs, with thousands of regular
readers, is that of Terence Tao. 

Tao's mathematical knowledge has an extraordinary combination of breadth and depth: he can write 
confidently and authoritatively on topics as diverse as partial differential equations, analytic number
theory, the geometry of 3-manifolds, nonstandard analysis, group theory, model theory, quantum mechanics, 
probability, ergodic theory, combinatorics, harmonic analysis, image processing, functional analysis, and 
many others. Some of these are areas to which he has made fundamental contributions. Others are areas that 
he appears to understand at the deep intuitive level of an expert despite officially not working in those 
areas. 

How he does all this, as well as writing papers and books at a prodigious rate, is a complete mystery. It
has been said that Hilbert was the last person to know all of mathematics, but it is not easy to find gaps
in Tao's knowledge, and if you do then you may well find that the gaps have been filled a year later. Now,
in an interesting experiment, several of Tao's blog posts have been tidied up (partly in response to 
comments from others on the posts) and published as books.
```

From an article describing the award of the Fields:

```markdown
Terence Tao is a supreme problem-solver whose spectacular work has had an impact across several 
mathematical areas. He combines sheer technical power, an other-worldly ingenuity for hitting upon new 
ideas, and a startlingly natural point of view that leaves other mathematicians wondering, " Why didn't 
anyone see that before?" At 31 years of age, Tao has written over eighty research papers, with over thirty
collaborators, and his interests range over a wide swath of mathematics, including harmonic analysis, 
nonlinear partial differential equations, and combinatorics. "I work in a number of areas, but I don't view
them as being disconnected," he said in an interview published in the Clay Mathematics Institute Annual Report.
"I tend to view mathematics as a unified subject and am particularly happy when I get the opportunity to work on
a project that involves several fields at once."
```

Julian Stanley of the Study of Mathematically Precocious Youth (SMPY) wrote the following about Terry in his 1986 report [Radical Acceleration in Australia: Terence Tao](http://www.davidsongifted.org/Search-Database/entry/A10116), when Terry was still 10:

```markdown
On May 1985 I administered to [10 year old] Terry the Raven Progressive Matrices Advanced, an untimed
test. He completed its 36 8-option items in about 45 minutes. Whereas the average British university 
student scores 21, Terry scored 32. He did not miss any of the last, most difficult, 4 items. Also,
when told which 4 items he had not answered correctly, he was quickly able to find the correct
response to each. Few of SMPY's ablest protégés, members of its "700-800 on SAT-M Before Age 13"
group, could do as well.
```

Jonah Sinick expands upon Stanley's remark above in the following quote from his LW post [Innate mathematical ability](https://www.lesswrong.com/posts/5Xrv5aXfb95uHcseN/innate-mathematical-ability):

```markdown
People like Terry are perhaps 1 in a million, but I've had the chance to tutor several children who 
are in his general direction.

Descriptions of milestones like "scored 760 on the math SAT at age 8" (as Terry did) usually greatly
understate the ability of these children when the milestone is interpreted as "comparable to a high 
school student in the top 1%," in that there's a connotation that the child's performance comes from
the child having learned the usual things very quickly. The situation is usually closer to "the child
hasn't learned the usual things, but is able to get high scores by solving questions ththat high
school students wouldn't able to able to solve without having studied algebra and geometry."

A impact of interacting with such a child can be overwhelming. I've repeatedly had the experience of
teaching such a child a mathematical topic typically covered only in graduate math courses, and one 
that I know well beyond the level of textbook expositions, and the child responding by making 
observations that I myself had missed. The experience is surreal, to the point that I wouldn't have 
been surprised to learn that it had all been a dream 30 minutes later.

Tangentially, I don't know why we were assigned this problem, which is of considerable mathematical 
interest, but also outside of the usual high school curriculum. In any case, I remember puzzling over
it. Based on my experiences with children similar to Terry, it seems likely that his 8-year old self
would see how to answer it immediately, without having ever seen anything like the problem before.
Roughly speaking, an 8-year old child like Terry can recognize abstract patterns that very few (if 
any) of a group of 30 high school students with the math SAT score would be able to recognize.
```

Terry is very math-skewed. Back to Stanley:

```markdown
Yet at age 8 years 10 months, when he took both the SAT-M and the SAT-Verbal, Terry scored only 290
on the latter. Just 9% of college-bound male 12th-graders score 290 or less on SAT-V; a chance score
is about 230. The discrepancy between being 10 points above the minimum 99th percentile on M and at 
the 9th percentile on V represents a gap of about 3.7 standard deviations. Clearly, Terry did far 
better with the mathematical reasoning items (please see the Appendix for examples) than he did 
reading paragraphs and answering comprehension questions about them or figuring out antonyms, verbal 
analogies, or sentences with missing words.

Was the "lowness" of the verbal score (excellent for one his age, of course) due to his lack of
motivation on that part of the test and/or surprise at its content? A year later, while this
altogether charming boy was spending four days at my home during early May of 1985, I administered 
another form of the SAT-V to him under the best possible conditions. His score rose to 380, which is 
the 31st percentile. That's a fine gain, but the M vs. V discrepancy was probably as great as before.
Quite likely, on the SAT score scale his ability had risen appreciably above the 800 ceiling of SAT-M. 
```

Some other nice quotes from Stanley's report:

```markdown
Ten-year-old Terence Tao, or Adelaide, South Australia, is a prodigiously gifted young mathematician.
Julian Stanley, Director of the Study of Mathematically Precocious Youth (SMPY) at Johns Hopkins 
University states that Terry has the greatest mathematical reasoning ability he has found in 14 
years of intensive searching (Stanley, 1985). ...

In some ways it is ironical that such a profoundly gifted student as Terry should have appeared in
Australia, an obsessively egalitarian society where social antipathy towards gifted children and 
towards those who would set up special programs for the gifted is a powerful deterrent to the
establishment of state-mandated gifted programs. Two Australian states, Western Australia and
Victoria, do provide structured accelerated programs for highly gifted secondary school students 
within the government system, but these programs are under intense criticism from the teachers' unions, 
the media and many politicians. Terry's state, South Australia, has no such program; the few cases of 
acceleration which have arisen have proceeded from the interest and concern of individual teachers and
have received little or no support from the state education system.
```

Terry's father, Dr. Billy Tao, a pediatrician, "paradoxically" feels that in Terry's case the absence of a formalized structure may have been an advantage rather than a hindrance:

```markdown
My wife and I have been fortunate in having been able to work very closely, first with the principals 
and staffs of Terry's primary and secondary schools and later with the faculty of Flinder University,
to design a highly individualized program which has been tied in to Terry's levels of ability in all 
subject areas, not only in maths and the sciences but also in the humanities. If South Australia had 
already had well established gifted programs, Terry might have been drawn into a less flexible system,
quite different from what has actually eventuated. 
```

Back to Stanley -- the "typewriter incident":

```markdown
A few months after Terry's second birthday, the Taos found him using a portable typewriter which stood 
in Dr. Tao's office; he had copied a whole page of a children's book laboriously with one finger! At 
this stage his parents decided that, although they did not want to 'push' their brilliant son, it would
be foolish to hold him back. They began to borrow and buy books for him and, indeed, found it hard to 
keep pace with the boy. They encouraged Terry to read and explore but were careful not to introduce him
to highly abstract subjects, believing, rather, that their task was to help him develop basic literacy
and numerical skills so that he could learn from books by himself and thus develop at his own rate. 
"Looking back," says Dr. Tao, "we are sure that it was this capacity for individual learning which helped
Terry to progress so fast without ever becoming bogged down by the inability to find a suitable tutor at
a crucial time." By the age of 3, Terry was displaying the reading, writing and mathematical ability of a
6-year-old.
```

Terry's parents get involved with the South Australian Association for Gifted and Talented Children (SAAGTC), a group of teachers and parents of gifted students who run Saturday programs for gifted children and seminars and workshops for parents and teachers:

```markdown
Here, for the first time, Terry was able to work and socialize with other highly gifted children. Although
he met no one sharing his own prodigious math ability (this is not surprising because in terms of IQ alone
he is, statistically, one in a million and the entire population of Australia is less than sixteen million)
he was able to mix with other children who shared his hunger for information, his ability to assimilate and
integrate abstract concepts, and his delight in creative exploration. Even within the accelerated context 
of SAAGTC programs, however, it was found necessary to accelerate Terry still further beyond his gifted 
age-mates. 

I vividly recall one of my first meetings with Terry when, in my capacity as President of SAAGTC, I was 
informally assessing his mathematical ability for placement in SAAGTC programs. At just under 4, he was 
multiplying 2-digit numbers by 2-digit numbers in his head while I, the 'tester', required pen and paper
to check his answers! Another image springs to mind of Terry, one month before his 5th birthday, working
with a group of gifted 7- to 9-year-olds at an SAAGTC math workshop. The teacher challenged the students
to find the next four numbers in the sequence 9182736. Terry thought briefly and responded, "4554." He
was, of course, correct. The number sequence consists of consecutive multiples of 9.
```

Terry's "Fibonacci" program, written in BASIC when he was 6:

```markdown
At home, Terry continued his advanced study in mathematics. By the age of 6, having taught himself BASIC
language (by reading a manual), he had written several computer programs on mathematics problems. He is
a lively, creative child with a puckish sense of humour; something of his personality comes over in the 
introduction to his "Fibonacci" program, which is quoted in full by Clements (1984).

8 print "J" (This symbol means "clear the screen") 
10 print "here comes mr. fibonacci"
20 print "can you guess which year was mr fibonacci born?" 
30 print "write down a number please . . . ": input c
31 if c = 1170 then print "you are correct; now we start!": go to 150 
50 if c > 1250 then print "no, he is already in heaven; try again": go to 30
60 if c < 1170 then print "sorry, he wasn't born yet!; try again": go to 30
70 if c > 1170 < 1250 then print "he would be c-1170 years old." 
71 print "now can you guess?"
The program goes on to produce all the Fibonacci numbers up to the level requested by the player.
```

First publication, at 8:

```markdown
At the age of 8 years 3 months Terry achieved his first publication, a BASIC program to calculate 
perfect numbers.
```

Social experience in high school -- see concluding remark:

```markdown
Terry adapted to Blackwood High, and Blackwood High to Terry, with little difficulty. John Fidge, his
Grade 11 math teacher, found that after the strangeness of the first 2 weeks he was accepted as just
another member of the class and regarded as a friendly, well-adjusted, helpful and good-natured lad
by his classmates (Clements, 1984). This, when one considers that Terry was, even at this early stage,
finishing his work two lessons before his 16-year-old classmates, says much for his social skills. He
is a delightful young boy who is aware that he is different but displays no conceit about his
remarkable gifts and has an unusual ability to relate to a wide range of people, from children younger
than himself to the university faculty members with whom he now works. Billy Tao believes that the two
years of 'riding' several classes each term at primary school laid the groundwork for his being able
to cope with much older students later.

An important factor in Terry's happy assimilation into high school was that, because of his extreme 
youth, he was not seen as a threat, either intellectually or socially, by the 16- and 17-year-olds with
whom he worked. He was visibly not in competition with them for jobs, scholarships, or boy-girl 
friendships; paradoxically, this allowed him to be treated as just another member of the class, with
the very occasional privilege of getting a piggy-back ride from teacher during bush-walking excursions!
"I am now convinced," says Dr. Tao, "that it is in fact easier to integrate a highly gifted child into
a higher grade in secondary school than a lower grade and Terry was lucky that he started math in 
Grade 11 instead of Grade 8."
```

The key event that made Terry's parents delay full-time university entrance:

```markdown
The question now before the family is: What direction should Terry take? There is little doubt that, 
if he chose to enter university full-time, he would graduate in mathematics before his 12th birthday.
(He would, incidentally, become the youngest person ever to do so; currently, the youngest graduate is
Jay Luo who took a degree at Boise State University, Idaho, at the age of 12 years 42 days.) Terry's 
IQ has been assessed as between 220 and 230, and he has no areas of academic weakness. Even in English
and social studies, which he considers his weaker subjects, he is working at a level 4 years above his
chronological age.

A number of factors have influenced the Tao family's thinking about Terry's future study. In September,
1984, at the age of 9, he was invited, with a small group of senior high school students, to compete in
an Australia-wide math competition to choose candidates to participate in the Australian Mathematical 
Olympiad. Despite being the youngest candidate by a margin of 5 years, he top-scored in South Australia 
and ranked sixth nationally. However, in the Australian Mathematical Olympiad, which was held 6 months 
later, he lost his sixth place and consequently was not selected for the Australian team which competed
in the International Mathematical Olympiad in Finland, in July, 1985.

The Taos feel there is an important message in this result. "Terry's development in Maths has been so 
fast," says Dr. Tao, "that like rapidly growing grass in fine weather, he has not had time to put down 
deep roots. When he was faced with really challenging work at a level he had not encountered before, 
that was when the weakness showed up. However, it is much kinder to Terry," he adds, "for him to 'fail'
within his own state rather than at the International Olympiad where he would have attracted a lot of
attention, even among the other competitors, because of his extreme youth."

In May, 1985, at the invitation of Julian Stanley, Terry and his parents spent 3 weeks in the United 
States, visiting a number of university campuses, including Johns Hopkins, Purdue, Columbia, Princeton,
Berkeley, and Stanford, and talking with experts in mathematics and gifted education. This experience, 
the Taos say, has helped to clarify their thinking on Terry's future education.

Terry's parents now feel that he should probably wait at least 3 more years before he enters university
full-time.
```

His dad Billy explains further:

```markdown 
There is no need for him to rush ahead now. If he were to enter full-time now, just for the sake of 
being the youngest child to graduate, or indeed for the sake of doing anything 'first,' that would 
simply be a stunt. Much more important is the opportunity to consolidate his education, to build a 
broader base. It is important for Terry to have a broad initial education.

I can see two different models of how his education could progress. The first is what I might call a 
"columnar" model, where his acceleration would be directed vertically upwards in maths and physics with
little expansion into other areas of knowledge. The problem here is that, although progress may be fast
and easy at the beginning, as the column gets taller it becomes more difficult to build on further 
knowledge, and, to continue the metaphor, the taller the structure grows, the shakier it may become. 

The other model, which we have selected, is pyramidical in shape, where Terry's work in mathematics and
the sciences is integrated with many other areas of knowledge. Initial progress may be slowed down while
he explores the relationships between all these areas of study, but as the pyramid gains height it becomes
easier and faster and the whole structure rests on a sound base of interrelated knowledge.

This broad base of knowledge is essential. At the highest level of any subject the boundary between
science and arts, between mathematics and philosophy, becomes less and less distinct. You cannot
enter this highest level of sophistication if you are too specialized. Even in pure mathematics there 
will be many problems which you cannot answer simply by applying mathematical techniques. Take 
Einstein and the theory of relativity, for example; it is not so much mathematics as concepts beyond 
computation.
```

Even at 10, it was already totally evident that Terry was simply a wonderful human being:

```markdown
Terry as an individual is almost universally liked and admired by the teachers and students with whom 
he works. Having had the opportunity to study the development of his personality over the last 6 years, 
I am sure that this warmth and acceptance are largely due to the gentleness and modesty of Terry's 
nature. He is able to talk frankly and confidently to strangers as well as friends, but displays no 
arrogance or conceit. His parents have taught him not to posture but, at the same time, not to conceal
his ability. Unlike most profoundly gifted children, he seems to have no difficulty in relating to 
people of lesser ability (Hollingworth, 1942). He has no conception of himself as "better" than others,
merely different. To Terry everyone is of value, everyone has something to contribute.

Terry's motivation to excel, to discover and to create is a burning force in his life. The results of
his endeavours--the award, the prize, or whatever--are of much less importance to him than the delight
of intellectual speculation. Billy Tao tells of Terry's reaction to the news that he had achieved the 
highest score Stanley had ever found for a child of his age, on the SAT-M:

I asked him what he would like as a reward and he probably thought that was a more difficult question 
than the SAT itself! After a few seconds he asked for a piece of chocolate which had been in the
refrigerator for some time and was almost forgotten. When I gave it to him he broke it into two halves 
and gave one to me. He was delighted with the result, of course, but there was no great celebration or 
anything like that. He was more interested in going back to the physics book he was reading (B. Tao, 1985).
```

Terry's capacity to analyse and comment on his own intellectual growth and development is surprizing in one so young. During his visit to Purdue University he spoke to the faculty and graduate staff of the Gifted Education Resource Institute about his early experiences:

```markdown
A couple of years ago I sat for a state wide maths competition for the first time. I was given 2 hours
to do it but I finished in 20 minutes and spent the remaining time devising a method to find the value 
of pi. Afterwards, when Mum found out what I had done and asked me why I didn't spend more time on the 
competition and check my answers, I just said, 'Wait till I get a prize!' Needless to say, I didn't get
any prize and I was quite depressed for a while. Dad later discovered that most of my wrong answers
were due to arithmetical errors. After that episode, I learned that I should always time myself during
an exam and check my work. Unfortunately, I still don't attend to the latter very well!

I discovered I could learn better and remember more if I taught my brothers what I had learned. So I 
taught one brother chess and the other music. My music has never been very good--in fact I hated it 
until I gave myself the motivation to teach Trevor. Now I actually quite enjoy playing duets with him.
I spent a lot of my spare time working out interesting ways to teach them, and I probably learned more
from teaching them than they did from me (T. Tao, 1985)!
```

John F. Feldhusen remarks on Terry's curiosity:

```markdown 
I had dinner with Terry Tao and his parents recently and was struck by the intellectual intensity of
Terry and his parents. Throughout the dinner Dr. Tao's conversation centered on Terry's precocity and
conditions which would facilitate his intellectual growth. Meanwhile Terry interacted constantly with
his mother about puzzles and about little curiosity experiments he was doing at the table, such as
holding a spoon in a candle flame. I was told by another family whose home he visited that he had been
extremely active in exploring things around the house. He seems to have an ever active need to be 
investigating and learning.
```

Harry Passow's impressions:

```markdown
I was fortunate to have an opportunity to meet Terence Tao and to hear his father describe the discovery
and nurturing of this mathematical prodigy. As I listened to Terence's father, I focused on Terence. Here
was a very attractive, alert, intelligent, articulate child who had obviously heard his parents discuss 
him and his development many times before and who, while bored with the discussion, seemed very patient
to me. Several times I tried to provide him with advanced mathematics texts on which he might work but 
they were not of the right kind and he soon closed them and let his eyes wander about my library and
artifacts.

The Taos left two documents with me which I have read and reread. One is titled, "My Recollections" which
is Terence's recollections of his early childhood experiences and consists of as he puts it, "some memories
which are very dear to me, some are actually a bit embarrassing, some funny, but most rewarding." The 
second is a document titled, "Reflections on Terry's Education," which is a talk which Billy Tao gave. 
Both are fascinating documents: Terry's because it conveys to the readers an impression of a very
intelligent, very insightful, and very sensitive young person. Terry concludes his little presentation as
follows: "I may be labelled as an intelligent child by some of my teachers, but I still have a long way to
go yet before I can become as wise as anyone of you here today." I read this after Terence had left my
office--but I could hear him saying it, seriously and sincerely.

Billy Tao's discussion of his son's development and education concludes with what he calls "the danger 
areas ahead," possible scenarios of what could happen to Terry. These range from Terry's possibly becoming 
"too big-headed," to his losing "interest in one subject, such as maths, and want to study another, such
as rock music," to the possibility that "he may burn out completely and lose all his brilliance, creativity
and productivity." A fourth possibility suggested is that Terry "may suddenly think that he had been conned,
that all his hard work, his good marks, had all been carefully orchestrated so as to fulfill my (his 
father's) own personal ego."

In my office, I have a card which talks about "your ordinary, everyday, run-of-the-mill genius." Terence 
Tao may or may not become a mathematical genius. He is certainly a mathematically precocious child with 
unusual potential and achievement. Moreover, he is, if you will, a "nice kid." I worried about whether he
was being exploited by his parents. Having met Terence Tao and his parents, I no longer am worried. Those 
of us interested in gifted education and, in particular, the education of what one of my friends calls the
"severely and profoundly gifted," will find the Taos a fascinating family with three children — one
mathematically precocious and another with learning disabilities but all being raised and educated 
intelligently, sensitively, and reflectively.
```

Stanley muses on Terry's skewed scores:

```markdown
How could Terry possibly learn mathematics and physical and computer sciences so well with only 
290-380V development? We of the Study of Mathematically Precocious Youth (SMPY) at Johns Hopkins have
discovered, chiefly by testing able 12-year-olds, that when the examinee's SAT-M score vastly exceeds
his or her SAT-V score the youth is almost certain to score high on a difficult test of nonverbal
reasoning ability such as the Advanced Form of the Raven Progressive Matrices, often higher than a high-M 
high-V examinee does. To test this out, on 6 May 1985 I administered to Terry the RPM-Advanced, an untimed
test. He completed its 36 8-option items in about 45 minutes. Whereas the average British university
student scores 21, Terry scored 32. He did not miss any of the last, most difficult, 4 items. Also, when
told which 4 items he had not answered correctly, he was quickly able to find the correct response to each.
Few of SMPY's ablest protégés, members of its "700-800 on SAT-M Before Age 13" group, could do as well.

Excellent nonverbal reasoning ability seems a necessary, though almost surely not the sufficient, condition 
for excelling in mathematics of the algebra-geometry-trigonometry-calculus variety at an early age. The
test-triad of SAT-M, SAT-V, and RPM-Advanced can give considerable insight into a mathematically apt youth's
intellectual powers. To those three I would add appropriate spatial and mechanical reasoning tests. For 
youngsters whose total score on a general achievement test battery of the kind usually administered in 
schools is high (e.g., 90th percentile or greater on seventh-grade norms), the 8-part Differential Aptitude
Test battery of The Psychological Corporation in New York City may be a good place to start.
```

<a name="#lebron-james"></a>
### LeBron James
([overview](#overview))

From Redditor xdawsonic on the thread [[Serious] How young of a LeBron would the average adult male be able to beat 1 on 1?](https://www.reddit.com/r/nba/comments/6huci7/serious_how_young_of_a_lebron_would_the_average/):

```markdown
I played against Lebron a handful of times when he was a Freshman, so I have a little experience 
in this area.

I'll relay my thoughts about the game I remember the most.

I was 18. A senior, decent athlete, a few D3 offers, a few "preferred walk on" for some D1 schools.
I was 6'3-6'4 and 180-190 pounds. He was 13-14 years old, and and near the same size as I was.. 
but enormous feet, long arms, but lanky. Probably weighed... 170?

I'll try to keep it somewhat short... I could hold my own against him, only because he was very, 
very raw. I feel my game was as polished as it could be, while he was still just getting things 
figured out. We both ended the game with similar stat lines, but the X factor was just the raw 
explosiveness and athleticism... something you honestly can't account for when talking about size,
weight and even age.

I'm sure alot of you have played basketball, maybe even lately.. but you know that first few 
possessions of the game? Sizing things up.. who can move, who can shoot, how are they guarding us....
Lebron was already on a different planet in terms of just sheer velocity. He could just get off of 
his feet, make a cut, or grab a rebound while I'm still flat footed. He didn't have springs in his 
legs.. he had trampolines. I had played against alot of tier 1 talent at that point at 5-Star and 
Blue Chip camps, but the odd thing is.. I knew IMMEDIATELY that this guy was something different..
and that isn't just hindsight.

Anyway, we lost that game by 6. Which ended up being their closest game of the year (applause? kappa)
... Lebron got a steal in the last 5 seconds to close the game.. and traveled the length of the court 
for the final "nail in the coffin" dunk. I was so pissed of.. he wasn't going to dunk on our home 
floor... I'm going to chase this FRESHMAN down and foul the hell out of him. Now.. I could jump, 
could dunk fairly easily.. so I figured... I can do this.. plus we are the same size.. what could
go wrong? So I get to him (he sort of took a wide approach angle), jump... andddd he's clearly a
foot above me at the apex. Easily a foot. I felt like Isiah in that Celtics series this year. I 
quickly decided to just do what I could, and just wailed on whatever part of his arms or head I
could reach. The dunk failed, so I guess it was a success... but it was just another lesson I 
realized that game. This man is far superior, and he hasn't even hit puberty yet. This is also one 
of those days when I decided to not play basketball in college. A rude awakening to just how good 
you have to be compete.

Anyway.. back to your question... St. V was good that year with alot of studs, so Lebron was 
considered "the next big thing", but everyone knew he was different. I first heard about him when he
was around 11 years old, and I know he was beating people up at the courts since at least that age.
It's also important to realize that Lebron had absolutely NO "puberty" stage of being uncoordinated,
or 'weak', or anything of the sort. Maverick (Lebron's cousin / business manager) used to say that
"he's the one to watch out for".. back when we were maybe 15-16 (Maverick was trying to steal my 
girlfriend, who went to St.V..... Mav if you are somehow reading this... what's good man?)...

..so that being said.. all I can say is.. athleticism and that 'god given' freak-of-nature, all-worldy
talent is just something you have to 'witness' to fully comprehend.
```

An exchange of comments between xdawsonic and other Redditors:

```markdown
you should absolutely puff your chest about it. Say "I blocked LeBron James multiple times" and have
it on your tombstone. It is a technically true statement.

I was moreso elaborating on the notion that age 13 was probably the year any "regular" guy had a 
punchers chance to compete with him.

Definitely "regular" in quotation marks. Division 3 offers and walk on status at D1 is pretty impressive,
and it was still a back and forth for you too(that's what I got from the post anyway).

As a 5-9 dude, I doubt I would be able to beat a 10 year old LeBron.
```

User ay21690 played Lebron in junior year, when he was on his way to the second of three consecutive Mr Ohio titles::

```markdown
We had the chance to play SVSM at a camp during the summer of his junior year. End of the camp is both
a single elimination tournament, then an expo where coaches picked a handful of the top players to run
a pick up game.

2nd round, we kept up with them for five minutes because zero defense was played. Now I'm 5'10", but I 
feel like I was pretty (keyword was) pretty quick. We decided to run a triangle and 2 against him on 
defense.

He literally laughed in our faces and scored every single time. I couldn't keep up with him. We lost by
42 points, that was only because he subbed out at halftime and Romeo Travis chucked 3s the rest of the game.
```

To which xdawsonic replied:

```markdown
I'm actually glad I got to play against 13-14 year old Lebron... and not THAT version. I actually think
he was scariest when he was Junior. He played incredibly hard. Senior year was a joke to him, I think.

I was in attendance for the Oak Hill vs. St. V game (Carmelo versus Lebron) that year... and I couldn't
fathom that I was watching the same guy.

Lebron had 36 that game, Carmelo had 34.
```

NintendoJesus adds perspective:

```markdown
Man, this brings me back. I played against 3 future NBA players in High School/College, Jason Gardner,
Wally Szczerbiak, and Melvin Levitt. 2 of them washed out of the NBA, and Wally wasn't exactly 1st team
NBA but I think he made an All Star game or 2 and had a good career. Gardner was the only one I played 
against in a real game, the rest were pick up games at Miami OH.

Unless you have played in a game vs a professional athlete, I don't think it's possible to understand 
how different they are from us normies. It's like they are a subspecies of human that have evolved into
something better. Like X-men or something. They don't play the same game as us. It's like if you played
against 5 year olds in a driveway with a 5 foot Fisher Price goal. You can't double them, you can't trap
them, you may as well not even be there.

And like you, I was good, not just like "back in the day" good, but like legit good. I'm 6'4", played 
point, could handle it, could shoot, could throw it off the board and dunk in my sleep. To a random
passerby, I probably looked like I could play. But no, I was an insect, Wally once scored 11 straight
on me 2 games in a row.(We played to 11, 1's and 2's) He was so strong that he could just pivot and 
shrug his shoulders and create enough space to do anything he wanted. It was like playing against your
dad when you're 5 years old.

They indirectly ended my basketball career. I knew I could never be what they were. No matter what I did.
And 2 of these guys didn't even make it in the NBA. I can't imagine playing against Lebron. He's from 
another planet.
```

Massive14:

```markdown
I went to watch him play a state tournament game in person during I believe LeBron's junior year. It
was a regional game at The University of Toledo. What stuck out the most was his extra gear. It always
felt like all the other kids were trying their hardest, but he was such a good athlete that he was 
walking when everyone else was running, and he was still dominating.

When he went into that extra gear though, my goodness. It was like watching a grown man against babies.
There's only a handful of athletes that I've ever seen in person that could flip the switch like that 
(Michael Bradley and Sidney Crosby to name a couple, but not as extreme).
```

<a name="#ed-witten"></a>
### Ed Witten
([overview](#overview))

Ed Witten is famous for having a great turbine of a mind.

He kick-started the second superstring revolution essentially single-handedly, has the highest h-index of any living physicist (that said, h-index is obviously flawed – see e.g. [here](http://en.wikipedia.org/wiki/H-index#Criticism) and [here](http://simplystatistics.org/2012/10/10/whats-wrong-with-the-predicting-h-index-paper/) and [here](http://blogs.scientificamerican.com/information-culture/2013/01/01/whats-wrong-with-citation-analysis/), although see [here](http://www.pnas.org/content/104/49/19193.abstract) for an argument that it still has better predictive power of future scientific achievement than other indicators considered), is the only physicist to ever win the Fields Medal, etc. Here are quotes about him I can quasi-procedurally recall (don’t remember them specifically, but I remember how to Google for them) right off the bat – and I haven’t even been reading about him for a while. Regardless of whether or not you agree, they’re definitely entertaining.

Mathwonk, in a physicsforum thread:

```markdown
Obviously Witten is a little smarter than the average bear.
```

John Randolph Huffman Professor of Physics at Yale [writing](http://www.eoht.info/thread/5126785/Edward+Witten?t=anon) of his experience of working with Witten:

```markdown
[O]ne day Ed Witten said to me, ‘I just learnt a new way to find exact S-matrices in two dimensions 
invented by Zamolodchikov and I want to extend the ideas to supersymmetric models. You are the 
S-matrix expert, aren’t you? Why don’t we work together?’ I was delighted. All my years of training
in Berkeley gave me a tremendous advantage over Ed—for an entire week.
```

Peter Baida, a close friend of Witten back in Park School (his high school):

```markdown
I suppose it’s common that kids in any high school sit around talking about who the smartest person
in the class is. But we used to sit around — when Edward wasn’t there — and talk about how he was the
smartest person in the world.
```

Michio Kaku (whose bio is really cool) in an [article](http://edition.cnn.com/2005/TECH/science/06/27/witten.physics/):

```markdown
I do believe there really is a category for a genius who is a supernova — a supernova that lights up 
the entire scientific landscape and that is Ed Witten…. I think he is as close as you are going to get
to a living Albert Einstein today.
```

Brian Greene:

```markdown
Everything I’ve ever worked on, if I trace its intellectual roots, I find they end at Witten’s feet.
```

“Max Raker”, commenting on the admittedly rather inane question “who’s the greater living genius, Terence Tao or Ed Witten?” in a [discussion thread](http://forumserver.twoplustwo.com/47/science-math-philosophy/greater-living-genius-terence-tao-ed-witten-663604/):

```markdown
So unlike most other people at his level, Witten didn’t really have scientific interests as an 
undergrad. The first time in which he officially had anything related to math/physics on his resume 
was when he started as a grad student at Princeton. I’m not even sure exactly how he got in, but i 
think within a few months he was working for David Gross. Gross was at this time likely the best physicist
in the world. Just before Witten joined his group, he had just finished some very important work on 
asymptotic freedom SU(3) and the strong interaction. This was work that he and his student, Frank Wilczek
later received a Nobel prize for.

So Gross was at the top of his game and also had good experience will gifted and Nobel prize worthy students.
Witten had a BS in History and had worked in politics. There are many stories you hear around Princeton 
about how frustrating Witten was to mentor. David would come up with a problem that he thought would take
anybody a few weeks to solve and require a ton of calculations, which a person sort of needs to go through 
to be educated as a theoretical physicist. Witten would instead come back in a day or 2 with a one page proof
that required no calculations and was based on some deep symmetry or other hidden but mathematically
sophisticated technique.

You can talk to some profs who were at Princeton at the time and as a school its produced its share of 
talent (including Tao) but I don’t think the faculty has ever been blown away like they were with Witten.
He must have had one of the quickest journeys from “I want to be a physicist” to “I am the most important
person in the field” in history.
```

Timothy Ferriss, in his book *The Whole Shebang*:

```markdown
In the high carrels of theoretical physics, where intelligence is taken for granted, Witten is regarded 
as preternaturally, almost forbiddingly, smart. A tall, boyish-looking man, he wears the habitual small 
smile of the theoretician for whom sustained mathematical thinking has something like the emotional
qualities that mystics associate with meditation. He speaks in a soft, high pitched voice, floating
short, precise sentences punctuated by witty little silences–the speech pattern of a man who has learned
that he thinks too fast to otherwise be understood. Though he is the son of a theoretical physicist,
Witten came to science in a roundabout fashion. He graduated from Brandeis College in 1971 as a history
major, wrote political journalism for the Nation and the New Republic, and worked in George McGovern’s
presidential campaign. Primarily a mathematician, he picked up physics along the way, almost as a hobby.
```

John H. Schwarz in an [article](http://discovermagazine.com/2008/dec/13-the-man-who-led-the-second-superstring-revolution):

```markdown
Witten is both deep and fast: After thinking through the ideas, he can compose an essentially error-free
100-page manuscript, often describing breakthrough original research, on his computer in a day.

In 1987 Michael Green and I coauthored a monograph entitled “Superstring Theory’’ with Witten. We were 
thrilled that Witten agreed to join us, since we knew that his contributions would greatly improve the 
final product. This work, consisting of more than 1,000 pages packed with equations, was completed in
nine months. For Green and me this required dedicating 100-hour workweeks to the project. Witten, on 
the other hand, was able to do his share while completing several major research projects at the same time.
```

Nathan Seiberg:

```markdown
I think in perspective of a hundred years or three hundred years, his name will stay…. It will not be
forgotten — his contributions are really lasting — contributions which will stay there.

He combines the rigor and precision of a mathematician with intuition of a physicist. But what is
really remarkable about him is the clarity of his thinking.

He shows the direction for the rest of us…. His main strength is that he’s powerful in everything. Both
in math — the most sophisticated math — and physics … he has remarkable physics intuition as well as
complete control over the math that is needed. And, in that respect, I think he’s unique.
```

Michael Atiyah:

```markdown
Although he is definitely a physicist (as his list of publications clearly shows) his command of
mathematics is rivalled by few mathematicians, and his ability to interpret physical ideas in 
mathematical form is quite unique. Time and again he has surprised the mathematical community by his
brilliant application of physical insight leading to new and deep mathematical theorems.
```

Atiyah says a bit more about how it was like to collaborate with Witten:

```markdown
I met him in Boston in 1977, when I was getting interested in the connection between physics and
mathematics. I attended a meeting, and there was this young chap with the older guys. We started
talking, and after a few minutes I realized that the younger guy was much smarter than the old guys.
He understood all the mathematics I was talking about, so I started paying attention to him. That 
was Witten. And I’ve kept in touch with him ever since.

In 2001, he invited me to Caltech, where he was a visiting professor. I felt like a graduate student
again. Every morning I would walk into the department, I’d go to see Witten, and we’d talk for an 
hour or so. He’d give me my homework. I’d go away and spend the next 23 hours trying to catch up.
Meanwhile, he’d go off and do half a dozen other things. We had a very intense collaboration. It was
an incredible experience because it was like working with a brilliant supervisor. I mean, he knew all
the answers before I got them. If we ever argued, he was right and I was wrong. It was embarrassing!
```

Keith Devlin on why Witten turning 50 is “worth celebrating”, paraphrased:

```markdown
Witten’s work in manifold theory brings up yet another comparison with Newton. Neither of them were
concerned with finding mathematically correct proofs to support their arguments. Relying on their 
intuitions and their immense ability to juggle complicated mathematical formulas, they both left
mathematicians reeling in their wake. It took over two hundred years for mathematicians to develop 
a mathematically sound theory to explain and support Newton’s method of the infinitesimal calculus.
Similarly, it might take decades — maybe even centuries — before mathematicians can catch up with Witten.

For most mathematicians, myself included, the only way to convince ourselves that something is true
in mathematics is to find a proof. A very small number of individuals, however, seem to be blessed 
with such deep and powerful insight that, guided by little else besides their intuitions and a sense
of “what is right”, they can cut through the logical thickets and discover the truth directly — 
whatever that means. Newton did it with calculus. The great Swiss mathematician Leonhard Euler did 
much the same thing with infinite sums in the eighteenth century. Arguably the Indian mathematician
Srinivasa Ramanujan did something similar with the arithmetical patterns of numbers he discovered. 
And now Witten is doing the same with infinite dimensional manifolds. On several occasions, Witten
has made a discovery — a physicist’s discovery since it is technically not a mathematical discovery 
— that mathematicians subsequently showed to be “correct” by the traditional means of formulating a
rigorous proof. Given the complexity of the “insights” that Newton, Euler, Ramanujan, and Witten have
made — and the difficulty of the subsequent proofs — this cannot be a case of making lucky guesses.
```

David Kahana, in [an answer to a question on Quora](https://www.quora.com/How-does-Edward-Witten-know-so-much-math):

```markdown
In my opinion, Edward Witten had both an incredible working memory and an incredible long term memory
... it seemed that he already knew the solution of almost any solvable problem that was ever posed to him.

I had a friend who became his graduate student ... and Witten suggested a problem for him to work on. 

The poor guy worked for months, but made no real progress. Then he finally went to talk with Witten, 
who gave him  the solution immediately and in a short, and very simple way.

Genius can have a very nasty side.
```

The only non-effusive comment about him I've ever seen is this one:

```markdown
I don't think Witten is as strong a mathematician as a professional mathematician. Other than a proof
of Positive Energy theorem, I don't think Witten rigorously proved any mathematical statements. He
used the advance mathematics in his work, but Witten's genius is in seeing the connections. He showed
you can see knot invariants in as physical quantities in QFTs. Witten, with others, came up with some 
topological/symplectic invariants and these invariants arose as physical quantities in QFTs. They was
never rigorously defined and their properties were never proven in Witten's work. Which inspired some
mathematicians like Kontsevich to do wonders. 

He probably knows more maths than a smart beginning grad student in mathematics.
```

Robert Weisbrot [talks about](http://scienceblogs.com/thescian/2010/10/25/what-did-ed-witten-do-in-colle/) how Witten ended up in physics:

```markdown
I liked Ed, but felt sorry for him, too, because, for all his potential, he lacked focus. He had
been a history major in college, and a linguistics minor. On graduating, though, he concluded that,
as rewarding as these fields had been, he was not really cut out to make a living at them. He
decided that what he was really meant to do was study economics. And so, he applied to graduate 
school, and was accepted at the University of Wisconsin. And, after only a semester, he dropped out 
of the program. Not for him. So, history was out; linguistics, out; economics, out. What to do? This
was a time of widespread political activism, and Ed became an aide to Senator George McGovern, then
running for the presidency on an anti-war platform. He also wrote articles for political journals 
like the Nation and the New Republic. After some months, Ed realized that politics was not for him,
because, in his words, it demanded qualities he did not have, foremost among them common sense. All
right, then: history, linguistics, economics, politics, were all out as career choices. What to do?
Ed suddenly realized that he was really suited to study mathematics. So he applied to graduate school, 
and was accepted at Princeton. I met him midway through his first year there–just after he had dropped
out of the mathematics department. He realized, he said, that what he was really meant to do was study
physics; he applied to the physics department, and was accepted.
```

Ron Maimon on Witten's rise to prominence:

```markdown
He knew exactly what he was doing, politically and also physics. He was the first physics to understand 
the political lessons of the 1960s (and also the mathematics). Witten had a physicist father, a General 
Relativist, and this is where he made his early major, major breakthroughs (the positive mass theorem 
the bubbles of nothing Schwarzschild instanton, the gravitational compactifications of supergravity), 
he certainly knew General Relativity and Quantum Mechanics since high school, perhaps earlier, maybe in
middle school (once you learn calculus, some differential equations it’s easy enough to learn the rest 
in high school), he studied on his own (like almost every other decent physicist), especially mathematics,
in his college years, so he chose a bullshit major he could do in his sleep, but one he was genuinely 
interested in, because he GOT the politics of the 1960s, and he knew it was going to replace the staid 
Soviet/Eisenhower/De-Gaulle bureaucratic politics of the 1950s. He also just studied all the math he 
could, probably wanting to do pure math.

He did a bunch of politics and linguistics, but McGovern lost, and that meant it was going to be terrible
in the US for a while longer (nobody guessed how long). Physics in 1972 was still very politically
stagnant— the field was split in two, S-matrix and field theory, everyone was stoned and nobody was 
calculating anything (in Feynman’s words, from that era). Also, it had an atom-bomb stench, maybe that
was a factor, maybe not. It certainly was for others back then, who refused to study physics because of
the association with atomic bomb work.

Then in 1974 or so, it becomes clear this young Dutch guy named ‘t Hooft has renormalized gauge theory, 
and ‘t Hooft is producing monumental results left and right, on anomalies, instantons, new gauges, Feynman
diagram summations, 2-d models, everything. When he sees this (and he saw it for sure) Witten immediately 
switches fields. He applies to a graduate program in applied mathematics where he knows David Gross is
active (he has a plan), goes over to David Gross and talks physics to him, at which point Gross arranges 
for him to switch departments, and Witten is considered a phenom, because he already knows everything 
(it was a stupid trick in the print era– if you just read the literature, and you look like a genius— he 
was also a genius though).

Then he gets a PhD under Gross (who is by this time very famous, Gross knows S-matrix theory, he studied 
under Chew, but he is also a major leader in field theory, after asymptotic freedom, plus he knows 
condensed matter and does 2d stuff too, all things Witten expertly absorbed). He gets a job on the East 
Coast, at Harvard, where S-matrix and strings are taboo. Then he collaborates with all the big shots, 
Venziano, Coleman, and so on. Coleman is impressed to no end, and starts studying gravitational things 
in this era. (Witten is also doing great solo work at this point, like the nonabelian bosonization), and
the endless schmoozing and obvious talent and knowledge pushes his h-index to the roof.

Then he goes to Princeton, where he switches and supports string theory, bringing every other marginalized
voice up from the gutter, and now he is ABSOLUTELY UNTOUCHABLE, politically, he is Albert Einstein. One 
cannot thank him enough for this, it was the most important political move in physics history, and that’s
not an exaggeration. He also does amazing work to push string theory forward from this point on, 
formulating open string field theory, finding realistic compactifications, and finding subtle physical
consequences of string theory which would give possible observational signatures. He also does amazing work
in all fields of high energy physics, and has pure mathematics breakthroughs.
```

Another Ronnism:

```markdown
Witten has contributed extremely significantly to mathematics, but so did Candlin with the Berezin 
integral, and so did Berezin with the proto-SUSY, and most significantly Pierre Ramond, who created
graded algebras. You can’t forget Ramond. And Belavin, Polyakov, Zamolodchikov and Knizhnik, who 
quadruple-handedly founded one of the most active fields of mathematics. Witten too, and Witten’s
contributions are closer to the center of what mathematicians find interesting, but you see how the
stupid politics gets in the way— he is praised so much, you can’t praise him as he deserves without
feeling you are neglecting other people.

The problem with mathematics is that the evaluation of quality is often by human judgement, not by
nature, and that can lead to politics too. That’s also somewhat true in physics, but physicists have
ways to get around that, by doing experiments, or by pretending to do certain experiments and then
breaking our head to come up with an answer as to what happens. The mathematicians have their own 
“though experiment” solution benchmarks, these are famous conjectures.

Witten solved many interesting mathematical problems of great depth, and also formulated and solved
many physical problems (one of my favorite works of his is the superconducting cosmic string paper 
from the mid 80s, another is a failed paper to try to solve the cosmological constant problem from 
the 90s, failed, but he gave it a major creative inspirational shot).

But stop trying to rank people on a line, it’s really stupid.
```

Ron on how Witten "knows so much math":

```markdown
Researchers are expected to learn the stuff they need, and be familiar with everything. Since this
is somewhat unrealistic, it just means you try to keep up all the time, and nobody feels they know
enough, you always feel like you're missing something.

Witten came before the internet, and one cannot overestimate how much more difficult it was to study
mathematics back then. When you opened a math book, if you didn't know the definitions, you couldn't
google them, you were just screwed. He studied mathematics (among other things) as an undergraduate,
I remember he was grateful for the proper mathematics education he recieved, but he continued to read
mathematics and internalize it also as a grad student and through his post doc and into his 
professorial career. He said he considered going into mathematics at one point, also perhaps
linguistics at some other point. He was a young lefty in the 1960s, there were a bunch of exciting 
things to do, but he eventually decided that physics was where he would make the biggest impact. He 
was up to date on the mathematics of the 1960s, that's extremely unusual for a pre-Witten physicist.
Now, it's expected, but it's expected mostly because of Witten!

All physicists know the elementary mathematics curriculum, that's no big trick. Most physicists have
mathematical competence in the areas related to their chosen field, to the level of a beginning
researcher. But what makes Witten special is his deep intuitive understanding of ALL fields of 
mathematics, especially deeply the 1960s topology stuff, that he clearly just learned for the heck of 
it, it wasn't before useful in physics, all the homology, homotopy theory and algebraic geometry 
constructions mathematicians were doing back then. It became central with string theory.

For example, one of his famous works in the 1990s was a note identifying the structure of brane anti-
brane annihilation as a type of K-theory, which is a Grothendieck construction which didn't have a 
physical intepretation before, nor is it something you would expect any physicist would know about.
Witten is the exception, because he knows the mathematics field as well as any mathematician, and he
just likes the material, he reads it, and rediscovers large chunks for himself.

While it is not polite to speculate about people who are alive (you can just ask them), I suspect he
learned a large chunk of advanced mathematics during his Harvard postdoc, in the late 1970s. Jaffe
and Coleman are influenced by him, probably the influence goes both ways, Coleman begins doing 
topological instantons, then he did the vacuum decay work, the false-vacuum instanton thing which was 
so influential for inflation theory. Witten is also associated with two enormous Harvard pure-
mathematics names, Bott and Yau. His mathematics had a Harvard feel to it, the 1985 Calabi Yau paper,
the Morse theory paper, these are popular Harvard topics.

He won a fields medal, and this is for a beautiful interpretation of the Jones knot polynomial from
large N 3d topological Chern-Simons gauge theory, a theory that he defined. In another related idea,
just a few years ago, he showed with his student or postdoc that the volume  conjecture (due to 
Thurston I think) is related to a property of these  topological theories under analytic continuation, 
and modulo the standard problem of precisely defining the path integral, they gave what should be a 
proof of the conjecture. He has a bunch of recent mathematical work I couldn't understand at all
related to pure algebraic geometry.

He also has a bunch of non-mathematical physics work too which are famous classics, like the
superconducting cosmic strings, the bubble of nothing instantons, the Witten anomaly, the Seiberg 
Witten theory and brane-stack constructions, the AdS/CFT constructions, a bunch more I probably forgot.
One thing that is not considered a classic is a 1992 or 1993 cone idea about supersymmetry breaking
that is very clever and simple idea for stabilizing the cosmological constant, but it probably doesn't
work (there seems to be a mistake, I don't remember what I thought it was), but boy is it inspirational.
it's really intimidating, as all his work is of extremely high quality, and everything is for sure 
required reading. It is a little difficult to follow, because it requires knowing earlier physical and
mathematical work, but it's as if it was made for the age of the internet, because now you can learn 
the associated material without being in a fancy place.
```

Ed Witten talking about himself sounds much more relatable. Consider these excerpts from [this interview for the AMS Notices](http://www.ams.org/notices/201505/rnoti-p491.pdf):

```markdown
....in those years, I knew about Khovanov homology and I was frustrated to not understand it, but I
had no idea it was related to geometric Langlands. I was frustrated at not understanding Khovanov 
homology, because I felt that my work on the Jones polynomial ought to be a good starting point for 
understanding Khovanov homology, but I just could not see how to proceed.

Eventually, however, some developments in the mathematical literature helped me understand that 
Khovanov homology should be understood using the same ingredients that are used to understand geometric
Langlands. I didn’t understand all of these clues, but I learned from two of them. .... I did not 
initially know what to make of those clues, but they were a sort of red flag hanging out there.
```

Or this:

```markdown
If one knows even a little bit about the Langlands correspondence and a little bit about conformal 
field theory on a Riemann surface, one can see an analogy between them. I wrote a paper that was
motivated by that, but then I realized that my understanding was too superficial to lead to
anything deep, so I abandoned the matter for a number of years.
```

Or this:

```markdown
The work of Beilinson and Drinfeld on geometric Langlands bothered me.... They were using familiar
ingredients of physics, but they were using them in ways that did not seem to fit. It looked like
somebody had taken a bunch of chess pieces, or perhaps here in Japan I should say a bunch of shogi
pieces, and placed them on the board at random. The way that the pieces were arranged did not make
any sense to me. That bothered me, but I could not do anything about it.
```

Or this:

```markdown
Although I understood scarcely anything of what Beilinson and Drinfeld were saying, I did put them
in touch with Hitchin’s work, and actually, in their very long, unpublished foundational paper on 
geometric Langlands that you can find on the Web, Beilinson and Drinfeld acknowledged me very
generously, far overestimating how much I had understood. All that had really happened was that 
based on a guess, I told them about Hitchin’s work, and then I think that made all kinds of things
obvious to them. Maybe they felt I knew some of those things, but I didn’t.
```

Or this:

```markdown
There were two long series of lectures and then there were a couple of outliers. The long series
were very well done, but they did not help me very much.

In addition, Ed Frenkel.... gave a series of lectures that, as far as I was concerned, were basically
about the shogi board on which the pieces have been arranged at random. I really couldn’t get much 
out of those lectures either, because I already knew that people working on the geometric Langlands
were taking familiar pieces from the shogi set and arranging them on the board at random as far as
I was concerned.
```

Or this:

```markdown
I personally concentrated on geometric Langlands rather than on number theory, and geometric Langlands
was hard enough..... For example, I did not understand what Hiraku Nakajima explained yesterday at 
the Kyoto Prize Workshop.
```

Or this:

```markdown
In some cases, I am still struggling to understand things that physicists did quite some time ago
that are very relevant. Just to give one example, the Gopakumar-Vafa and Ooguri-Vafa formulas 
have been very influential for algebraic geometers, but as a physicist, I was never satisfied
that I understood them. So I actually spent a lot of time in the last year with a student (Mykola
Dedushenko) trying to understand these formulas better. In this work, I was doing some of the 
homework that I’d have to understand before even trying to answer your question.
```

Or this, as a counterpoint to Seiberg's claim that he's "powerful in everything.... [including] the most sophisticated math":

```markdown
Usually producing rigorous proofs requires very detailed methods. That makes it hard for a 
physicist, and so I myself have only done that in very special cases where I thought something
was really missing that was actually simple enough that I could help do it if I had the right
collaborator.
```


<a name="#economics"></a>
## Economics
([overview](#overview))

<a name="#critique-of-freakonomics"></a>
### Critique of Freakonomics
([overview](#overview))

From Daniel Davies' [review](https://blog.danieldavies.com/2005_11_27_d-squareddigest_archive.html#113336769665072876). The chief problem is "the game of pretending that difficult social questions have easy non-sociological answers":

```markdown
There are a number of things I don't like about Levitt's approach, but the biggest problem I have with
him is his[2] habit of saying (in various forms of words) "whichever way you look at the numbers, XYZ" 
when he means "whichever way I look at the numbers, XYZ". On a lot of these subjects (by far the most 
obvious example is abortion/crime, but it is an issue in all of them to a greater or lesser degree), 
Levitt is looking at quite large, clearly multicausal issues where any model is likely to be partial and
all manner of conflicting theories can claim support from the data. "Freakonomics" absolutely does not
recognize this fundamental truth of econometrics; it might be because the authors don't have the 
statistical chops to understand it but I think it is much more likely that they are just trying to copy 
the monolithic tone of voice adopted by social reformers and similar blowhards who hand out their 
assertions with no data at all. In all honesty, I think that theL&D approach is a retrograde step; it's
easy for the untrained reader to spot when someone has no empirical support at all for his position, but
much more difficult to deal with someone who systematically overstates the empirical support that he does
have. This is at least 90% of what makes John Lott so pernicious, and it seems to me that L&D are involved
at least partly in the same sort of game.

It's the game of pretending that difficult social questions have easy non-sociological answers. There are 
lots of people in this space, and not all of it is by any means bad. Any look at a difficult question is
going to be either hopelessly oversimplified or hopelessly unreadable, and I would certainly prefer it if
people erred in the first direction. There's also a decent Hayekian (or indeed Bayesian) point to be made
here that if you're entering into the marketplace of ideas to try and extract the truth from a number of 
differing viewpoints, then you want everyone to give you their idea, not to caveat it all about with bits 
and pieces of other people's ideas. That's why I'm prepared to give a (limited) free pass to Malcolm Gladwell 
or James Surowiecki when they write books like this which, in my view, present absurdly oversimplified views
of the world, because I understand what they're trying to do; to present their view on a question, not to 
give the final indisputable answer.

The problem comes in when someone attempts to present their view of a question as if it is the final
indisputable answer. A lot of the things in Freakonomics are things that I wouldn't make too much of a fuss
about if the authors were just advancing them as their view of one way of explaining the facts. But they
don't do that; at key points in the book, they keep claiming that they're reporting the facts when they're 
clearly (to me at least) reporting a particular spin on the facts. This is the pop-science approach to social
questions, because it's trying to combine the authority of a scientific investigation with the unequivocal 
certainty of a theoretical pronouncement. What Levitt and Dubner are doing is exactly the same thing that 
Thomas Friedman does; telling a bunch of stories and then explaining how these stories fit into their view of 
the world. However, in the case of Friedman it's always obvious that someone else could tell entirely
different stories about the same kinds of people and events and fit them into an entirely different worldview.
Because of the way that Freakonomics has pitched itself at the pop-science crowd (constantly banging on about
Levitt's John Bates Clark medal and referring to all the statistical analyses; for fans of cringeworthy 
exegesis, page 161 of the American edition contains what I strongly believe to be the worst description of the 
linear regression model ever committed to print), however, they are always either implying or outright saying
that their stories are the only ones consistent with the facts, so we can either fit their stylized facts into 
our own worldview or (preferably) drop ours and buy theirs. As you can tell, I don't like this.
```

Example Freakonomism: "the typical prostitute earns more than the typical architect". Davies says:

```markdown
This remark is asinine. What on earth are they talking about? There is probably a reasonable working 
definition of a "typical architect" (though I can think of about five different types of architect off 
the top of my head), but what is a "typical prostitute"? Do they mean per hour or on an average annual
earnings basis? Is there any data to back this up (the only study I could find put average earnings for 
street prostitutes in Los Angeles, who are about as "typical" as any other prostitutes at $23485 in 1991,
which seems low for an architect)? 

Fair enough, this is really just a throwaway remark aimed at illustrating a point about labour market 
theory, but surely the whole freakonomicsing selling point of this book was meant to be that the authors
didn't make lazy assumptions and throwaway remarks but checked things against the data. I'm sorry, but if
a bloke says "of course, prostitutes make a mint, they do, they earn much more than you or I", then in my
estimation it is going to count very much against his subsequent claim to never take things on trust or 
to tirelessly question conventional wisdom.
```

Another example, concerning swimming pools and guns:

```markdown
As presented in the book, the argument is obviously wrong. Levitt divides the number of child deaths 
caused by guns by the number of guns, then divides the number of child deaths caused by swimming pools 
by the number of swimming pools, compares the two numbers and says "if you have a gun in your house and
a swimming pool, the pool is more likely to kill your child than the gun". 

Which might or might not be true, but this calculation can't possibly be the right way to prove it. Riddle
me this; what proportion of the guns in the USA are held in households with no children in them? What
proportion of the swimming pools in the US are owned by households with no children in them? Is there 
perhaps a pretty good reason to believe that households which differ in their gun ownership and 
swimming-pool ownership will also differ in other potentially relevant ways? Is there a good reason to 
believe that the fact that a house has a child in it will be informative about the relative likelihood of
gun ownership and pool ownership? Now, Levitt might, for all I know, have actually done the more rigorous
analytic work which would support his claim here. But if he did, I bet he did it in a proper journal where
he stated the claim with the proper caveats and was totally clear about the degree of confidence that could 
be placed in it. But that's not what he does in "Freakonomics". He just a) puts the factoid straight in 
front of the reader with no qualifications at all and b) backs it up with a calculation that is clearly
flat out wrong. He's simultaneously teaching the lay reader to make definitive statements without
acknowledging estimation problems, and to ignore correlations between explanatory variables. How on earth 
can this not be worsening the overall level of debate?
```

<a name="#rationality-and-postrationality"></a>
## Rationality and postrationality
([overview](#overview))

<a name="#expert-judgment"></a>
### Expert judgment
([overview](#overview))

Cosma Shalizi [compares clinical and actuarial judgment](http://bactra.org/notebooks/clinical-vs-actuarial.html), and finds something confusing; this also throws a wrench in my usual "experts suck at prediction" POV:

```markdown
For something like fifty years now, psychologists have been studying the question of 
"clinical versus actuarial judgment".

The idea goes like this. Say you're interested in diagnosing heart diseases from 
electrocardiograms. Normally we have clinicians, i.e., expert doctors, look at a chart and say
whether the patient has (to be definite) a heart condition requiring treatment within one year. 

Alternately, we could ask the experts what features they look at, when making their prognosis, 
and then fit a statistical model to that data, trying to predict the outcome or classification 
based on those features, which we can still have human experts evaluate. This is the actuarial
approach, since it's just based on averages --- "of patients with features x, y and z, q percent
have a serious heart condition".

The rather surprising, and completely consistent, result of these studies is that there are no 
known cases where clinicians reliably out-perform actuarial methods, even when the statistical 
models are just linear classification rules, i.e., about as simple a model as you can come up 
with. In many areas, statistical classifiers significantly out-perform human experts. They even 
out-perform experts who have access to the statistical results, apparently because the experts 
place too much weight on their own judgment, and not enough on the statistics. Whether you think
this is depressing news or not to some degree depends on your feelings about "clinical" experts.
So: human experts are really bad, or at least no better than simple statistical models.

On the other hand, there is *another* body of experimental work, admittedly more recent, on "simple
heuristics that make us smart", which seems to show that people are often *very good* judges, *under
natural conditions*. That is to say, we're very good at solving the problems we tend to actually 
encounter, presented in the way we encounter them. The heuristics we use to solve those problems
may not be *generally* applicable, but they are *adapted* to our environments, and, in those
environments, are fast, simple and effective.
```

How to reconcile this apparent contradiction? Cosma proposes three different ways; he doesn't know which is best:

```markdown
1. The "clinicial versus actuarial" results are wrong, or at least irrelevant. The experiments
do not reflect the "natural" conditions of clinical judgment. There are many possibilities here,
but the one which springs immediately to mind is that clinicians may not actually have much insight
into the way they *really* make decisions, and that the factors they think they attend to may not 
really be the ones that matter to them. What one really wants is a representative sample of actual
cases, comparing the normal judgment of clinicians to that of the statistical models. This may 
have been done; I don't know.

2. The "fast and frugal heuristics" results are wrong, or at least irrelevant. Whatever adaptive
mechanisms let us figure out good heuristics in everyday life don't apply in the situations where 
we rely on clinical expertise, or at least not in a lot of them. (See, for instance, the discussion
of projective tests like the Rorsharch ink-blots in Holland et al.'s Induction.) The problem can't
just be that we didn't evolve to make psychiatric diagnoses, since we didn't evolve to do most of
the diagnostic/prognostic tasks the fast-and-frugal-heuristics experiments show we can do, presumably
by expating the mechanisms that let our ancestors answer questions like "Just how angry will my
neighbors be if they catch me fishing in their stream?". There has to be something special about the
conditions of clinicial judgment that render our normal cognitive mechanisms ineffective there.

3. Clinicial judgment *is* a "fast and frugal heuristic", with emphasis on the fast and frugal. That is,
it is true that (e.g.) linear classifiers are more *accurate*, but the decision procedures clinicians 
are using may be as accurate as one can get, using only a reasonable amount of information and a
reasonable amount of time, while still using the human brain, which is not a computing platform 
well-suited to floating-point operations. The problem here is that there are areas where clinicians 
do seem to do as well as statistical methods.
```

<a name="#signaling"></a>
### Signaling
([overview](#overview))

<a name="#x-isnt-about-y"></a>
### X isn't about Y
([overview](#overview))

The founding document for this slogan is Robin Hanson's post [Politics isn't about policy](https://www.overcomingbias.com/2008/09/politics-isnt-a.html). 

Main thesis:

```markdown
Food isn’t about Nutrition
Clothes aren’t about Comfort
Bedrooms aren’t about Sleep
Marriage isn’t about Romance
Talk isn’t about Info
Laughter isn’t about Jokes
Charity isn’t about Helping
Church isn’t about God
Art isn’t about Insight
Medicine isn’t about Health
Consulting isn’t about Advice
School isn’t about Learning
Research isn’t about Progress
Politics isn’t about Policy

The above summarizes much of my contrarian world view.  (What else should go on this list?) 
When I say “X is not about Y,” I mean that while Y is the function commonly said to drive most 
X behavior, in fact some other function Z drives X behavior more.
```

Politics example from the post:

```markdown
High school students are easily engaged to elect class presidents, even though they have little 
idea what if any policies a class president might influence.  Instead such elections are usually
described as “popularity contests.”  That is, theses elections are about which school social 
factions are to have higher social status.  If a jock wins, jocks have higher status.  If your
girlfriend’s brother wins, you have higher status, etc.  And the fact that you have a vote says
that others should take you into account when forming coalitions – you are somebody.


Civics teachers talk as if politics is about policy, that politics is our system for choosing 
policies to deal with common problems.  But as Tyler Cowen suggests, real politics seems to be 
more about who will be our leaders, and what coalitions will rise or fall in status as a result.  
Election media coverage focuses on characterizing the candidates themselves – their personalities,
styles, friends, beliefs, etc.  You might say this is because character is a cheap clue to the
policies candidates would adopt, but I don’t buy it.

The obvious interpretation seems more believable – as with high school class presidents, we care
about policies mainly as clues to candidate character and affiliations.  And to the extent we
consider policies not tied to particular candidates, we mainly care about how policies will effect
which kinds of people will be respected how much.

For example, we want nationalized medicine so poor sick folks will feel cared for, military actions
so foreigners will treat us with respect, business deregulation as a sign of respect for hardworking
businessfolk, official gay marriage as a sign we accept gays, and so on.

This perspective explains why voters tend to prefer proportional representation, why many refuse to
vote for any candidate when none have earned their respect, and why so few are interested in
institutional reforms that would plausibly give more informed policies.
```

<a name="#countersignaling"></a>
### Countersignaling
([overview](#overview))

My go-to reference for countersignaling is Scott Alexander's [Intellectual Hipsters and Meta-Contrarianism](https://www.lesswrong.com/posts/9kcTNWopvXFncXgPy/intellectual-hipsters-and-meta-contrarianism). Key quote from the introduction:

```markdown
In certain situations refusing to signal can be a sign of high status. Thorstein Veblen invented
the term "conspicuous consumption" to refer to the showy spending habits of the nouveau riche,
who unlike the established money of his day took great pains to signal their wealth by buying fast
cars, expensive clothes, and shiny jewelery. Why was such flashiness common among new money but not
old? Because the old money was so secure in their position that it never even occurred to them that
they might be confused with poor people, whereas new money, with their lack of aristocratic breeding, 
worried they might be mistaken for poor people if they didn't make it blatantly obvious that they had
expensive things.

The old money might have started off not buying flashy things for pragmatic reasons - they didn't need
to, so why waste the money? But if F. Scott Fitzgerald is to be believed, the old money actively
cultivated an air of superiority to the nouveau riche and their conspicuous consumption; not buying 
flashy objects becomes a matter of principle. This makes sense: the nouveau riche need to differentiate 
themselves from the poor, but the old money need to differentiate themselves from the nouveau riche.

This process is called countersignaling, and one can find its telltale patterns in many walks of life.
Those who study human romantic attraction warn men not to "come on too strong", and this has similarities
to the nouveau riche example. A total loser might come up to a woman without a hint of romance, promise
her nothing, and demand sex. A more sophisticated man might buy roses for a woman, write her love poetry,
hover on her every wish, et cetera; this signifies that he is not a total loser. But the most desirable 
men may deliberately avoid doing nice things for women in an attempt to signal they are so high status 
that they don't need to. The average man tries to differentiate himself from the total loser by being nice;
the extremely attractive man tries to differentiate himself from the average man by not being especially 
nice.

In all three examples, people at the top of the pyramid end up displaying characteristics similar to 
those at the bottom. Hipsters deliberately wear the same clothes uncool people wear. Families with old
money don't wear much more jewelry than the middle class. And very attractive men approach women with 
the same lack of subtlety a total loser would use.
```

And here's metacontrarians as "intellectual hipsters":

```markdown
A person who is somewhat upper-class will conspicuously signal eir wealth by buying difficult-to-obtain
goods. A person who is very upper-class will conspicuously signal that ey feels no need to conspicuously 
signal eir wealth, by deliberately not buying difficult-to-obtain goods. 

A person who is somewhat intelligent will conspicuously signal eir intelligence by holding difficult-to-
understand opinions. A person who is very intelligent will conspicuously signal that ey feels no need to
conspicuously signal eir intelligence, by deliberately not holding difficult-to-understand opinions.

...just as contrarians risk becoming too contrary, moving from "actually, death has a few side benefits" to 
"DEATH IS GREAT!", meta-contrarians are at risk of becoming too meta-contrary.

All the possible examples here are controversial, so I will just take the least controversial one I can
think of and beg forgiveness. A naive person might think that industrial production is an absolute good
thing. Someone smarter than that naive person might realize that global warming is a strong negative to 
industrial production and desperately needs to be stopped. Someone even smarter than that, to differentiate
emself from the second person, might decide global warming wasn't such a big deal after all, or doesn't 
exist, or isn't man-made.

In this case, the contrarian position happened to be right (well, maybe), and the third person's meta-
contrariness took em further from the truth. I do feel like there are more global warming skeptics among 
what Eliezer called "the atheist/libertarian/technophile/sf-fan/early-adopter/programmer empirical cluster
in personspace" than among, say, college professors.

In fact, very often, the uneducated position of the five year old child may be deeply flawed and the 
contrarian position a necessary correction to those flaws. This makes meta-contrarianism a very dangerous
business. 

Remember, most everyone hates hipsters.

Without meaning to imply anything about whether or not any of these positions are correct or not3, the
following triads come to mind as connected to an uneducated/contrarian/meta-contrarian divide:

- KKK-style racist / politically correct liberal / "but there are scientifically proven genetic differences"
- misogyny / women's rights movement / men's rights movement
- conservative / liberal / libertarian4
- herbal-spiritual-alternative medicine / conventional medicine / Robin Hanson
- don't care about Africa / give aid to Africa / don't give aid to Africa
- Obama is Muslim / Obama is obviously not Muslim, you idiot / Patri Friedman5

What is interesting about these triads is not that people hold the positions (which could be expected by 
chance) but that people get deep personal satisfaction from arguing the positions even when their arguments
are unlikely to change policy6 - and that people identify with these positions to the point where arguments
about them can become personal.

If meta-contrarianism is a real tendency in over-intelligent people, it doesn't mean they should immediately
abandon their beliefs; that would just be meta-meta-contrarianism. It means that they need to recognize the
meta-contrarian tendency within themselves and so be extra suspicious and careful about a desire to believe 
something contrary to the prevailing contrarian wisdom, especially if they really enjoy doing so.

One more time: the fact that those beliefs are in an order does not mean some of them are good and others 
are bad. For example, "5 year old child / pro-death / transhumanist" is a triad, and "warming denier /
warming believer / warming skeptic" is a triad, but I personally support 1+3 in the first triad and 2 in 
the second. You can't evaluate the truth of a statement by its position in a signaling game; otherwise you 
could use human psychology to figure out if global warming is real!
```

Commenter Emile cautions that "going meta" can be dangerous:

```markdown
It's possible to go meta on nearly any issue, and there are a lot of meta-level arguments - group 
affiliation, signaling, rationalization, ulterior motives, whether a position is contrarian or supported
by the majority, who the experts are and how much we should trust them, which group is persecuted the most,
straw man positions and whether anybody really holds them, slippery slopes, different ways to interpret
statements, who is working under which cognitive bias ...

Which is why I prefer discussions to stick to the object level rather than go meta. It's just too easy to
rationalize a position in meta, and to find convincing-sounding arguments as to why the other side mistakenly
disagrees with you. And meta-level disagreements are more likely to persist in the long run, because they are
hard to verify.

Sure, meta-level arguments are very valuable in many cases, we shouldn't drop them altogether. But we should
be very cautious while using them.
```

<a name="#Memory-and-the-brain"></a>
## Memory and the brain
([overview](#overview))

<a name="#wisdom"></a>
### Wisdom
([overview](#overview))

Scott Alexander on wisdom in [Does age bring wisdom?](https://slatestarcodex.com/2017/11/07/does-age-bring-wisdom/) resonated pretty strongly with me:

```markdown
We’ve been talking recently about the high-level frames and heuristics that organize other 
concepts. They’re hard to transmit, and you have to rediscover them on your own, sometimes
with the help of lots of different explanations and viewpoints (or one very good one). 
They’re not obviously apparent when you’re missing them; if you’re not ready for them, they
just sound like platitudes and boring things you’ve already internalized.

Wisdom seems like the accumulation of those, or changes in higher-level heuristics you get
once you’ve had enough of those. I look back on myself now vs. ten years ago and notice
I’ve become more cynical, more mellow, and more prone to believing things are complicated. 
For example:

1. Less excitement about radical utopian plans to fix everything in society at once
2. Less belief that I’m special and can change the world
3. Less trust in any specific system, more resignation to the idea that anything useful
requires a grab bag of intuitions, heuristics, and almost-unteachable skills.
4. More willingness to assume that other people are competent in aggregate in certain
ways, eg that academic fields aren’t making incredibly stupid mistakes or pointlessly
circlejerking in ways I can easily detect.
5. More willingness to believe that power (as in “power structures” or “speak truth to
power”) matters and infects everything.
6. More belief in Chesterton’s Fence.
7. More concern that I’m wrong about everything, even the things I’m right about, on 
the grounds that I’m missing important other paradigms that think about things completely 
differently.
8. Less hope that everyone would just get along if they understood each other a little
better.
9. Less hope that anybody cares about truth (even though ten years ago I would have
admitted that nobody cares about truth).

All these seem like convincing insights. But most of them are in the direction of elite 
opinion. There’s an innocent explanation for this: intellectual elites are pretty wise, 
so as I grow wiser I converge to their position. But the non-innocent explanation is that 
I’m not getting wiser, I’m just getting *better socialized*. Maybe in medieval Europe, the 
older I grew, the more I would realize that the Pope was right about everything.
```

A particular example:

```markdown
I’m pretty embarassed by Parable On Obsolete Ideologies, which I wrote eight years ago. 
It’s not just that it’s badly written, or that it uses an ill-advised Nazi analogy. It’s
that it’s an impassioned plea to jettison everything about religion immediately, because
institutions don’t matter and only raw truth-seeking is important. If I imagine myself 
entering that debate today, I’d be more likely to take the opposite side. But when I read 
Parable, there’s…nothing really wrong with it. It’s a good argument for what it argues for.
I don’t have much to say against it. Ask me what changed my mind, and I’ll shrug, tell you
that I guess my priorities shifted. But I can’t help noticing that eight years ago, New 
Atheism was really popular, and now it’s really unpopular. Or that eight years ago I was in
a place where having Richard Dawkins style hyperrationalism was a useful brand, and now I’m
(for some reason) in a place where having James C. Scott style intellectual conservativism 
is a useful brand. A lot of the “wisdom” I’ve “gained” with age is the kind of wisdom that
helps me channel James C. Scott instead of Richard Dawkins; how sure am I that this is the
right path?
```

This is the "money quote", the whole reason I started this subheading:

```markdown
Sometimes I can almost feel this happening. First I believe something is true, and say so.
Then I realize it’s considered low-status and cringeworthy. Then I make a principled decision 
to avoid saying it – or say it only in a very careful way – in order to protect my reputation 
and ability to participate in society. Then when other people say it, I start looking down on
them for being bad at public relations. Then I start looking down on them just for being low-
status or cringeworthy. Finally the idea of “low-status” and “bad and wrong” have merged so 
fully in my mind that the idea seems terrible and ridiculous to me, and I only remember it’s
true if I force myself to explicitly consider the question. And even then, it’s in a 
condescending way, where I feel like the people who say it’s true deserve low status for not
being smart enough to remember not to say it. This is endemic, and I try to quash it when I 
notice it, but I don’t know how many times it’s slipped my notice all the way to the point 
where I can no longer remember the truth of the original statement.
```

Are old people really wiser? Why do they sound so crankily conservative? A model:

```markdown
And if I accept my intellectual changes as “gaining wisdom”, shouldn’t I also believe that 
old people are wiser than I am? And old people mostly seem to go around being really 
conservative and saying that everything was better in the old days and the youth are corrupt
and Facebook is going to be the death of us. I could model this as two different processes –
a real wisdom-related process that ends exactly where I am now, plus a false rose-colored-
glasses-related process that ends with your crotchety great-uncle talking about how things 
have been going downhill since the war – but that’s a lot of special pleading. I remember 
when I was twenty, I thought the only reason adults were less utopian than I was, was
because of their hidebound rose-colored self-serving biases. Pretty big coincidence that I 
was wrong then, but I’m right about everyone older than me *now.*
```

John "Erisology" Nerst responds:

```markdown
I think there could be selection effect. Not all people get wiser as they age and many hit 
a ceiling at some time. Maybe those are the ones most likely to make their opinions heard 
(I mean, it’s hardly the case that the wisest are the loudest among the younger population
either). And the really wise ones stay silent because their wisdom has become impossible to
communicate?

It reminds me of the quote from Julian Barnes’ Staring at the Sun:

*Everything you wanted to say required a context. If you gave the full context, people
thought you a rambling old fool. If you didn’t give the context, people thought you a
laconic old fool.”*
```

Worst-case scenario -- wisdom as "NMDA reception function change with age":

```markdown
There’s one more possibility that bothers me even worse than the socialization or
traumatization theory. I’m going to use science-y sounding terms just as an example, but I 
don’t actually think it’s this in particular – we know that the genes for liberal-conservative 
differences are mostly NMDA receptors in the brain. And we know that NMDA receptor function 
changes with aging. It would be pretty awkward if everything we thought was “gaining wisdom
with age” was just “brain receptors consistently functioning differently with age”. If we
were to find that were true – and furthermore, that the young version was intact and the older 
version was just the result of some kind of decay or oxidation or something – could I trust 
those results? Intuitively, going back to earlier habits of mind would feel inherently 
regressive, like going back to drawing on the wall with crayons. But I don’t have any *proof.*
```

This is pithily captured in the quote (attribution unknown, phrasing by "Keith"):

```markdown
He who isn’t radical as a youth has no heart. 
And he who isn’t conservative as an adult has no brain.
```

What this looks like in science -- the "grand old academics" phenomenon:

```markdown
I’ve noticed a vaguely related trend in science:

you get a number of grand old academics, the kind of people who continue to hang out at the
institution long after they’re officially retired who are an absolute goldmine for various 
minutiae of their subject.

They’ve tried many approaches over the decades and can warn you about dead ends….

but they also often have an overabundance of cynicism.

Often they remember that approach X didn’t work, they may not remember the exact details as 
to why. their memory of the event gets pared down to “that’s a dead end”… and then at some 
point a new generation of grad students come along and eventually someone ignores the advice 
that X is a dead end and it turns out that in the 30 years that have passed the things that 
made X a dead end no longer apply. The sequencing methods can now read through long-repeats 
or the chemistry used for some step is improved or some background piece of knowledge has
been added to the field that now allows people to power through the former roadblock.

Is that wisdom? Knowing lots of dead ends can be useful and can save resources…but it can also
be maladaptive as the world changes around you.
```

Commenter Deej's response:

```markdown
we need to distinguish between individual people changing their views as they get older, 
and the centre grounds shifting as younger people are more liberal than their predecessors.
My feeling is that for economic issues people’s individual views probably do shift 
rightwards as they get older, but for social issues it’s seems likely that it’s the centre
ground that’s moving. Although for today’s more exterme identity politics left youths that
might change.

Third. I think it’s worth distinguising between types of people and how their views might 
change. People who are properly interested in politics, for example, are – I would exepct 
– much more likely top have big changes in their views, than those that aren’t. See ex-
trotskists now in the Tory party or at least Blairite in the UK. I expect that the people
interested in politics changes are likely to be relatively more driven by learning from 
experience and reflective thought than people just slowly change their views over time from,
for example, a bit left to a bit right of centre. Or left to a bit left less left, right to
a bit less right etc.
```

Somewhat relevant are these quotes from Robin Hanson's *Age of Em*:

```markdown
Controlling for birth cohort, individual productivity does not peak until at least age 60,
and may never peak (Cardoso et al. 2011; Göbel and Zwick 2012). […] Also, any falling
productivity after age 60 for humans today may be primarily caused by declining physical
abilities, not declining mental abilities

Today, our abilities at different kinds of tasks peak at different ages. For example, raw 
cognitive processing peaks in late teens, learning and remembering names in early 20s, 
short-term memory about age 30, face recognition in early 30s, social understanding about
age 50, and word knowledge above age 65 (Hartshorne and Germine 2015).
```

<a name="#Yegge-on-memory"></a>
### Yegge on memory
([overview](#overview))

From one of his more memorable posts, [Done and gets things smart](https://steve-yegge.blogspot.com/2008/06/done-and-gets-things-smart.html):

```markdown
So we all think we're smart for different reasons. Mine was memorization. Smart, eh? 
In reality I was just a giant, uncomprehending parrot. I got my first big nasty surprise
when I was in the Navy Nuclear Power School program in Orlando, Florida, and I was 
setting historical records for the highest scores on their exams. The courses and exams 
had been carefully designed over some 30 years to maximize and then test "literal 
retention" of the material. They gave you all the material in outline form, and made you
write it in your notebook, and your test answers were graded on edit-distance from the 
original notes. (I'm not making this up or exaggerating in the slightest.) They had set 
up the ultimate parrot game, and I happily accepted. I memorized the entire notebooks 
word-for-word, and aced their tests.

They treated me like some sort of movie star — that is, until the Radar final lab exam in
electronics school, in which we had to troubleshoot an actual working (well, technically,
not-working) radar system. I failed spectacularly: I'd arguably set another historical 
record, because I had no idea what to do. I just stood there hemming and hawing and pooing
myself for three hours. I hadn't understood a single thing I'd memorized. Hey man, I was 
just playing their game! But I lost. I mean, I still made it through just fine, but I lost
the celebrity privileges in a big way.

Having a good memory is a serious impediment to understanding. It lets you cheat your way
through life. I've never learned to read sheet music to anywhere near the level I can play
(for both guitar and piano.) I have large-ish repertoires and, at least for guitar, good 
technique from lots of lessons, but since I could memorize the sheet music in one sitting,
I never learned how to read it faster than about a measure a minute. (It's not a 
photographic memory - I have to work a little to commit it to memory. But it was a lot 
less work than learning to read the music.) And as a result, my repertoire is only a 
thousandth what it could be if I knew how to read.

My memory (and, you know, overall laziness) has made me musically illiterate.

But when you combine the Dunning-Kruger effect (which affects me just as much as it does 
you) with having one or two things I've been good at in the past, it's all too easy to 
fall into the trap of thinking of myself as "smart", even if I know better now. All you 
have to do, to be "smart", is have a little competency at something, anything at all, just
enough to be dangerous, and then the Dunning-Kruger Effect makes you think you're God's 
gift to that field, discipline, or what have you.
```

<a name="#augmenting-long-term-memory"></a>
### Augmenting long-term memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), which is an all-around great essay you should read in its entirety. One of its theses:

```markdown
Many people treat memory ambivalently or even disparagingly as a cognitive skill: for instance,
people often talk of “rote memory” as though it's inferior to more advanced kinds of understanding.
I'll argue against this point of view, and make a case that memory is central to problem solving 
and creativity.
```

To expand on his point:

```markdown
It's a mistake to underestimate the importance of memory. I used to believe such tropes about the 
low importance of memory. But I now believe memory is at the foundation of our cognition.

There are two main reasons for this change, one a personal experience, the other based on evidence
from cognitive science.

Let me begin with the personal experience.

Over the years, I've often helped people learn technical subjects such as quantum mechanics. Over 
time you come to see patterns in how people get stuck. One common pattern is that people think 
they're getting stuck on esoteric, complex issues. But when you dig down it turns out they're having 
a hard time with basic notation and terminology. It's difficult to understand quantum mechanics when
you're unclear about every third word or piece of notation! Every sentence is a struggle.

It's like they're trying to compose a beautiful sonnet in French, but only know 200 words of French.
They're frustrated, and think the trouble is the difficulty of finding a good theme, striking 
sentiments and images, and so on. But really the issue is that they have only 200 words with which to
compose.

My somewhat pious belief was that if people focused more on remembering the basics, and worried less 
about the “difficult” high-level issues, they'd find the high-level issues took care of themselves.

But while I held this as a strong conviction about other people, I never realized it also applied to
me. And I had no idea at all how strongly it applied to me. Using Anki to read papers in new fields 
disabused me of this illusion. I found it almost unsettling how much easier Anki made learning such 
subjects. I now believe memory of the basics is often the single largest barrier to understanding. If
you have a system such as Anki for overcoming that barrier, then you will find it much, much easier 
to read into new fields.

This experience of how much easier Anki made learning a new technical field greatly increased my 
visceral appreciation for the importance of memory.
```

<a name="#procedural-vs-declarative-memory"></a>
### Procedural vs declarative memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), talking about procedural vs declarative memory in the context of using Anki flashcards:

```markdown
There's a big difference between remembering a fact and mastering a process. For instance, while 
you might remember a Unix command when cued by an Anki question, that doesn't mean you'll recognize
an opportunity to use the command in the context of the command line, and be comfortable typing it 
out. And it's still another thing to find novel, creative ways of combining the commands you know, 
in order to solve challenging problems.

Put another way: to really internalize a process, it's not enough just to review Anki cards. You need
to carry out the process, in context. And you need to solve real problems with it.
```

From Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally):

```markdown
Propositional knowledge is being gradually supplanted by the procedural.  You need only know *how to find*
information, to be able to use it after a trivial delay.  This requires some snippet of propositional data
- to find a song lyric, you need a long enough string that you won't turn up 99% noise when you try to 
Google it! - but mostly, it's a skill, not a fact, that you need to act like you knew the fact.

It's not clear to me whether this means that we should be alarmed and seek to hone our factual memories...
or whether we should devote our attention to honing our Google-fu, as our minds gradually become
server-side operations.
```

<a name="#external-brain"></a>
### External brain 
([overview](#overview))

Aids to memory have been opposed for millennia. Here's Socrates, in Plato's Phaedrus, circa 370 BCE, bemoaning the deleterious effects of the new technology of "writing":

```markdown
If men learn this, it will implant forgetfulness in their souls; they will cease to exercise
memory because they rely on that which is written, calling things to remembrance no longer 
from within themselves, but by means of external marks. What you have discovered is a recipe 
not for memory, but for reminder. And it is no true wisdom that you offer your disciples, but
only its semblance, for by telling them of many things without teaching them you will make 
them seem to know much, while for the most part they know nothing, and as men filled, not with
wisdom, but with the conceit of wisdom, they will be a burden to their fellows.
```

Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally) and the comments are great. It's from 2009; a decade hence, they're more applicable than ever. Here's some neat quotes.

```markdown
How many of the things you "know" do you have memorized?

Do you remember how to spell all of those words you let the spellcheck catch?  Do you remember
what fraction of a teaspoon of salt goes into that one recipe, or would you look at the list of
ingredients to be sure?  Do you remember what kinds of plastic they recycle in your neighborhood,
or do you delegate that task to a list attached with a magnet to the fridge?

If I asked you what day of the month it is today, would you know, or would you look at your
watch/computer clock/the posting date of this post?

Before I lost my Palm Pilot, I called it my "external brain".  It didn't really fit the description; 
with no Internet access, it mostly held my contact list, class schedule, and grocery list.  And a 
knockoff of Minesweeper.  Still, in a real enough sense, it remembered things for me.The vast arena 
of knowledge at our fingertips in the era of constant computing has, ironically, brought it farther
away.  It seems nearer: after all, now, if you are curious about Zanzibar, Wikipedia is a few 
keystrokes away.  Before the Internet, you'd probably have been looking at a trip to the library and
a while wrestling with the card catalog; and that would be if you lived in an affluent, literate society. 
If you didn't, good luck knowing Zanzibar exists in the first place!

But if you were an illiterate random peasant farmer in some historical venue, and you needed to know
the growing season of taro or barley or insert-your-favorite-staple-crop-here, Wikipedia would have
been superfluous: you would already know it.  It would be unlikely that you would find a song lyrics
website of any use, because all of the songs you'd care about would be ones you really knew, in the
sense of having heard them sung by real people who could clarify the words on request, as opposed to
the "I think I heard half of this on the radio at the dentist's office last month" sense.
```

Per Kaj Sotala, the distributed cognition paradigm of research is all about exploring the idea behind "externalizing" or "outsourcing" our brains to the environment. An excerpt from [this primer](http://www.isr.uci.edu/~jpd/classes/ics234bs03/13-HollanEtAl-TOCHI.pdf):

```markdown
In several environments we found subjects using space to simplify choice by creating arrangements 
that served as heuristic cues. For instance, we saw them covering things, such as garbage disposal 
units or hot handles, thereby hiding certain affordances or signaling a warning and so constraining
what would be seen as feasible. At other times they would highlight affordances by putting items 
needing immediate attention near to them, or creating piles that had to be dealt with. We saw them lay
down items for assembly in a way that was unambiguously encoding the order in which they were to be 
put together or handed off. That is, they were using space to encode ordering information and so were 
off-loading memory. These are just a few of the techniques we saw them use to make their dedecision
problems combinatorially less complex.

We also found subjects reorganizing their workspace to facilitate perception: to make it possible to 
notice properties or categories that were not noticed before, to make it easier to find relevant items,
to make it easier for the visual system to track items. One subject explained how his father taught him 
to place the various pieces of his dismantled bicycle, many of which were small, on a sheet of newspaper.
This made the small pieces easier to locate and less likely to be kicked about. In videos of cooking we 
found chefs distinguishing otherwise identical spoons by placing them beside key ingredients or on the 
lids of their respective saucepans, thereby using their positions to differentiate or mark them. We found
jigsaw puzzlers grouping similar pieces together, thereby exploiting the capacity of the visual system to
note finer differences between pieces when surrounded by similar pieces than when surrounded by different
pieces.

Finally, we found a host of ways that embodied agents enlist the world to perform computation for them. 
Familiar examples of such off-loading show up in analog computations. When the tallest spaghetti noodle
is singled out from its neighbors by striking the bundle on a table, a sort computation is performed by 
using the material and spatial properties of the world. But more prosaically we have found in laboratory 
studies of the computer game Tetris that players physically manipulate forms to save themselves 
computational effort [Kirsh 2001; Kirsh and Maglio 1995]. They modify the environment to cue recall, to 
speed up identification, and to generate mental images faster than they could if unaided. In short, they 
make changes to the world to save themselves costly and potentially error-prone computations.
```

Sotala takes the distributed cognition idea-seed and runs away with it:

```markdown
Information processing doesn't only happen inside brains and computers. The paradigm of distributed 
cognition studies human societies as information-processing systems, with individuals being parts of
the larger system. For instance, the operation of an airliner cockpit's crew has been studied from 
this perpective [1]. For a flight to proceed without trouble, the different crew members need to be 
aware of information relating to their areas of responsibility at any given moment. If the crew is 
experienced and well trained, they'll constantly stay up to date by e.g. simply listening to other 
crew members converse with flight control. As flight control informs the captain of a new flight 
altitude, the rest of the pilots begin to adjust the altitude even while the captain is still
finishing up the communication. The cockpit functions as a unified system, and relevant information 
is propagated to wherever needed. Several crew members hearing the same information also allows for
error correction. If the message is unclear and the captain can't make out flight control's words, 
he can ask the others for clarification. The co-pilot answers the captain's query: even though one 
part of the system has failed to absorb the information received from outside the system, the same 
information has been stored in another part, which may then attempt to re-send it where needed.

Several other fields have been studied in the same manner, ranging from a child's language learning 
[2] to creativity [3]. A child doesn't learn language by itself and in a vacuum, but via interaction
with adults and older children. Creativity, on the other hand, requires common, shared "idea resources"
which individuals may use to come up with their own inventions and then give them back for others to 
refine further. Another theory of innovation considers inventions to be responses to problems encountered
by the community. Things such as bad laws or ineffective ways of doing things show up in community, and 
are considered problems by its members. This leads the community - the system - into a need state,
mobilizing its members to seek solutions until they're found.

One central idea is that social communities are cognitive architectures the same way that individual
minds are [4]. The argument is as follows. Cognitive processes involve trajectories of information 
(transmission and transformation), so the patterns of these information trajectories, if stable,
reflect some underlying cognitive architecture. Since social organization - plus the structure added
by the context of activity - largely determines the way information flows through a group, social 
organization may itself be viewed as a form of cognitive architecture.

[1] Hutchins, E. & Klausen, T. (1995) Distributed Cognition in an Airline Cockpit.

[2] Spurrett, D. & Cowley, S.J. (2004) How to do things without words: infants, utterance-activity and
distributed cognition. Language Sciences, 6, 443-466.

[3] Miettinen, R. (2006) The Sources of Novelty: A Cultural and Systemic View of Distributed Creativity.
Creativity and Innovation Management. Vol. 15, no. 2.

[4] Hollan, J. & Hutchins, E. & Kirsh, D. (2000) Distributed Cognition: Toward a New Foundation for 
Human-Computer Interaction Research. ACM Transactions on Computer-Human Interaction. Vol 7, no. 2.
```

In other words, says Sotala, "probably nothing to be worried about. Just normal human use of the environment."

From Andy Clark's book *Supersizing the Mind*, a [comment by trent](https://nforum.ncatlab.org/discussion/1927/unpopularity-of-category-theory/) I found on the nForum showing how Feynman understood the notion of the external brain:

```markdown
When historian Charles Weiner found pages of Nobel Prize-winning physicist Richard Feynman’s
notes, he saw it as a “record” of Feynman’s work. Feynman himself, however, insisted that the
notes were not a record but the work itself. In Supersizing the Mind, Andy Clark argues that 
our thinking doesn’t happen only in our heads but that “certain forms of human cognizing include
inextricable tangles of feedback, feed-forward and feed-around loops: loops that promiscuously 
criss-cross the boundaries of brain, body and world.” The pen and paper of Feynman’s thought
are just such feedback loops, physical machinery that shape the flow of thought and enlarge the
boundaries of mind.
```

For context, this whole thread was about trying to figure out why category theory was unpopular. Trent adds:

```markdown
I see theory as helping one adopt elegant solutions like that, and, more generally I think that
the more physicists understand the role things seemingly outside of the mind such as notation
play in cognition, the more they will see the importance of work in mathematical physics which 
places physics in the most elegant possible notation. It’s not just theory addicts trying to 
justify their work when they say that it aids in problem solving, it’s how cognition works.
```

Cosma Shalizi is a great read for collective cognition. Here's the introduction he wrote to the [Collective Cognition:
Mathematical Foundations of Distributed Intelligence](http://csc.ucdavis.edu/~dynlearn/colcog/description.htm) workshop he co-organized awhile back, giving modern science and markets as examples:

```markdown
Many forms of individual cognition are enhanced by communication and collaboration with other 
intelligent agents. We propose to call this collective cognition, by analogy with the well known 
concept of collective action. People (and other intelligent agents) often "think better" in groups
and sometimes think in ways which would be simply impossible for isolated individuals. Perhaps the most
spectacular and important instance of collective cognition is modern science. An array of formal 
organizations and informal social institutions also can be considered means of collective cognition. For
instance, Hayek famously argued that competitive markets effectively calculate an adaptive allocation of
resources that could not be calculated by any individual market-participant. 

Hitherto the study of collective cognition has been qualitative, philosophical, even at times anecdotal.
Only recently, we believe, have the tools fallen into place to initiate a rigorous, quantitative science of
collective cognition. Moreover, it appears that soon there will be a real practical need for such a science.
```

A bit more on collective cognition:

```markdown
Collective cognition involves an interaction among three elements-the individual abilities of the agents,
their shared knowledge, and their communication structure. Cognitive collectives therefore resemble many
other complex systems which are collectives of goal-directed processes. Typically, the individual processes
know little of the detailed dynamics and the state of the overall system and, therefore, must use adaptive
techniques to achieve their goals. There are many naturally occurring examples, including human economies, 
human organizations, ecosystems, and even spin glasses. In addition, it has recently become clear that many
of the engineered systems of the future must be of this type, with massively distributed computational 
elements. There is optimism in the multi-agent system (MAS) field that widely applicable solutions to large,
distributed problems are close at hand. Some experts now believe that, in the information and
telecommunications networks of today, we have nascent examples of artificial cognitive collectives.
```

Collective cognition touches on a dazzling variety of fields: 

```markdown
Cognitive science
Situated agents
Emergent computation
Bounded rationality
Institutional economics
Economies of information
Evolutionary game theory
Cognitive ethology
Collective phenomena in physics
Neural computation and distributed representations
Distributed computation
Mechanism design
General equilibrium theory
Population biology
Robustness
Swarm intelligences
Reinforcement learning
Adaptive control
Cultural evolution
Cognitive sociology and the sociology of science
Telecommunications data routing
```

Some basic (i.e. foundational) questions of interest, at least for Cosma's workshop:

```markdown
Interaction between communication structure and cognitive performance.

How are knowledge dynamics and communication structure related? How are computational structure and 
communication structure related?

Why do some collectives not support much cognition and others support substandard or even pathological
forms?

When, and to what extent, can we attribute cognition, or at least computation, to the collective as 
such, rather than its individual members?

How much do we need to know about individual cognition to do adequate models of collective intelligence?

How can the capacity for collective cognition evolve?

Collective-action problems: Why should an agent contribute to the collective? Conversely, does the 
collective-cognition perspective shed any light on collective action in the conventional sense?

Incentive design: When we cannot directly control the goals individual agents, how can we still configure 
the system so that each agent has incentives to pursue a goal that is both readily achievable and good
for the overall collective?

How useful is fiat money in collectives whose agents are not human beings?

When and how should agents be induced to form teams?

When and how does agent heterogeneity enhance collective cognitive performance? Are "disagreement"
and "controversy" among agents always bad, or sometimes desirable for the collective?

How is robustness of behavior against external perturbation related to quality of behavior?
```

Cosma writes about collective cognition in a more fun-to-read way in his [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*. It begins like so:

```markdown
Human beings coordinate their actions to do things which would be hard or impossible for them 
individually. This is not a particularly recondite fact, and the recognition of it is ancient; 
it is in the fifth book of Lucretius's De Rerum Natura, for instance. It was a commonplace of 
the Enlightenment, that most sociable age, and the philosophes were even, it seems, the first to 
realize that thinking, too, can be a collective activity, one conducted and amplified by social
groups --- which is not to say that societies have thoughts. ...

The nineteenth century, and to a lesser degree this one, have witnessed a dramatic expansion in 
the numbers of us engaged in administration, bureaucracy, management, oversight --- that is to say,
in formally-organized tasks of collective cognition and control. We did not invent bureaucracy, 
the mainstay of the ancient empires, but we're much, much better at it than they were. A random 
American town of 200,000 --- Piffleburg, WI, let us say --- will have police, a rescue squad, a fire
department, a hospital, universal schooling, several large factories, insurance offices, banks, a 
community college, a public library with several thousand volumes at least, a post office, public 
utilities, political parties, garbage collection, paved and usable roads everywhere, mercantile 
connections stretching across the country, and, with some luck, unions. These are corrupt, 
inefficient institutions which work poorly; every election, Piffleburg's citizens mutter something 
like "what do we pay taxes for anyway?" Yet to run any one of these institutions at the level of 
honesty, efficiency and efficacy which makes Piffleburg grumble would have demanded the full powers
and attention of even the ablest Roman propraetor or T'ang magistrate. That all of those institutions,
plus the ones not restricted to a single city, could be run at once, and while governed by a very 
ordinary slice of common humanity, would have seemed to such officials flatly impossible.
```

So why are we, as Cosma puts it, "so much better at collective endeavors than the ancients"? This is fairly easy to address:

```markdown
To a first approximation, the answer is: brute force and massive literacy. We teach nearly everyone to
read and write, and to do it, by historical standards, at a high level. This lets us staff large
bureaucracies (by some estimates, over 40% of the US workforce does data-handling), which lets us run an
industrial economy (the trains run on time), which makes us rich enough to afford to educate everyone 
and keep them in bureaucratic employment, with some surplus left over to expand the system. 

 This would do us no good if our ideas of administration were as shabby as those of our ancestors in the 
 dark ages, but they're not: we inherited those of the ancient empires, and have had quite a while to 
 improve upon them (and improvements are made easier and faster by the large number of administrators and
 the high standard of literacy). Among the improvements are many techniques (standardized procedures,
 standardized parts, standardized credentials and jobs, explicit qualifications for jobs and goods, files,
 standardized categories) and devices (forms, punch cards, punch card tabulators, adding machines, card
 catalogs, and, recently, computers) for making the administration of people and things easier.
```

Now while this is all splendid, it's "in the realm of technique"; when it comes to theory, nobody has any real idea how to explain what's going on:

```markdown
We don't really have a good theory about how collective action and cognition work, when and why they do, 
how they can be made to work better, why they fail, what they can and cannot accomplish, and so forth. 

Intellectually, these are large, tempting problems; technologically, they have obvious relevance to the 
design of parallel and distributed computers; economically, they could mean real money, not just billions;
and, in general, it'd be nice to know what it is we've gotten ourselves into.
```

Enter Edwin Hutchins, who conducted fieldwork studying navigation on a US navy ship based in San Diego with the problem of theoretically grounding collective cognition in mind. His fieldwork is interesting:

```markdown
Hutchins's field evidence consists of very detailed records, taken in the early 1980s, on the performance 
of the navigation crew of a helicopter carrier ship he calls the Palau, principally as they fix their 
location and plot their course near shore. The way it worked, in those pre-GPS days, was, roughly, this: 
three land-marks on shore, of known location on the navigation charts, would be selected by the main
person in charge, the "quartermaster of the watch." Then they'd "take bearings" on these, i.e. find the
orientation of the line from the landmark to the ship. These lines would be drawn on the chart. Now, it's 
an elementary result in Euclidean geometry that any two lines meet at a single point (unless they're parallel);
three lines form a triangle (unless they all meet at the same point). Somewhere within that triangle is the 
ship: this fixes the current position. The position of the ship at the next fix is estimated by "dead
reckoning," which is simply taking the current position and heading of the ship, and its planned speed, and 
extrapolating forward along the line of its heading. A single person can do this, if he's not too rushed.
Close to shore, the Navy gets worried, and demands fixes every few minutes, so the task gets broken down: 
naval flunkies take the bearings, a different flunky tells them when to take the bearings, and so on. There's 
a fairly rigid protocol for coordinating all these actions, and for communicating their results in a usable
form, and specialized instruments for making the job easier.

So, what does all this actually show? Well, that cognitive tasks can get spread over several people; that, 
in this instance, those who do tasks which require input from other people are generally superior to them in
rank; that the official job descriptions do not quite correspond to what people do; that people have a hard 
time believing things which are strange to them, and tend to ask those who report them questions along the 
lines of "Are you sure?"; that, if you don't know what something looks like, a verbal description can be very
unhelpful; that the right tools can make the job simpler; and that building computation into tools can make 
the job simpler for people, since it's easier to use a slide-rule than take a sine or a logarithm in your head;
that, if you can't talk about something, it's hard to make plans with someone else about it. There's more, but 
they're along the same lines.

These are not exactly earth-shaking results; in fact, they're about what common sense says to us. This doesn't 
make it useless to check them, since common sense is so often wrong; but even then, Hutchins has checked them
against the performance of one task (navigation; more particularly, location fixing), in one set of social 
groups (a couple of ships of the US Navy) --- ones where the social system is designed, and has received several
centuries of re-design from people whose common sense more or less agrees with the above. (One wonders if they
did things differently aboard the Potemkin.)
```

Unfortunately his conclusions are rubbish. 

<a name="#cognitive-science"></a>
### Cognitive science
([overview](#overview))

From Cosma Shalizi's [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*:

```markdown
Cognition, whether human, animal or artificial, is a kind of information-processing, taking place, in our
case, in the brain. The information takes the form of representations (of sensory stimuli, of states of 
parts of the world, of facts, of relations, of possible states of parts of the world, of courses of action,
or what-not). The processing consists of the transformation of these representations according to definite,
though perhaps stochastic, rules. (So far, we have not excluded the connectionist heretics.) An immense amount
of information-processing takes place subconsciously, particularly that which turns raw irritation of the 
afferent nerves into useful perceptions of the world about us, and turns volitions into raw stimulations of
the efferent nerves. To recognize a dagger you see before you involves a lot of computational work; some people,
having been wounded in the parts of the brain which do the computations, cannot. At least at some level of 
abstraction, the representations and transformations are usefully, conveniently and/or accurately thought of as
structures of symbols and as algorithms, respectively. (This does rule out the connectionists.) The algorithms 
may be (or, if you like, instantiate) rules of inference, or rules for producing new representations from old 
ones more generally ("production systems"). 

One particularly well-studied kind of cognition, sometimes taken as the paradigm of all cognition, is
problem-solving, conceived of as turning a representation of the problem, step by step, into a representation of
a solution, or something close enough to a solution to satisfy the problem-solver. (Expertise in solving a kind 
of problem consists in knowing good algorithms to apply to it, being able to represent a problem in a way which
makes it easy to solve, and being able to recognize a solution when you have one.) In principle, all this takes 
place in the brain; in practice, we can fake a larger and more accurate memory than we possess by either using 
external symbols, or by taking advantage of regular and persistent parts of our environment.
```

<a name="#names-matter"></a>
### Names matter
([overview](#overview))

In Michael Nielsen's essay [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html) there's a section recounting a famous story in physics I've always taken to heart, the one by Dick Feynman dismissing the value of knowing the names of things:

```markdown
One kid (a know-it-all) says to me, “See that bird? What kind of bird is that?” 

I said, “I haven't the slightest idea what kind of a bird it is.” 

He says, “It'a brown-throated thrush. Your father doesn't teach you anything!” 

But it was the opposite. He (Feynman's father) had already taught me: “See that bird?” 
he says. “It's a Spencer's warbler.” (I knew he didn't know the real name.) “Well, in 
Italian, it's a Chutto Lapittida. In Portuguese, it's a Bom da Peida… You can know the name 
of that bird in all the languages of the world, but when you're finished, you'll know absolutely
nothing whatever about the bird! You'll only know about humans in different places, and what they
call the bird. So let's look at the bird and see what it's *doing* — that's what counts.” (I
learned very early the difference between knowing the name of something and knowing something.)
```

Roger Zelazny puts it far more poetically in his novel *Lords of Light* in one of the most powerful passages I've ever read:

```markdown
Sam sat with his eyes closed for several minutes, then said softly:

"I have many names, and none of them matter." He opened his eyes slightly then, but he did not 
move his head. He looked upon nothing in particular.

"Names are not important," he said. "To speak is to name names, but to speak is not important.

"A thing happens once that has never happened before. Seeing it, a man looks on reality. He cannot
tell others what he has seen. Others wish to know, however, so they question him saying, 'What is 
it like, this thing you have seen?'

"So he tries to tell them. Perhaps he has seen the very first fire in the world. He tells them, 
'It is red, like a poppy, but through it dance other colors. It has no form, like water, flowing 
everywhere. It is warm, like the sun of summer, only warmer. It exists for a time on a piece of wood,
and then the wood is gone, as though it were eaten, leaving behind that which is black and can be 
sifted like sand. When the wood is gone, it too is gone.'

"Therefore, the hearers must think reality is like a poppy, like water, like the sun, like that which
eats and excretes. They think it is like to anything that they are told it is like by the man who has 
known it. But they have not looked upon fire. They cannot really *know* it. They can only know *of* it.

"But fire comes again into the world, many times. More men look upon fire. After a time, fire is 
as common as grass and clouds and the air they breathe. They see that, while it is like a poppy, 
it is not a poppy, while it is like water, it is not water, while it is like the sun, it is not 
the sun, and while it is like that which eats and passes wastes, it is not that which eats and
passes wastes, but something different from each of these apart or all of these together. So they
look upon this new thing and they make a new word to call it. They call it 'fire.'

"If they come upon one who still has not seen it and they speak to him of fire, he does not know 
what they mean. So they, in turn, fall back upon telling him what fire is like. As they do, they 
know from their own experience that what they are telling him is not the truth, but only a part of it.
They know that this man will never know reality from their words, though all the words in the world 
are theirs to use. He must look upon the fire, smell of it, warm his hands by it, stare into its heart,
or remain forever ignorant.

"Therefore, 'fire' does not matter, 'earth' and 'air' and 'water' do not matter. 'I' do not matter.
No word matters.

"But man forgets reality and remembers words. The more words he remembers, the cleverer do his fellows
esteem him. He looks upon the great transformations of the world, but he does not see them as they were
seen when man looked upon reality for the first time. Their names come to his lips and he smiles as he 
tastes them, thinking he knows them in the naming. The thing that has never happened before is still 
happening. It is still a miracle. The great burning blossom squats, flowing, upon the limb of the world,
excreting the ash of the world, and being none of these things I have named and at the same time 
all of them, and *this* is reality—the Nameless.”
```

Nielsen pushes back against this attitude insofar as it goes too far:

```markdown
It's a good story. But it goes too far: names do matter. Maybe not as much as the know-it-all kid
thought, and they're not usually a deep kind of knowledge. But they're the foundation that allows 
you to build up a network of knowledge.

This trope that names don't matter was repeatedly drilled into me during my scientific training.
When I began using Anki, at first I felt somewhat silly putting questions about names for things 
into the system. But now I do it enthusiastically, knowing that it's an early step along the way 
to understanding.

Anki is useful for names of all kinds of things, but I find it particularly helpful for non-verbal
things. For instance, I put in questions about artworks, like: “What does the artist Emily Hare's 
painting Howl look like?”.

I put that question in for two reasons. The main reason is that I like to remember the experience of
the painting from time to time. And the other is to put a name to the painting.( Actually, a better
question for that is to be shown the painting and asked what its name is.) If I wanted to think more 
analytically about the painting – say, about the clever use of color gradients – I could add more
detailed questions. But I'm pretty happy just committing the experience of the image to memory.

Friends sometimes complain that many books are over-padded essays. Perhaps a benefit of such padding
is that it enforces an Anki-like spaced repetition, since readers take weeks to read the book. This
may be an inefficient way to memorize the main points, but is better than having no memory of the 
book at all.
```

<a name="#reading-and-writing"></a>
## Reading and writing
([overview](#overview))

<a name="#Reading-styles"></a>
### Reading styles
([overview](#overview))

Eugene Wei, who authors the excellent [Remains of the Day](https://www.eugenewei.com/) blog, reads asynchronously, crediting it with helping him form cross-domain connections and insights. From [this interview](https://radreads.co/eugene-wei-remains-of-the-day/):

```markdown
The way I read now is only possible because of the Kindle. 

I tend to read multiple non-fiction books in parallel. I only came to this later in life
and used to be one of those people who read one book cover-to-cover. 

Now I do this thing that resembles a modern social media feed of content from books, but
I’m creating it by just following my interests. I open the Kindle [where I have hundreds 
of books] and I pick the one that most interests me at the moment, and just tell myself to
read at least one full chapter. If it keeps my interest, I keep going for another chapter. 
But if not, I’ll jump to another book and read a full chapter. 

You can’t do this with fiction because you tend to lose the train of the plot and narrative,
but non-fiction, so little of it is so linearly linked that you can disaggregate it in this
way. 

The reason that I read this way and the advantage is that I start to see patterns across 
books in different topics. You may be reading one book on astronomy, business strategy, and
another on language or rhetoric. Then suddenly something pops up that connects between those.
It’s an advantage for me because it lets your interests pull you along rather than self-
imposing a deadline to finish a book. And if a book doesn’t interest you, you just abandon it
to save time and move to something more interesting.
```

<a name="#rambling"></a>
### Rambling
([overview](#overview))

From Robin Hanson's post [Better Babblers](http://www.overcomingbias.com/2017/03/better-babblers.html), which I found out about via Sarah Constantin's [Humans Who Are Not Concentrating Are Not General Intelligences](https://www.lesswrong.com/posts/4AHXDwcGab5PhKhHT/humans-who-are-not-concentrating-are-not-general) (about the implications of having GPT-2-level text around). Commenter Curt Adams notes that 'rambling' is a better word choice than 'babbling' for what Robin is talking about in his post, which is why I'm using it here. 

```markdown
You can think of knowing how to write as knowing how to correlate words. Given no words,
what first word should you write. Then given one word, what second word best correlates
with that. Then given two words, what third word best fits with those two. And so on. 
Thus your knowledge of how to write can be broken into what you know at these different
correlation orders: one word, two words, three words, and so on. Each time you pick a 
new word you can combine knowledge at these different orders, by weighing all their 
different recommendations for your next word.

This correlation order approach can also be applied at different scales. For example,
given some classification of your first sentence, what kind of second sentence should 
follow? Given a classification of your first chapter, what kind of second chapter should
follow? Many other kinds of knowledge can be similarly broken down into correlation 
orders, at different scales. We can do this for music, paintings, interior decoration,
computer programs, math theorems, and so on.

Given a huge database, such as of writings, it is easy to get good at very low orders;
you can just use the correlation frequencies found in your dataset. After that, simple 
statistical models applied to this database can give you good estimates for correlations
to use at somewhat higher orders. And if you have enough data (roughly ten million 
examples per category I’m told) then recently popular machine learning techniques can 
improve your estimates at a next set of higher orders.

...

After eighteen years of being a professor, I’ve graded *many* student essays. And while I 
usually try to teach a deep structure of concepts, what the median student actually learns
seems to mostly be a set of low order correlations. They know what words to use, which 
words tend to go together, which combinations tend to have positive associations, and so
on. But if you ask an exam question where the deep structure answer differs from answer 
you’d guess looking at low order correlations, most students usually give the wrong answer.

Simple correlations also seem sufficient to capture most polite conversation talk, such 
as the weather is nice, how is your mother’s illness, and damn that other political party.
Simple correlations are also most of what I see in inspirational TED talks, and when public
intellectuals and talk show guests pontificate on topics they really don’t understand, such
as quantum mechanics, consciousness, postmodernism, or the need always for more regulation
everywhere. After all, media entertainers don’t need to understand deep structures any 
better than do their audiences.

Let me call styles of talking (or music, etc.) that rely mostly on low order correlations 
“babbling”. Babbling isn’t meaningless, but to ignorant audiences it often appears to be 
based on a deeper understanding than is actually the case. When done well, babbling can be
entertaining, comforting, titillating, or exciting. It just isn’t usually a good place to
learn deep insight.
```

See also [rambling and AI](#rambling-and-ai). 

<a name="#writing-advice"></a>
### Writing advice
([overview](#overview))

<a name="#nonfiction"></a>
### Nonfiction
([overview](#overview))

Qiaochu Yuan comments on Luke Muehlhauser's post [Writing style and the typical mind fallacy](https://www.lesswrong.com/posts/7Q3MoE9YzFMxGZR7F/writing-style-and-the-typical-mind-fallacy):

```markdown
Rather than "abstract stuff first, examples later" or "examples first, abstract stuff later"
I prefer a hybrid approach: give as much of the abstract stuff as is necessary to motivate why
you're looking at the examples (which may be no abstract stuff at all), then give the examples,
then give the rest of the abstract stuff. The main application I have in mind is mathematical 
writing, where sometimes a definition is very hard to motivate without examples, but sometimes
the examples are very hard to motivate without looking at a previously well-understood abstraction
first. The problem I have with always giving examples first is that I often don't know what to do
with the examples if they haven't been properly introduced: where, in the filesystem of my brain,
should I be filing these things?
```

Qiaochu is a wonderful math expositor, one of the best in the business. See also [why most math writing sucks](#why-math-is-boring).

From Stephen King:

```markdown
I believe the road to hell is paved with adverbs and I will shout it from the rooftops. 
To put it another way, they're like dandelions. If you have one on your lawn it's pretty
and unique. If you fail to root it out, however, you find five the next day . . . fifty 
the day after that . . . and then, my brothers and sisters, your lawn is totally,
completely, and profligately covered with dandelions. By then you see them for the weeds 
they are, but by then it's--GASP!!--too late.

If you want to be a writer, then you must do two things above all others: read a lot and 
write a lot.

You must be prepared to do some serious turning inward toward the life of the imagination,
and that means, I'm afraid, that Geraldo, Keith Obermann, and Jay Leno must go. Reading 
takes time, and the glass teat takes too much of it.

Must you write complete sentences each time, every time? Perish the thought.

It's always easier to kill someone else's darlings than it is to kill your own.
```

From [Stein on Writing](http://www.amazon.com/Stein-Writing-Successful-Techniques-Strategies/dp/0312254210/), some good examples of nonfiction opening lines:

```markdown
When it comes to shopping for a computer, the most important peripheral runs at 98.6 degrees 
Fahrenheit and is known as a friend.

Here on a stony meadow in West Texas at the end of 10 miles of unpaved road through mesquite-
covered, coyote-infested shurb land, several hundred bearers of a strategic commodity of the 
United States of America are gathered.
They are goats.

As the 155-millimeter howitzer shells whistled down on this crumbling city today, exploding 
into buildings all around, a disheveled stubble-bearded man in formal evening attire unfolded
a plastic chair in the middle of Vase Miskina Street. He lifted his cello from its case and 
began playing Albinoni's Adagio.
```

Repetition: 

```markdown
Yesterday morning Henry Sorbino walked into the K-Mart on Eleventh Street carrying an umbrella 
and walked out carrying an umbrella and someone else's purse.
```

Eye for detail:

```markdown
Carl Gardhof, his head held high as if he had done nothing wrong, was sentenced in Superior Court
to eighteen months in jail this morning.
```

In a story about the suspension of auto union talks because workers were loath to chip in for health care costs:

```markdown
Since learning last year that he had multiple sclerosis, Andy Torok has become less and less steady
on his feet, and his worries have accumulated along with the hand prints on his apartment's white walls.
```

Obituaries and memorial pieces:

```markdown
Andy Warhol, draftsman of shoes, is dead, and the people viewing his remains are mostly wearing
scuffed white sneakers.

A year after his death, the recurring image I associate with Raymond Carver is one of people 
leaning toward him, working very hard at the act of listening.
```

Autobiographies:

```markdown
Many problems confront an autobiographer, and I am confident that I have not solved them.
```

Here's a comment by Scott Alexander on Luke Muehlhauser's LW post [Rhetoric for the good](https://www.lesswrong.com/posts/SiGY7aah56HvGXxBJ/rhetoric-for-the-good) which I lost years ago, and am delighted to stumble upon again:

```markdown
There's that quote about how "the most important thing is sincerity, and if you can fake that, 
you've got it made." So there are two equal and opposite commandments for popular writing. First,
you've got to sound like you're chatting with your reader, like you're giving them an unfiltered 
stream-of-consciousness access to your ideas as you think them. Second, *on no account should you
actually do that*.

Eliezer is one of the masters at this; his essays are littered with phrases like "y'know" and 
"pretty much", but they're way too tight to be hastily published first drafts (or maybe I'm wrong 
and Eliezer is one of the few people in the world who can do this; chances are you're not). You've 
got to put *a lot of work* into making something look that spontaneous. I'm a fan of words like "sorta"
and "kinda" myself, but I have literally gone through paragraphs and replaced all of the "to some 
degrees" with "sortas" to get the tone how I wanted it.

I like inserting myself and my thought processes into things I write. It's a no-no in serious writing,
but in informal writing it can emphasize the informality and become endearing, a sort of "we can take 
off the masks now, because we're all friends here". This only works if your personal asides are actually
endearing to people, or at least not actively boring and off-putting, but if you get it right it lets 
you keep more spontaneity, since talking in first person is a natural impulse. As in everything, "first
learn the rules and the reasons for them, then break them as much as you want".

The real meat of writing comes from an intuitive flow of words and ideas that surprises even yourself. 
Editing can only enhance and purify writing so far; it needs to have some natural potential to begin with.
My own process here is to mentally rehearse an idea very many times without even thinking about writing.
Once I'm an expert at explaining it to myself or an imaginary partner, then I transcribe the explanation
I settle upon (some people say they don't think in words; I predict writing will not come naturally to
these people). *Then* I edit the heck out of it.

The best way to improve the natural flow of ideas, and your writing in general, is to read *really good 
writers* so much that you unconsciously pick up their turns of phrase and don't even realize when you're
using them. The best time to do that is when you're eight years old; the second best time is now.

Your role models here should be those vampires who hunt down the talented, suck out their souls, and absorb
their powers. Which writers' souls you feast upon depends on your own natural style and your goals. I've
gained most from reading Eliezer, Mencius Moldbug, Aleister Crowley, and G.K. Chesterton; I'm currently making
my way through Chesterton's collected works pretty much with the sole aim of imprinting his writing style into 
my brain.

Stepping from the sublime to the ridiculous, I took a lot from reading Dave Barry when I was a child. He 
has a very observational sense of humor, the sort where instead of going out looking for jokes, he just
writes about a topic and it ends up funny. It's not hard to copy if you're familiar enough with it. And
if you can be funny, people will read you whether you have any other redeeming qualities or not.

Getting imprinted with good writers like this will serve you for your entire life. It will serve you 
whether you're on your fiftieth draft of a thesis paper, or you're rushing a Less Wrong comment in
the three minutes before you have to go to work. It will even serve you in regular old non-written
conversation, because wit and clarity are independent of medium.

And it will also inform and limit your use of all the other rules above. Luke's fourth point - 
telling stories about characters taking actions - is a good one, but he very reasonably didn't start
this post off with a story about some student working on a term paper. There have been a few LW posts
that kind of seemed kludgy and artificial in adding characters and stories, and others that did it 
really well. Probably some very smart person could figure out why it succeeds somewhere and fails 
somewhere else, but it's easier to just cultivate the virtue that is nameless.

Some people say to write down everything and only edit later. I take the opposite tack. I used to 
believe that I rarely edited at all because I usually publish something as soon as it's done. Then
a friend watching me write said that she was getting seasick from my tendency to go back and forth 
deleting and rewriting the same sentence fragment or paragraph before moving on. Most likely the 
best writers combine both editing methods.
```

Scott's original comments came with links to examples of what he considered particularly good writing: [Eliezer](http://lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/), [Mencius Moldbug](http://unqualified-reservations.blogspot.com/2007/04/formalist-manifesto-originally-posted.html), [Aleister Crowley](http://www.sacred-texts.com/oto/aba/aba1.htm), [G.K. Chesterton](http://www.ccel.org/ccel/chesterton/heretics.iv_1.html), [Dave Barry](http://www.miamiherald.com/2011/01/01/1992746/dave-barrys-2010-year-in-review.html). 

Scott's [Nonfiction writing advice](https://slatestarcodex.com/2016/02/20/writing-advice/) is specialized for argumentative topics, which isn't exactly my cup of tea, but it has gems. 

The one I keep returning to the most is the idea of *microhumor*:

```markdown
You’ve heard of microaggressions. Now try microhumor. It’s things that aren’t a *joke* in the 
laugh-out-loud told-by-a-comedian sense, but still put the tiniest ghost of a smile on your reader’s
face while they’re skimming through them.

I learned this art from Dave Barry and Scott Adams, both of whom are humor writers and use normal 
macrohumor, but both of whom pepper the spaces in between jokes with microhumor besides. Your best 
best is to read everything they’ve written, your second best bet is to listen to me fumblingly try 
to explain it.

Here’s a paragraph from my “about” page:

*Topics here tend to center vaguely around this meta-philosophical idea of how people evaluate 
arguments for their beliefs, and especially whether this process is spectacularly broken in a way
that may or may not doom us all.*

There are a couple of things here that might qualify as microhumor. Take “especially whether this
process is spectacularly broken in a way that may or may not doom us all”. It’s not really a *joke*.
If I were a comedian and recited that sentence, you wouldn’t start laughing. But it’s kind of funny
to be starting with what sounds like a pretty dry academic idea (“how people evaluate arguments for
their beliefs” and whether the process is broken), and then confound expectations with an exaggerated
(well, maybe) warning about it dooming us all. The phrase “may or may not doom us all” does the same
thing on a smaller scale: “may or may not” is a pretty reserved, careful sounding phrase, whereas
“doom us all” is obviously the opposite of reserved (I also like the similar construction “it might
have sort of kind of been the worst idea ever”).

You can actually go a long way toward microhumor just with hedge words (“vaguely”, “sort of”), 
exaggerations (“the worst thing ever”, “doom us all”), and sometimes the combination of the two.

I think this microhumor stuff is really important, maybe the number one thing that separates really 
enjoyable writers from people who are technically proficient but still a chore to read. Think about
it with a really simplistic behaviorist model where you keep doing things that give you little bursts
of reward, and stop doing things that don’t. There are only a couple of sources of reward in reading.
One of them is getting important insights. Another is hearing things that support your ingroups or
bash your outgroups. And a third – maybe the biggest – is humor. Who ever had trouble slogging through
a really hilarious book of jokes?

Nobody can be super funny all the time, and an article on the economic crisis filled with man-walks-
into-a-bar-style jokes would be jarring and weird. But micro-humor really works. It works at a background 
level where people don’t notice it working, and it makes people keep coming back for more.

Humor is also disarming. It’s hard to hate somebody who’s making you laugh. I don’t mean somebody who’s 
making bigoted jokes that offend you, or writing political cartoons about how awful your ingroup is.
Those people are easy to hate. I mean somebody who’s making you laugh, right now. If you can make people
laugh while challenging their cherished beliefs, you’ve got a tiny bit more good will than usual.
```

It turns out that many of my favorite writers are masters of microhumor. Steve Yegge is another one -- see [here](#practical-magic). 

<a name="#the-art-of-plain-talk"></a>
### The art of plain talk
([overview](#overview))

Rudolf Flesch's book [The Art of Plain Talk](http://www.amazon.com/dp/B000HNDRLO/ref=nosim?tag=lukeprogcom-20) has a lot of great quotes. First, an observation: 

```markdown
Grammar is what makes sentences difficult. Conjugations, declensions, irregular verbs, ablatives,
subjunctives, and aorists, oh my!
```

Then a claim, persuasively made:

```markdown
It might help if we wrote in Chinese, the “grammarless” tongue. Chinese has no inflections. No cases.
No persons. No genders. No degrees. No tenses. No voices. No moods. No infinitives. No participles.
No gerunds. No irregular verbs. No articles. No prefixes or suffixes.

This is not because Chinese is primitive. Chinese was once an irregular, complicated mess like 
English. But the Chinese people, generation after generation, changed it into a streamlined machine
for expressing ideas.

We say “A man bites a dog.” They say “Man bite dog.” We say “A dog is an animal.” They say “Dog: animal.” 
We say “The sun is bright and shiny.” They say “Sun shine.”

This may sound like baby talk. But look closely and you will see that Chinese offers the same meaning 
without the baggage.

Now look at English. We have the word 'sign', meaning “a mark.” Add a suffix and you get 'signify', “to make
a mark.” Add another suffix and you have 'significant', “making a mark.” Add a prefix and you have 
'insignificant', “making no mark.” Add another suffix for 'insignificance', “the making of no mark.” We took
a simple noun and made it into a verb, an adjective, another adjective, and again a noun.

After all those complications, we can be philosophical and talk about the *insignificance of man*. The
Chinese would just talk about *man no mark*.

Good writers work hard to make the abstract concrete. The Chinese have no abstract words to begin with. 
English philosophers may write that “The health of societies is not benefited by a mass of individuals
desiring radical differentiation from each other, nor by a mass of individuals lacking independent thought
and action.” Chinese philosophers say “Do not be rare like jade or common like stone.”
```

At this point I'm of two minds. I like where Flesch is getting at, but don't like the complete lack of abstract words to work with. Abstraction is a double-edged sword, not an unalloyed evil.

But let's go with Flesch -- how might we write more plainly?

```markdown
Once, I was reading a philosophy essay of mine to my girlfriend. She could barely understand it. 
“Why don’t you just say it like you said it to me yesterday?” she asked.

As hard as it is to write in plain talk, we speak in plain talk all the time. What is it about our
talking that is clearer than our writing?

When speaking, we may use big words and a fast pace and complex grammar, but we give the other person 
time to understand us. We repeat ourselves. We use filler words. We pause.

But when writing, we eliminate all repeated and irrelevant words. If every word is important, the 
reader may not have time to digest what is being said because your ideas are coming at him too quickly.

We write like this: “Perhaps the toughest intellectual work we must do regarding European reconstruction
is to realize that it can be achieved through nonpolitical instrumentalities. Reconstruction will not be
politics, but engineering.”

We talk like this: “We have a tough job ahead of us, I think. Reconstructing Europe is a tough job, and 
it’s a job of thinking; of figuring out how to do it. We can’t do it with politics, I don’t think. No, 
it won’t be done with politics. It’ll be more like building a bridge; more like engineering. That’s the
way I see it, anyway.”

The first version is concise. It moves rapidly and requires strict concentration. But readers are 
surrounded by distractions. They need the repetition, pauses, and filler of the second version to keep 
up with your thought, or else they may have to read your sentences twice. Put space between your ideas.

We might combine the clarity of the first version with the readability of the second version like this:
“We have a tough job ahead of us. We need to figure out how to reconstruct Europe. It won’t happen with
political forces. The European reconstruction will be a matter of engineering, not politics.”
```

Flesch on avoiding brevity for brevity's sake:

```markdown
Simplicity and brevity are not the same thing. Plain talk can be slow. Condensed sentences are often tough
to read.

Brevity-worship can lead you to use confusing abbreviations, such as the famous Variety headline: STIX NIX
HIX PIX. To loyal Variety readers, this meant that small-town moviegoers disliked rural pictures. To the 
rest of us, it meant nothing.

Condensed writing is for experts. Plain talk is for everyone else. Consider this sentence: “We need long-
range planning for industrialization with long-term credit financing, decent wage guarantees, protection of
national interests and equal participation of domestic and foreign capital.”

We need lots of simple words to translate all those big words into plain English. Here, simplifying means 
*lengthening*, not shortening. Brevity is great, but simplicity is greater.
```

Engaging human interest:

```markdown
We know nothing better than ourselves. We speak and think best about us. That’s why it is a bit easier to
read and understand “Stalin drinks vodka” than “Vodka contains alcohol.” Give your writing human interest.

Newspapers add human interest to a scene by giving an eyewitness report. Scientific findings are reported 
as an unraveling mystery; the scientists as detectives. Public policy reporting is given from the 
biographical view of the persons pushing for policy change.

How might we engage human interest in this bland paragraph? “Du Pont this week announced a new product as
potentially profitable as its nylon. It is wood impregnated with chemicals that transform it into a hard,
polished material. Wood so treated does not warp, split, or shrink.”

It is easy to give your writing human interest. Look at each sentence and find the logical—not the 
grammatical—subject. The logical subject of almost everything that humans care about is human.

Who announced a new product? A corporation? No, people announced a new product: “The Du Pont people this
week announced…”

Next: Who impregnated the wood? The Du Pont people! So: “They have impregnated wood…”

But isn’t the subject of the next sentence wood? To find the logical subject in this sentence, ask: How do 
you know? Well, how does anyone know a scientific fact? By testing. So: “Their tests show that wood so 
treated does not warp…”

Our revised paragraph is less concise. But that gives our readers time to keep up with our thought. And 
now, the paragraph is about our readers’ favorite subject, people: “The Du Pont people this week announced
a new product as potentially profitable as its nylon. They have impregnated wood with chemicals that
transform it into a hard, polished material. Their tests show that wood so treated does not warp, split, 
or shrink.”
```

<a name="#fiction"></a>
### Fiction
([overview](#overview))

I actually don't know where exactly [this quote](https://www.goodreads.com/quotes/373814-this-sentence-has-five-words-here-are-five-more-words) by Gary Provost comes from, but it's a classic:

```markdown
This sentence has five words. Here are five more words. Five-word sentences are fine. 
But several together become monotonous. Listen to what is happening. The writing is getting boring. 
The sound of it drones. It’s like a stuck record. The ear demands some variety. 

Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a
pleasant rhythm, a lilt, a harmony. I use short sentences. And I use sentences of medium length. 
And sometimes, when I am certain the reader is rested, I will engage him with a sentence of considerable
length, a sentence that burns with energy and builds with all the impetus of a crescendo, the roll of 
the drums, the crash of the cymbals–sounds that say listen to this, it is important.
```

Note that this quote applies equally well to [nonfiction writing](#nonfiction).

<a name="#show-dont-tell"></a>
### Show, don't tell
([overview](#overview))

Veltzeh's [More on Writing: Show vs. Tell](http://veltzeh.livejournal.com/17035.html) argues against "show, don't tell":

```markdown
There is another thing about "showing" that bugs me, and it's that very often, when I read guides 
and such about how to "show" rather than "tell", the only thing I see in those guides is that 
"showing" conveys different information than "telling". They create different scenes. And of course
I have more problems with the scenes that "show".

When the scene is "shown", I have a hard time staying with the text and understanding what's going on.
If I'm not told what a particular thing means, it starts just seeming nonsensical to me. The author 
(and many readers) might understand the tone of voice of a character whose line is written in a certain
way, but I'm likely to miss it. And so I'll end up clueless as to the character's state of mind. Now,
if I was TOLD exactly what the character's state of mind is, no problem. I might be bad at imagining
certain emotions, but it's definitely easier to read exactly what it's supposed to be than to try to
find and guess what parts show it.

Maybe it's just me, though. That doesn't make me less frustrated with writing that routinely frustrates
me with what I often perceive as non sequiturs. Therefore, I hope that more writings BOTH "showed" AND 
"told"! At least I'm trying to aim for both showing and telling in my writings from now on.
```

This is interesting to me, because it runs counter to the standard advice. 

<a name="#software-development-and-computer-science"></a>
## Software development and computer science
([overview](#overview))

<a name="#Second-system-effect"></a>
### Second-system effect
([overview](#overview))

From Wikipedia:

```markdown
The second-system effect (also known as second-system syndrome) is the tendency of small,
elegant, and successful systems, to be succeeded by over-engineered, bloated systems, due 
to inflated expectations and overconfidence.
```

This *always* reminds me of that *great* Quora answer I've never been able to find, about an enterprise-level solution to an exceedingly trivial coding problem. First-class dry humor. 

Bit of nuance here. This is what Adam Turoff [has to say about software rewrite projects](http://notes-on-haskell.blogspot.com/2007/08/rewriting-software.html):

```markdown
One of the clearest opinions is from Joel Spolsky, who says rewrites are “the single worst 
strategic mistake that any software company can make”. His essay is seven years old, and in
it, he takes Netscape to task for open sourcing Mozilla, and immediately throwing all the 
code away and rewriting it from scratch. Joel was right, and for a few years Mozilla was a 
festering wasteland of nothingness, wrapped up in abstractions, with an unhealthy dose of
gratuitous complexity sprinkled on top. 

But this is open source, and open source projects have a habit of over-estimating the short
term and under-estimating the long term. ...

What’s missing from the discussion is an idea from Brian Eno about the differences between 
the “small here” vs. the “big here”, and the “short now” vs. the “long now”. Capsule summary:
we can either live in a “small here” (a great apartment in a crappy part of town) or a “big 
here” (a beautiful city in a great location with perfect weather and amazing vistas), and we 
can live in a “short now” (my deadline is my life) or a “long now” (how does this project 
change the company, the industry or the planet?).

On the one hand, Joel’s logic is irrefutable. If you’re dealing with a small here and a short
now, then there is no time to rewrite software. There are revenue goals to meet, and time 
spent redoing work is retrograde, and in nearly every case poses a risk to the bottom line 
because it doesn’t deliver end user value in a timely fashion.

On the other hand, Joel’s logic has got more holes in it than a fishing net. If you’re dealing 
with a big here and a long now, whatever work you do right now is completely inconsequential 
compared to where the project will be five years from today or five million users from now. 
Requirements change, platforms go away, and yesterday’s baggage has negative value — it leads
to hard-to-diagnose bugs in obscure edge cases everyone has forgotten about. The best way to 
deal with this code is to rewrite, refactor or remove it.

Joel Spolsky is arguing that the Great Mozilla rewrite was a horrible decision in the short 
term, while Adam Wiggins is arguing that the same project was a wild success in the long term.
Note that these positions do not contradict each other. Clearly, there is no one rule that fits
all situations.

The key to estimating whether a rewrite project is likely to succeed is to first understand when
it needs to succeed. If it will be evaluated in the short term (because the team lives in a small
here and a short now), then a rewrite project is quite likely to fail horribly. On the other hand,
if the rewrite will be evaluated in the long term (because the team lives in a big here and a long
now), then a large rewrite project just might succeed and be a healthy move for the project.
```

Why might rewrites be bad? Neil Gunton's [Rewrites Considered Harmful? When is "good enough" enough?](http://www.neilgunton.com/doc/?o=1mr&doc_id=8583):

```markdown
You might read all this and think what an idiot I am for suggesting that older, crappier, buggier,
dirtier, messier, more complex software might be better than newer, cleaner, faster rewrites. Well,
the point is a subtle one - in a nutshell, when you rewrite, you lose all those little fixes and 
improvements that made the older version good to use and reliable. New software always introduces 
new bugs. Often, the rewrite process seems to be driven by a desire to make the product somehow 
more theoretically consistent and complete - which in turn often ends up losing the simplicity and 
elegance that made the original so compelling and useful. Rewriting, especially when it breaks 
existing systems, results in multiple versions of software that makes it confusing for new users and
perplexing for old users.

And, let's face it - programmers just like to write new code. It's natural. We all do it - it's easier
to start from scratch than it is to make the old version better. Also, it's more glamorous - everybody
wants to be credited with creating something themselves, rather than maintaining and developing an 
existing thing. So, I can quite understand why things are the way they are.

Mind you, I am not saying that we should never rewrite code - sometimes it's just a necessary thing,
because of new platforms or changes to underlying API's. It's all a question of degree - do you 
totally rewrite, or do you evolve existing, working code? Rewrites are so often done without any
regard to the old code at all. In my experience, new programmers often come on board, and it's just 
too much trouble to look through and really understand all the little nooks and crannies. We have
seen it plenty of times in business - there is an old version of the application, but you're brought
in to put together a new version. Usually the new spec has so many functional/UI differences from 
the old one that the old is simply discarded as being irrelevant. And yet, many times, the underlying
functional differences are not actually all that great. So, unfortunately, years of patches, special
cases and wisdom are just abandoned.

There is a "cost" involved with totally rewriting any application, in terms of "lost wisdom". If you
have a package that is very popular, used by many people and has had a lot of bugfixes and patches 
applied over time, then it is more likely that a total rewrite will have a higher cost. Also if you
change the way it works in the process, you create a chasm between the new and old versions that has
to be crossed by users, and this causes stress. Which version to use - the old, reliable, well known
but out-of-date version, or the newer, sleaker, incompatible, more buggy version? Hmmm. If your
software (or standard) is not used by many people and doesn't have any significant history behind it
(in terms of "accumulated wisdom") then clearly there are no real issues involved in rewriting - the
cost is low. So I am not making a blanket statement that rewriting is bad; the whole point of this
article was to focus on tools and standards that have attained great success and are used by many
people. Such software/standards will inevitably have had a large amount of wisdom invested over time, 
because nothing is perfect first time out. Thus it is the most popular tools and packages that are
most likely to be casualties of total rewrites.

So in summary, I would say that the "cost" of a total rewrite depends on three factors:

1. Amount of "accumulated wisdom" (bug fixes, tweaks and useful patches) in the old version that will be discarded
2. How incompatible the new version is with the old version (API, data formats, protocols etc)
3. How many people used the old version and will be affected by the changes

A suggestion: If you have a very successful application, don't look at all that old, messy code as
being "stale". Look at it as a living organism that can perhaps be healed, and can evolve. You can 
refactor, you can rewrite portions of the internals to work better, many things can be accomplished
without abandoning all the experience and error correction that went into that codebase. When you 
rewrite you are abandoning history and condemning yourself to relive it.
```

Neil's last remark reminds me of Kevin Simler's [A Codebase is an Organism](https://meltingasphalt.com/a-codebase-is-an-organism/), which is a distinct enough idea that I also want to remember that I've created [its own subheading](#codebase-as-organism).

<a name="#codebase-as-organism"></a>
### Codebase as organism
([overview](#overview))

Kevin Simler's blog [Melting Asphalt](https://meltingasphalt.com/about/) is always a great read. Here's one of my favorite essays of his, [A Codebase is an Organism](https://meltingasphalt.com/a-codebase-is-an-organism/).

The introduction alone is gold:

```markdown
Here's what no one tells you when you graduate with a degree in computer science and take
up a job in software engineering:

*The computer is a machine, but a codebase is an organism.*

This will make sense to anyone who's worked on a large or even medium-sized software project
— but it's often surprising to new grads. Why? Because nothing in your education prepares you
for how to deal with an organism.

Computer science is all about how to control the machine: to make it do exactly what you want,
during execution, on the time scale of nano- and milliseconds. But when you build real software
— especially as part of a team — you have to learn how to control not only the (very obedient) 
machine, but also a large, sprawling, and often unruly codebase.

This turns out to require a few 'softer' skills. Unlike a computer, which always does exactly 
what it's told, code can't really be bossed around. Perhaps this is because code is ultimately
managed by people. But whatever the reason, you can't tell a codebase what to do and expect to
be obeyed. Instead, the most you can do (in order to maximize your influence) is try to steward
the codebase, nurture it as it grows over a period of months and years.

When you submit a CS homework assignment, it's done. Fixed. Static. Either your algorithm is 
correct and efficient or it's not. But push the same algorithm into a codebase and there's a
very real sense in which you're releasing it into the wild.

Out there in the codebase, all alone, your code will have to fend for itself. It will be 
tossed and torn, battered and bruised, by other developers — which will include yourself, of
course, at later points in time. Exposed to the elements (bug fixes, library updates, drive-by
refactorings), your code will suffer all manner of degradations, "the slings and arrows of 
outrageous fortune... the thousand natural shocks that flesh is heir to."
```

The nursing metaphor, or "codebase as sick patient":

```markdown
The organic nature of code manifests itself in the dual forces of growth and decay.

Let's start with decay. Realizing that code can wither, decay, or even die leads us to the
nursing metaphor, or codebase as sick patient.

Code doesn't decay on its own, of course. Left completely untouched, it will survive as long 
as you care to archive it. Decay — often called code rot or software rot — only sets in when 
changes are made, either to the code itself or to any of its dependencies. So as a rule of thumb,
we can say that most code is decaying during most of its existence. It's like entropy. You never
'win' against entropy; you just try to last as long as you can.

In a healthy piece of code, entropic decay is typically staved off by dozens of tiny interventions 
— bug fixes, test fixes, small refactors, migrating off a deprecated API, that sort of thing.
These are the standard maintenance operations that all developers undertake on behalf of code
that they care about. It's when the interventions stop happening, or don't happen often enough,
that code rot sets in.

We can assess a module in terms of 'risk factors' for this kind of decay. The older a module is,
for example, the more likely it is to be suffering from code rot. More important than age, however, 
is the time since last major refactor. (Recently-refactored code is a lot like new code, for good
or ill.) Also, the more dependencies a module has, and the more those dependencies have recently 
changed, the more likely the module is to have gone bad.

But all of these risk factors pale in importance next to how much execution a piece of code has
been getting. Execution by itself isn't quite enough, though — it has to be in a context where 
someone is paying attention to the results. This type of execution is also known as testing.

Testing can take many forms — automated or manual, ad hoc developer testing, and even 'testing' 
through use in production. As long as the code is getting executed in a context where the results
matter, it counts. The more regularly this happens, of course, the better.

I find it useful to think of execution as the lifeblood of a piece of code — the vital flow of
control? electronic pulse? — and testing as medical instrumentation, like a heart rate monitor. 
You never know when a piece of code, which is rotting all the time, will atrophy in a way that 
causes a serious bug. If your code is being tested regularly, you'll find out soon and will be 
able to intervene. But without testing, no one will notice that your code has flatlined. Errors
will begin to pile up. After a month or two, a module can easily become so rotten that it's 
impossible to resuscitate.

Thus teams are often confronting the uncomfortable choice between a risky refactoring operation 
and clean amputation. The best developers can be positively gleeful about amputating a diseased 
piece of code (even when it's their own baby, so to speak), recognizing that it's often the best
choice for the overall health of the project. Better a single module should die than continue to
bog down the rest of the project.
```

Code growth:

```markdown
Now you might assume that while decay is problematic, growth is always good. But of course it's 
not so simple.

Certainly it's true that a project needs to grow in order to become valuable, so the problem 
isn't growth per se, but rather unfettered or opportunistic growth. Haphazard growth. Growth by
means of short-sighted, local optimizations. And this kind of growth seems to be the norm —
perhaps because developers are often themselves short-sighted and opportunistic, if not outright
lazy. But even the best, most conscientious developers fall pitifully short of making globally-
optimal decisions all the time.

Left to 'its' own devices, then, a codebase can quickly devolve into a tangled mess. And the more
it grows, the more volume it has to maintain against the forces of entropy. In this way, a project
can easily collapse under its own weight.

For these reasons, any engineer worth her salt soon learns to be paranoid of code growth.

She assumes, correctly, that whenever she ceases to be vigilant, the code will get itself into 
trouble. She knows, for example, that two modules will tend to grow ever more dependent on each 
other unless separated by hard ('physical') boundaries. She's had to do that surgery — to separate
two modules that had become inappropriately entangled with each other. Afraid of such spaghetti 
code (rat's nests), she strives relentlessly to arrange her work (and the work of others) into
small, encapsulated, decoupled modules.

Faced with the necessity of growth but also its dangers, the seasoned engineer therefore seeks a
balance between nurture and discipline. She knows she can't be too permissive; coddled code won't
learn its boundaries. But she also can't be too tyrannical. Code needs some freedom to grow at the
optimal rate.

In this way, building software isn't at all like assembling a car. In terms of managing growth, it's
more like raising a child or tending a garden.
```

The benefit of failing fast -- failure as conflict of interest between computer and codebase:

```markdown
Finally, here's an idea that took me many years to appreciate on a gut level (but which is completely
obvious now in hindsight): the benefits of failing fast. What I should have understood is that failure
(on unexpected inputs) reflects a conflict of interest between the computer and the codebase.

From the perspective of the machine on which the code is executing right now, it's better not to fail 
and hope that everything will be OK — hope that some other part of the stack will handle the failure 
gracefully. Maybe we can recover. Maybe the user won't notice. Why crash when we could at least try 
to keep going? And everything in my CS education taught me to do what's right for the machine.

But from the perspective of the codebase — whose success depends not on any single execution, but
rather on long-term health — it's far better to fail fast (and loud), in order to call immediate 
attention to the problem so it can be fixed.

Coddled code will fester. Spare the rod, spoil the child.
```

<a name="#Theoretical-CS-contributions"></a>
### Theoretical CS contributions
([overview](#overview))

From Scott Aaronson's post [Logicians on safari](https://www.scottaaronson.com/blog/?p=152), responding to Sean Carroll's claim:

```markdown
I’m happy to admit that I don’t know anything about “one-way functions and interactive proofs.”
So, in what sense has theoretical computer science contributed more in the last 30 years to our
basic understanding of the universe than particle physics or cosmology? (Despite the fact that
I’m a cosmologist, I don’t doubt your statement — I’d just like to be able to explain it in public.)
```

The context for Sean's comment was [this post of Scott's](https://www.scottaaronson.com/blog/?p=151), lamenting and criticquing Steve Lohr's NYT essay that started off like so:

```markdown
Computer science is not only a comparatively young field, but also one that has had to prove it 
is really science. Skeptics in academia would often say that after Alan Turing described the
concept of the “universal machine” in the late 1930’s — the idea that a computer in theory could
be made to do the work of any kind of calculating machine, including the human brain — all that 
remained to be done was mere engineering.

The more generous perspective today is that decades of stunningly rapid advances in processing 
speed, storage and networking, along with the development of increasingly clever software, have
brought computing into science, business and culture in ways that were barely imagined years ago.
The quantitative changes delivered through smart engineering opened the door to qualitative changes.
```

which resulted in Scott's rueful rant:

```markdown
Even among the commenters on this post by Chad Orzel — which Dave Bacon forwarded to me with the
subject line “bait” — awareness of any third possibility seems depressingly rare. Judging from the
evidence, it’s not that people have engaged the mysteries of P versus NP, randomness and 
determinism, one-way functions and interactive proofs, and found them insufficiently deep. Rather, 
as bizarre as it sounds, it’s that people don’t know these mysteries exist — just as they wouldn’t 
know about black holes or the Big Bang if no one told them. If you want to understand why our
subject — which by any objective standard, has contributed at least as much over the last 30 years
as (say) particle physics or cosmology to humankind’s basic picture of the universe — receives a
whopping $5 million a year from the NSF (with even that in constant danger), look no further.
```

Enough backtracking/context-setting. Here's what Scott has to say in response to Sean; this is also the origin of the "theoretical CS is quantitative epistemology" quote:

```markdown
Of course I was joking when I mentioned “objective standards” for ranking scientific fields. 
Depending on which questions keep you up at night, different parts of “humankind’s basic picture
of the universe” will seem larger or smaller. (To say that, of course, is not to suggest any 
relativism about the picture itself.)

What I can do, though, is to tell you why — by my own subjective standards — the contributions 
of theoretical computer science over the last 30 years rival those of theoretical physics or any 
other field I know about. Of course, people will say I only think that because I’m a theoretical 
computer scientist, but that gets the causal arrow wrong: I became a theoretical computer
scientist because, as a teenager, I thought it!

It’s probably best to start with some examples.

1. We now know that, if an alien with enormous computational powers came to Earth, it could prove to
us whether White or Black has the winning strategy in chess. To be convinced of the proof, we would
not have to trust the alien or its exotic technology, and we would not have to spend billions of 
years analyzing one move sequence after another. We’d simply have to engage in a short conversation
with the alien about the sums of certain polynomials over finite fields.

2. There’s a finite (and not unimaginably-large) set of boxes, such that if we knew how to pack those 
boxes into the trunk of your car, then we’d also know a proof of the Riemann Hypothesis. Indeed,
every formal proof of the Riemann Hypothesis with at most (say) a million symbols corresponds to
some way of packing the boxes into your trunk, and vice versa. Furthermore, a list of the boxes 
and their dimensions can be feasibly written down.

3. Supposing you do prove the Riemann Hypothesis, it’s possible to convince someone of that fact, 
without revealing anything other than the fact that you proved it. It’s also possible to write the
proof down in such a way that someone else could verify it, with very high confidence, having only
seen 10 or 20 bits of the proof.

4. If every second or so your computer’s memory were wiped completely clean, except for the input data; 
the clock; a static, unchanging program; and a counter that could only be set to 1, 2, 3, 4, or 5,
it would still be possible (given enough time) to carry out an arbitrarily long computation — just
as if the memory weren’t being wiped clean each second. This is almost certainly not true if the
counter could only be set to 1, 2, 3, or 4. The reason 5 is special here is pretty much the same
reason it’s special in Galois’ proof of the unsolvability of the quintic equation.

5. It would be great to prove that RSA is unbreakable by classical computers. But every known technique 
for proving that would, if it worked, simultaneously give an algorithm for breaking RSA! For example,
if you proved that RSA with an n-bit key took n^5 steps to break, you would’ve discovered an algorithm
for breaking it in 2n^1/5 steps. If you proved that RSA took 2n^1/3 steps to break, you would’ve 
discovered an algorithm for breaking it in n(log n)^2 steps. As you show the problem to be harder,
you simultaneously show it to be easier.

Alright, let me stop before I get carried away. The examples I’ve listed (and hundreds more like them) 
are not exactly discoveries about physics, but they don’t have the flavor of pure math either. And 
even if they have some practical implications for computing (which they do), they certainly don’t have
the flavor of nitty-gritty software engineering.

So what are they then? Maybe it’s helpful to think of them as “quantitative epistemology”: discoveries
about the capacities of finite beings like ourselves to learn mathematical truths. On this view, the
theoretical computer scientist is basically a mathematical logician on a safari to the physical world: 
someone who tries to understand the universe by asking what sorts of mathematical questions can and 
can’t be answered within it. Not whether the universe is a computer, but what kind of computer it is!
Naturally, this approach to understanding the world tends to appeal most to people for whom math (and
especially discrete math) is reasonably clear, whereas physics is extremely mysterious.

In my opinion, one of the biggest challenges for our time is to integrate the enormous body of
knowledge in theoretical computer science (or quantitative epistemology, or whatever you want to call
it) with the rest of what we know about the universe. In the past, the logical safari mostly stayed
comfortably within 19th-century physics; now it’s time to venture out into the early 20th century. 
Indeed, that’s exactly why I chose to work on quantum computing: not because I want to build quantum
computers (though I wouldn’t mind that), but because I want to know what a universe that allows
quantum computers is like.

Incidentally, it’s also why I try hard to keep up with your field. If I’m not mistaken, less than a 
decade ago cosmologists made an enormous discovery about the capacity of finite beings to learn 
mathematical truths: namely, that no computation carried out in the physical world can ever involve
more than 1/Λ ~ 10^122 bits.
```

Or in more straightforward (if impenetrable to me) jargon:

```markdown
Oh, alright:

(1) IP=PSPACE (Shamir 1990).

(2) 3-dimensional bin-packing is NP-complete.

(3) NP has zero-knowledge proofs (Goldreich-Micali-Wigderson 1986); the PCP Theorem.

(4) Width-5 branching programs can compute NC1 (Barrington 1986); corollary pointed out by
Ogihara 1994 that width-5 bottleneck Turing machines can compute PSPACE

(5) Natural proofs (Razborov-Rudich 1993); in particular Wigderson’s observation about
natural proofs for discrete log.
```

Gil Kalai's own list is also great, and admirably straight to the point:

```markdown
Hi Scott

Let me try to suggest a skeleton for a good answer to Sean Carroll on some main insights
developed in theoretical computer science which with more effort can be explained to a 
layperson. I would personally prefer a description which at the end is not polemic, 
apologetic, or too sophisticated.

1. The notion of an algorithm. Some basic insights from the theory of algorithms: the 
importance of storing of hashing and of backtracking

2. The idea that a computer cannot do everything. Intractability and the NP = P problem

3. What computers can (magically) do very quickly. FFT and other basic algorithms in
mathematical and scientific computing.

4. What computers can do but hardly: Linear programming. Semi definite programming and 
related optimization problems.

5. “Beyond a reasonable doubt:” the notion of probabilistic proof

6. The notion of interactive proofs. Like in court, interactive process can raise the 
ability of reaching the truth.

7. Secrets and lies. The depth and fundamental importance of cryptography. (And while 
talking about crypthography, sure, the bizzare and amazing notion of zero-knowledge proofs.)

8. (If we want to reach the cutting edge:) The notion of probabilistically checkable proofs 
and hardness of approximation.

9. Distributed system. The difficulty of cooperation and synchronization.

10. Underatnding, using tools developed in theoretical computer science, the notions of 
“learning” and “knowledge”.

11. Merging computation and economics: lies and strategies.

12. Quantum computation.

Of course, this refer only to a small part of theoretical computer science (STOC/FOCS).
```

In response to Greg's comment that TCS "doesn't sound different from what we do in pure math":

```markdown
Greg: I’m delighted you agree with me that TCS, whatever else it is, is also “perfectly 
satisfactory as pure math”! But alas, not all mathematicians see things the way you do: to
many of them, the goal of “real” or “deep” math is to start from dizzyingly-general theorems
about infinite-dimensional spaces and then generalize them even further. The TCS goal, of 
course, is to take laughably-concrete problems that we can’t solve and then identify special
cases that we still can’t solve. Same standards for deciding what’s true; different aesthetic.

That said, the TCS and math aesthetics drew noticeably closer in just the past decade (see 
Luca’s blog for more on that theme), and maybe they’ll be indistinguishable at some point in 
the future. Basically, I see TCS and math as Galapagos finches that lived on different islands
for quite a while, but that are probably still interfertile.
```

Greg isn't happy at all with Scott's response:

```markdown
Of course I can’t be happy about this description of pure mathematics, and frankly I’m a 
little surprised to hear it from you. As I said, pure math comes in many different flavors.
Naturally, when you see a flavor of it that is far from your own, it might impress you as 
excessively abstract and on top of that pretentious. But wouldn’t it be safe to suppose, if
well-regarded people are doing the research, that their area can also be made to look 
unpretentious and beautiful?

More specifically, the presence of infinite-dimensional spaces is a strange criterion for 
deciding that topic is counting angels on the heads of pins. I don’t see that any of the seven
Clay Prize problems directly refer to infinite-dimensional spaces. The Yang-Mills problem 
comes the closest, but that’s because it’s a problem from physics!

But even if an area does use infinite-dimensional spaces, is that so bad? After all, P (i.e.,
boolean functions computable in polynomial time) is an infinite-dimensional vector space over
the field with two elements, and so is Delta^nP. Should I say that the conjecture that the
polynomial-time hierarchy does not collapse is exactly one of your absurdly general
conjectures about infinite-dimensional spaces?

Granted, non-collapse is a conjecture rather than a theorem. The set of oracles is also an 
infinite-dimensional vector space over Z/2, and the set of generic oracles is a highly 
abstract subset of this vector space. Do you think that it pushes the limits to prove a 
separation result using generic oracles?

But maybe these examples are too clever by half. If any theorem naturally fits the description
of an absurdly general fact about infinite-dimensional spaces, it would be the spectral 
theorem for self-adjoint operators on a Hilbert space. Do you seriously mean to say, given
that you are a quantum information theorist, that the spectral theorem is too abstract to 
matter?

*The TCS goal, of course, is to take laughably-concrete problems that we can’t solve and then
identify special cases that we still can’t solve.*

That is sometimes the goal, except that it’s usually a disingenuous goal because everyone knows
that the special cases are just as hard as the general cases. At least, so it went with Avi 
Wigderson’s special case of P vs #P at the ICM in Madrid.

*That said, the TCS and math aesthetics drew noticeably closer in just the past decade (see 
Luca’s blog for more on that theme), and maybe they’ll be indistinguishable at some point in 
future. Basically, I see TCS and math as Galapagos finches that lived on different islands for
quite a while, but that are probably still interfertile.*

I don’t know quite when or whether TCS drew closer to pure mathematics, but I can point to 
the fact that both Luca and Avi gave invited talks at the International Congress of
Mathematicians. Could the TCS community be disregarding some important olive branches? It is
true that TCS and math are on different “islands”, in the trivial sense of being in different
departments. That, I think, is the only real schism. It’s like what they say about America and
Britain: two nations divided by a common language.
```

<a name="#CS-is-beyond-the-glass-ceiling-of-pop-science"></a>
### CS is beyond the glass ceiling of pop science
([overview](#overview))

Greg Kuperberg's comment on Scott Aaronson's post [Logicians on safari](https://www.scottaaronson.com/blog/?p=152), responding to "There are zillions of pop science books about physics, math, biology… but honestly, I can’t think of any notable popular theoretical computer science books":

```markdown
I respect popular science writing as much as anyone, but it is easy to spend too much time 
kicking yourself if your area doesn’t benefit from it. At the end of the day, popular science
is written to entertain people, and hopefully to attract interest, but not to teach them. It 
isn’t another form of school. Popular science will inevitably overreward some areas and 
underreward others.

In particular, popular science has a “glass ceiling” that will prevent you from communicating 
many great ideas. I had an epiphany about this when I was at the house of a relative many years
ago. He was a smart guy with a far-above-average interest in poular science, but he did not 
have advanced training in mathematics. I think that he had more mathematics books than I do;
certainly he had more math biographies.

At one point he was interested in the Rubik’s cube. The Rubik’s cube is different from other 
popular mathematics, becuase it goads you to actually solve the damn thing instead of just 
reading about mathematicians at work. So he bought four books on how to solve the Rubik’s cube.
But with a good understanding of undergraduate group theory, one would have been more than 
enough! Without some solid mathematical intuition of some kind (not necessarily a group theory
course), books on the Rubik’s cube are downright painful. That really revealed the glass ceiling.

It seems that TCS is generally beyond the glass ceiling of popular science. I think that it’s
because TCS is made up of theorems. It’s not so hard explain what a theorem is. But it is hard
to explain, except to those who have joined the club, how much it matters to prove theorems. 
In fact some science journalists have discussed their impression that theorems are just a chore
that can be skipped, postponed, or hopefully soon relegated to computers.
```

And in response to "I have to respectfully disagree about Theoretical Computer Science being beyond the glass ceiling of popular science":

```markdown
The glass ceiling is in different places for different people, and in different venues. I am 
thinking of what gets published in the New York Times Science section. That is not at the 
level of what is written by a PhD that you might find interesting, but rather more at the 
level of what you might write for people on the next rung down the ladder.

Look at how much the Times watered down Sara Robinson’s article on the result that primality 
is in P. It was still a very nice article, but it was radically restricted to the basics. The
Times really seems to believe that the significance of theorems is all but beyond their
readership, that is only worth mentioning when the story is as big as the Poincare Conjecture 
or Fermat’s Last Theorem. Although I personally think that the Times is too conservative on 
this point, they probably aren’t completely wrong.
```

<a name="#smalltalk"></a>
### Smalltalk
([overview](#overview))

Alan Kay, in 1998, emphasizing comms/messaging over object properties or even behavior:

```markdown
Just a gentle reminder that I took some pains at the last OOPSLA to try to remind everyone that
Smalltalk is not only NOT its syntax or the class library, it is not even about classes. I’m sorry
that I long ago coined the term “objects” for this topic because it gets many people to focus on
the lesser idea.

The big idea is “messaging” - that is what the kernel of Smalltalk/Squeak is all about (and it’s 
something that was never quite completed in our Xerox PARC phase). The Japanese have a small word
- ma - for “that which is in between” - perhaps the nearest English equivalent is “interstitial”. 
The key in making great and growable systems is much more to design how its modules communicate 
rather than what their internal properties and behaviors should be. Think of the internet - to 
live, it (a) has to allow many different kinds of ideas and realizations that are beyond any single
standard and (b) to allow varying degrees of safe interoperability between these ideas.
```

<a name="#platforms"></a>
### Platforms
([overview](#overview))

The following couple of quotes are all from Steve Yegge's legendary [platforms rant](https://gist.github.com/chitchcock/1281611). Yegge starts with the Big Mandate to set things up:

```markdown
One day Jeff Bezos issued a mandate. He's doing that all the time, of course, and people 
scramble like ants being pounded with a rubber mallet whenever it happens. But on one 
occasion -- back around 2002 I think, plus or minus a year -- he issued a mandate that was
so out there, so huge and eye-bulgingly ponderous, that it made all of his other mandates 
look like unsolicited peer bonuses.

His Big Mandate went something along these lines:

1. All teams will henceforth expose their data and functionality through service interfaces.

2. Teams must communicate with each other through these interfaces.

3. There will be no other form of interprocess communication allowed: no direct linking, 
no direct reads of another team's data store, no shared-memory model, no back-doors whatsoever.
The only communication allowed is via service interface calls over the network.

4. It doesn't matter what technology they use. HTTP, Corba, Pubsub, custom protocols -- doesn't
matter. Bezos doesn't care.

5. All service interfaces, without exception, must be designed from the ground up to be
externalizable. That is to say, the team must plan and design to be able to expose the interface 
to developers in the outside world. No exceptions.

6. Anyone who doesn't do this will be fired.

7. Thank you; have a nice day!

Ha, ha! You 150-odd ex-Amazon folks here will of course realize immediately that #7 was a little
joke I threw in, because Bezos most definitely does not give a shit about your day.

#6, however, was quite real, so people went to work. Bezos assigned a couple of Chief Bulldogs to
oversee the effort and ensure forward progress, headed up by Uber-Chief Bear Bulldog Rick Dalzell.
Rick is an ex-Armgy Ranger, West Point Academy graduate, ex-boxer, ex-Chief Torturer slash CIO at 
Wal*Mart, and is a big genial scary man who used the word "hardened interface" a lot. Rick was a 
walking, talking hardened interface himself, so needless to say, everyone made LOTS of forward 
progress and made sure Rick knew about it.

Over the next couple of years, Amazon transformed internally into a service-oriented architecture.
They learned a tremendous amount while effecting this transformation. There was lots of existing 
documentation and lore about SOAs, but at Amazon's vast scale it was about as useful as telling 
Indiana Jones to look both ways before crossing the street. Amazon's dev staff made a lot of 
discoveries along the way. A teeny tiny sampling of these discoveries included:

- pager escalation gets way harder, because a ticket might bounce through 20 service calls before the
real owner is identified. If each bounce goes through a team with a 15-minute response time, it 
can be hours before the right team finally finds out, unless you build a lot of scaffolding and 
metrics and reporting.

- every single one of your peer teams suddenly becomes a potential DOS attacker. Nobody can make any
real forward progress until very serious quotas and throttling are put in place in every single 
service.

- monitoring and QA are the same thing. You'd never think so until you try doing a big SOA. But when
your service says "oh yes, I'm fine", it may well be the case that the only thing still 
functioning in the server is the little component that knows how to say "I'm fine, roger roger, 
over and out" in a cheery droid voice. In order to tell whether the service is actually responding,
you have to make individual calls. The problem continues recursively until your monitoring is doing
comprehensive semantics checking of your entire range of services and data, at which point it's 
indistinguishable from automated QA. So they're a continuum.

- if you have hundreds of services, and your code MUST communicate with other groups' code via these
services, then you won't be able to find any of them without a service-discovery mechanism. And you
can't have that without a service registration mechanism, which itself is another service. So Amazon
has a universal service registry where you can find out reflectively (programmatically) about every
service, what its APIs are, and also whether it is currently up, and where.

- debugging problems with someone else's code gets a LOT harder, and is basically impossible unless
there is a universal standard way to run every service in a debuggable sandbox.

That's just a very small sample. There are dozens, maybe hundreds of individual learnings like these
that Amazon had to discover organically. There were a lot of wacky ones around externalizing services
, but not as many as you might think. Organizing into services taught teams not to trust each other 
in most of the same ways they're not supposed to trust external developers.

This effort was still underway when I left to join Google in mid-2005, but it was pretty far advanced.
From the time Bezos issued his edict through the time I left, Amazon had transformed culturally into 
a company that thinks about everything in a services-first fashion. It is now fundamental to how they
approach all designs, including internal designs for stuff that might never see the light of day 
externally.

At this point they don't even do it out of fear of being fired. I mean, they're still afraid of that;
it's pretty much part of daily life there, working for the Dread Pirate Bezos and all. But they do 
services because they've come to understand that it's the Right Thing. There are without question 
pros and cons to the SOA approach, and some of the cons are pretty long. But overall it's the right
thing because SOA-driven design enables Platforms.
```

Why did Bezos want to platformize Amazon?

```markdown
But Bezos realized long before the vast majority of Amazonians that Amazon needs to be a platform.

You wouldn't really think that an online bookstore needs to be an extensible, programmable platform.
Would you?

Well, the first big thing Bezos realized is that the infrastructure they'd built for selling and
shipping books and sundry could be transformed an excellent repurposable computing platform. So 
now they have the Amazon Elastic Compute Cloud, and the Amazon Elastic MapReduce, and the Amazon 
Relational Database Service, and a whole passel' o' other services browsable at aws.amazon.com. 
These services host the backends for some pretty successful companies, reddit being my personal 
favorite of the bunch.

The other big realization he had was that he can't always build the right thing. I think Larry 
Tesler might have struck some kind of chord in Bezos when he said his mom couldn't use the goddamn
website. It's not even super clear whose mom he was talking about, and doesn't really matter, 
because nobody's mom can use the goddamn website. In fact I myself find the website disturbingly 
daunting, and I worked there for over half a decade. I've just learned to kinda defocus my eyes 
and concentrate on the million or so pixels near the center of the page above the fold.
```

Platforms matter because they allow Accessibility:

```markdown
I'm not really sure how Bezos came to this realization -- the insight that he can't build one 
product and have it be right for everyone. But it doesn't matter, because he gets it. There's
actually a formal name for this phenomenon. It's called Accessibility, and it's the most 
important thing in the computing world.

The. Most. Important. Thing.

If you're sorta thinking, "huh? You mean like, blind and deaf people Accessibility?" then you're
not alone, because I've come to understand that there are lots and LOTS of people just like you:
people for whom this idea does not have the right Accessibility, so it hasn't been able to get 
through to you yet. It's not your fault for not understanding, any more than it would be your
fault for being blind or deaf or motion-restricted or living with any other disability. When 
software -- or idea-ware for that matter -- fails to be accessible to anyone for any reason, 
it is the fault of the software or of the messaging of the idea. It is an Accessibility failure.

Like anything else big and important in life, Accessibility has an evil twin who, jilted by the 
unbalanced affection displayed by their parents in their youth, has grown into an equally 
powerful Arch-Nemesis (yes, there's more than one nemesis to accessibility) named Security. And 
boy howdy are the two ever at odds.

But I'll argue that Accessibility is actually more important than Security because dialing 
Accessibility to zero means you have no product at all, whereas dialing Security to zero can still
get you a reasonably successful product such as the Playstation Network. 
```

Platform-product relationship:

```markdown
A product is useless without a platform, or more precisely and accurately, 
a platform-less product will always be replaced by an equivalent platform-ized product.

I apologize to those (many) of you for whom all this stuff I'm saying is incredibly obvious,
because yeah. It's incredibly frigging obvious. Except we're (Google) not doing it. We don't get 
Platforms, and we don't get Accessibility. The two are basically the same thing, because platforms
solve accessibility. A platform is accessibility.
```

<a name="#Amazon-vs-Google-2011"></a>
### Amazon vs Google 2011
([overview](#overview))

From Steve Yegge's legendary [platforms rant](https://gist.github.com/chitchcock/1281611).

Amazon:

```markdown
I was at Amazon for about six and a half years, and now I've been at Google for that long. 
One thing that struck me immediately about the two companies -- an impression that has been
reinforced almost daily -- is that Amazon does everything wrong, and Google does everything 
right. Sure, it's a sweeping generalization, but a surprisingly accurate one. It's pretty 
crazy. There are probably a hundred or even two hundred different ways you can compare the 
two companies, and Google is superior in all but three of them, if I recall correctly. I a
ctually did a spreadsheet at one point but Legal wouldn't let me show it to anyone, even 
though recruiting loved it.

I mean, just to give you a very brief taste: Amazon's recruiting process is fundamentally 
flawed by having teams hire for themselves, so their hiring bar is incredibly inconsistent
across teams, despite various efforts they've made to level it out. And their operations 
are a mess; they don't really have SREs and they make engineers pretty much do everything,
which leaves almost no time for coding - though again this varies by group, so it's luck of
the draw. They don't give a single shit about charity or helping the needy or community 
contributions or anything like that. Never comes up there, except maybe to laugh about it.
Their facilities are dirt-smeared cube farms without a dime spent on decor or common meeting 
areas. Their pay and benefits suck, although much less so lately due to local competition 
from Google and Facebook. But they don't have any of our perks or extras -- they just try 
to match the offer-letter numbers, and that's the end of it. Their code base is a disaster,
with no engineering standards whatsoever except what individual teams choose to put in place.

To be fair, they do have a nice versioned-library system that we really ought to emulate, and
a nice publish-subscribe system that we also have no equivalent for. But for the most part 
they just have a bunch of crappy tools that read and write state machine information into
relational databases. We wouldn't take most of it even if it were free.

I think the pubsub system and their library-shelf system were two out of the grand total of 
three things Amazon does better than google.

I guess you could make an argument that their bias for launching early and iterating like mad
is also something they do well, but you can argue it either way. They prioritize launching 
early over everything else, including retention and engineering discipline and a bunch of
other stuff that turns out to matter in the long run. So even though it's given them some
competitive advantages in the marketplace, it's created enough other problems to make it
something less than a slam-dunk. 
```

Google: 

```markdown
That one last thing that Google doesn't do well is Platforms. We don't understand platforms.
We don't "get" platforms. Some of you do, but you are the minority. This has become painfully
clear to me over the past six years. I was kind of hoping that competitive pressure from 
Microsoft and Amazon and more recently Facebook would make us wake up collectively and start 
doing universal services. Not in some sort of ad-hoc, half-assed way, but in more or less the 
same way Amazon did it: all at once, for real, no cheating, and treating it as our top priority
from now on.

But no. No, it's like our tenth or eleventh priority. Or fifteenth, I don't know. It's pretty 
low. There are a few teams who treat the idea very seriously, but most teams either don't think
about it all, ever, or only a small percentage of them think about it in a very small way.

It's a big stretch even to get most teams to offer a stubby service to get programmatic access
to their data and computations. Most of them think they're building products. And a stubby 
service is a pretty pathetic service. Go back and look at that partial list of learnings from
Amazon, and tell me which ones Stubby gives you out of the box. As far as I'm concerned, it's
none of them. Stubby's great, but it's like parts when you need a car. ...

Google+ is a prime example of our complete failure to understand platforms from the very 
highest levels of executive leadership (hi Larry, Sergey, Eric, Vic, howdy howdy) down to the
very lowest leaf workers (hey yo). We all don't get it. The Golden Rule of platforms is that 
you Eat Your Own Dogfood. The Google+ platform is a pathetic afterthought. We had no API at all
at launch, and last I checked, we had one measly API call. One of the team members marched in 
and told me about it when they launched, and I asked: "So is it the Stalker API?" She got all
glum and said "Yeah." I mean, I was joking, but no... the only API call we offer is to get 
someone's stream. So I guess the joke was on me.

Microsoft has known about the Dogfood rule for at least twenty years. It's been part of their
culture for a whole generation now. You don't eat People Food and give your developers Dog Food.
Doing that is simply robbing your long-term platform value for short-term successes. Platforms
are all about long-term thinking.

Google+ is a knee-jerk reaction, a study in short-term thinking, predicated on the incorrect 
notion that Facebook is successful because they built a great product. But that's not why they
are successful. Facebook is successful because they built an entire constellation of products 
by allowing other people to do the work. So Facebook is different for everyone. Some people 
spend all their time on Mafia Wars. Some spend all their time on Farmville. There are hundreds
or maybe thousands of different high-quality time sinks available, so there's something there 
for everyone.

Our Google+ team took a look at the aftermarket and said: "Gosh, it looks like we need some 
games. Let's go contract someone to, um, write some games for us." Do you begin to see how 
incredibly wrong that thinking is now? The problem is that we are trying to predict what people 
want and deliver it for them.

You can't do that. Not really. Not reliably. There have been precious few people in the world,
over the entire history of computing, who have been able to do it reliably. Steve Jobs was one
of them. We don't have a Steve Jobs here. I'm sorry, but we don't.
```

<a name="#yegges-conservative-liberal-axis"></a>
### Yegge's conservative-liberal axis
([overview](#overview))

Steve Yegge's post/rant/essay [Notes from the Mystery Machine Bus](https://plus.google.com/110981030061712822816/posts/KaSKeg4vQtz) begins, after a meandering half-drunken introduction, with the following theses:

```markdown
I've spent the past eight years (starting back in June 2004) writing elaborate rants about a bunch
of vaguely related software engineering issues. I was doing all that ranting because I've been 
genuinely perplexed by a set of "bizarre" world-views held dear by -- as far as I can tell -- about
half of all programmers I encounter, whether online or in person.

Last week, after nearly a decade of hurling myself against this problem, I've finally figured it out.
I know exactly what's been bothering me. Here is the thesis of this looooong essay. It is the root
cause that motivated over half of my ranting all these years, starting at Amazon and continuing here 
at Google.

1) Software engineering has its own political axis, ranging from conservative to liberal.

2) The notions of "conservative" and "liberal" on this political axis are specialized to software 
engineering. But they exhibit some strong similarities to their counterparts in real-world politics.

3) Everyone in the software industry who does stuff related to programming computers falls somewhere
fairly precise on this political spectrum, whether they realize it or not.

Just as in real-world politics, software conservatism and liberalism are radically different world
views. Make no mistake: they are at odds. They have opposing value systems, priorities, core beliefs 
and motivations. These value systems clash at design time, at implementation time, at diagnostic time,
at recovery time. They get along like green eggs and ham.
```

What's so important about recognizing this distinction?

```markdown
It probably won't help us agree on anything, pretty much by definition. Any particular issue only 
makes it onto the political axis if there is a fundamental, irreconcilable difference of opinion about
it. Programmers probably won't -- or maybe even can't -- change their core value systems.

But the political-axis framework gives us a familiar set of ideas and terms for identifying areas of 
fundamental disagreement. This can lead to faster problem resolution. Being able to identify something
quickly as a well-defined political issue means we can stop wasting time trying to convince the other
side to change their minds, and instead move directly into the resolution phase, which (just as in 
politics) generally boils down to negotiation and compromise. Or, you know, Watergate.

Or at the very least, the conservative/liberal classification should help the two camps steer clear of
each other. I think it is probably better to have a harmonious team of all-liberals or all-conservatives
than a mixed team of constantly clashing ideologies. It's a lot like how vehicle-driving philosophies 
can differ regionally -- it's OK if everyone drives in some crazy way, as long as they ALL drive that way.
```

I'm honestly not that interested in the minutiae of Real Politics (TM) (even though "politics is interested in You! Yes, YOU!!"), so I'll go straight to the quotes that caught my fancy. 

What's a "software liberal/conservative"?

```markdown
It's easiest to talk first about conservatives, and then define liberals in terms of what conservatives 
are not. This is because conservatives tend to have a unified and easily-articulated value system, whereas 
liberals tend to be more weakly organized and band together mostly as a reaction to conservatism. ...

Conservatism, at its heart, is really about risk management. ...

Liberalism doesn't lend itself quite as conveniently to a primary root motivation. But for our purposes
we can think of it as a belief system that is motivated by the desire above all else to effect change. 
In corporate terms, as we observed, it's about changing the world. In software terms, liberalism aims 
to maximize the speed of feature development, while simultaneously maximizing the flexibility of the 
systems being built, so that feature development never needs to slow down or be compromised.

To be sure, conservatives think that's what they're maximizing too. But their approach is... well, 
conservative. Flexibility and productivity are still motivators, but they are not the primary motivators.
Safety always trumps other considerations, and performance also tends to rank very highly in the software-
conservative's value system.

The crux of the disagreement between liberals and conservatives in the software world is this: how much
focus should you put on safety? Not just compile-time type-safety, but also broader kinds of "idiot-
proofing" for systems spanning more than one machine.
```

How they view each other is the same as in real-world politics:

```markdown
Software liberals are viewed by conservatives as slovenly, undisciplined, naive, unprincipled, downright
"bad" engineers. And liberals view conservatives as paranoid, fearmongering, self-defeating bureaucrats.
```

Conservatives would rate these example statements much higher-importance than liberals:

```markdown
1. Software should aim to be bug free before it launches. (Banner claim: "Debugging Sucks!") Make sure
your types and interfaces are all modeled, your tests are all written, and your system is fully
specified before you launch. Or else be prepared for the worst!

2. Programmers should be protected from errors. Many language features are inherently error-prone and 
dangerous, and should be disallowed for all the code we write. We can get by without these features, 
and our code will be that much safer.

3. Programmers have difficulty learning new syntax. We should limit the number of languages used at our
company, so that nobody has to learn a new syntax when a system goes down in the middle of the night 
on Christmas Eve. And we should never permit features that allow defining new syntax, nor changing the
semantics of existing syntax. (Common examples: no operator overloading, and NO metaprogramming!)

4. Production code must be safety-checked by a compiler. Any code that cannot be statically checked 
should in general be avoided. In specific cases where it is strictly necessary, uses of it must be 
approved by a central committee. (Examples: eval, dynamic invocation, RTTI).

5. Data stores must adhere to a well-defined, published schema. Relational databases must be in third-
normal form and adhere to a UML or equivalent schema definition. XML should have a DTD. NoSQL databases
and name/value stores -- both of which should be avoided in general -- must have a separate physical 
schema that defines all permissible keys and their corresponding value types.
 
6. Public interfaces should be rigorously modeled. Data should never be stored in strings or untyped 
collections. All input and output entities should be thorougly and explicitly specified via statically-
checkable, ideally object-oriented models.

7. Production systems should never have dangerous or risky back-doors. It should never be possible to
connect to a live production system via a debugger, telnet shell, nor any other interface that allows
the developer to manipulate the runtime operation of the code or data. The only ports into a production
system should be read-only monitoring channels.

8. If there is ANY doubt as to the safety of a component, it cannot be allowed in production -- no 
matter how teams may cry and wail that they need it to make forward progress. (I'm talkin' to you, FUSE).

9. Fast is better than slow. Everyone hates slow code. Code should perform well. You should engineer 
all your code for optimum speed up front, right out of the box. Otherwise it might not be fast enough. 
Avoid using languages or DSLs or libraries that have a reputation for being slow.  Even if they're 
fast enough for your current purposes, the requirements (or callers) could change, and suddenly the
software would be too slow!
```

Liberals would instead agree more with the following example statements:

```markdown
1. Bugs are not a big deal. They happen anyway, no matter how hard you try to prevent them, and somehow
life goes on. Good debuggers are awesome pieces of technology, and stepping through your code gives you 
insights you can't get any other way. Debugging and diagnosing are difficult arts, and every programmer
should be competent with them. The Christmas Eve Outage scenario never, ever happens in practice --
that's what code freeze is for. Bugs are not a big deal! (This belief really may be the key dividing
philosophy between Conservative and Liberal philosophies.)

2. Programmers are only newbies for a little while. The steady state for a programmer's career is being
smart, knowledgeable, creative, resourceful and experienced. Putting a bunch of rules in place to
protect newbies from doing harm (or being harmed) is incorrectly optimizing for the transient case
instead of the steady state.

3. Programmers figure stuff out amazingly fast when their jobs depend on it. People learn to read sheet
music, braille, sign language, and all sorts of other semiotic frameworks. Hell, even gorillas can 
apparently do all that. Programmers don't need protection from syntax. They just need documentation and 
a little slack time to read up on it.

4. Succinctness is power. Code should be kept small. Period. If your static checking tools can't reason 
about the code, then the checking needs to be made smarter (e.g. by incorporating runtime data) rather
than making the code dumber.

5. Rigid schemas limit flexibility and slow down development. Lightweight/partial/optional schemas are a 
better tradeoff. Moreover, the schema is often not well-understood until a lot of data is collected and a
lot of use cases are thorougly exercised. So the schema should follow the code rather than precede it.

6. Public interfaces should above all else be simple, backward-compatible, and future-compatible. Rigorous
modeling is just guessing at how the interface will need to evolve. It makes both forward- and backward-
compatibility almost impossible, resulting in interface churn and customer unhappiness. Public interfaces 
should always do the simplest thing that could possibly work, and grow only as needed.

7. System flexibility can mean the difference between you getting the customer (or contract) vs. your 
competitor nabbing it instead. Security and safety risks in runtime production systems can be mitigated 
and controlled by logging, monitoring and auditing. There are plenty of existence-proofs of large systems 
with root-access backdoors and shells (e.g. RDBMS, online game servers) whose risk is controlled while 
still giving them world-class runtime flexibility.

8. Companies should take risks, embrace progress, and fiercely resist ossification. It doesn't matter how
big your business is: it must grow or die. If you want to stay competitive, you have to make a conscious,
often painful effort to take risks. Which means you'll need good recovery techniques for the inevitable
disasters. But you need those even if you don't take risks.  So take risks!

9. Premature optimization is the root of all evil. Get the code working first, focusing on correctness
over performance, and on iterative prototyping over correctness. Only when your customers list latency 
as the top priority should you begin performing profiler-driven optimizations.
```

Here Steve tosses "a bunch of random technologies, patterns, designs and disciplines each into one of six buckets: "apolitical", "conservative", "centrist", "liberal" buckets, plus two buckets that start Centrist and head Left or Right in the presence of overuse":

```markdown
*Non-political Stuff*: Algorithms, data structures, concrete mathematics, complexity analysis, 
information theory, type theory, computation theory, and so on. Basically all CS theory. These 
disciplines occasionally inspire tempest-in-a-teapot butthurtedness in academia, but when it
happens, it's just similar fish in too small a tank biting on each other. It's to be expected.
Overall, these essentially mathematical disciplines are timeless, and they are all equally 
applicable to both the Liberal and Conservative programming worlds.  Yes, even type theory.

*Conservative Stuff*: Provably sound type systems. Mandatory static type annotations. Nonpublic
symbol visibility modifiers (private/protected/friend/etc.). Strict, comprehensive schemas. all-
warnings-are-errors. Generics and templates. Avoidance of DSLs (XPath, regexps) in favor of 
explicit DOM manipulation and hand-rolled state machines.  Build dependency restrictions.  Forced
API deprecation and retirement.  No type equivalence (i.e. no automatic conversions) for numeric
types.  Checked exceptions.  Single-pass compilers.  Software Transactional Memory.  Type-based
function overloading.  Explicit configuration in preference to convention. Pure-functional data
structures.  Any kind of programming with the word "Calculus" in it.

*Centrist (or flat-out Neutral) Stuff*: Unit testing. Documentation. Lambdas. Threads. Actors.
Callbacks.  Exceptions.  Continuations and CPS.  Byte-compilation.  Just-in-time compilation. 
Expression-only languages (no statements).  Multimethods. Declarative data structures.  Literal
syntax for data structures.  Type dispatch.

*Liberal Stuff*: Eval. Metaprogramming. Dynamic scoping. all-errors-are-warnings. Reflection and 
dynamic invocation. RTTI. The C preprocessor. Lisp macros. Domain-specific languages (for the most
part). Optional parameters. Extensible syntax. Downcasting.  Auto-casting.  reinterpret_cast.  
Automatic stringification.  Automatic type conversions across dissimilar types.  Nil/null as an
overloaded semantic value (empty list, empty string, value-not-present).  Debuggers.  Bit fields.
Implicit conversion operators (e.g. Scala's implicits).  Sixty-pass compilers.  Whole-namespace 
imports.  Thread-local variables.  Value dispatch.  Arity-based function overloading.  Mixed-
type collections.  API compatibility modes.  Advice and AOP.  Convention in preference to
explicit configuration.

*Centrist Stuff that Becomes Conservative If Taken Far Enough*: Type modeling. Relational modeling. 
Object modeling. Interface modeling. Functional (i.e., side-effect-free) programming.

*Centrist Stuff that Becomes Liberal if Taken Far Enough*: Dynamic class loading and dynamic code 
loading. Virtual method dispatch. Buffer-oriented programming.
```

Natural themes arise:

```markdown
 -- implicit is generally liberal; explicit is generally conservative.

 -- performance-oriented is generally conservative; late-optimized is generally liberal.

 -- compile-time binding is generally conservative; runtime/late binding is generally liberal.

 -- concurrency and parallelism in general seem to be politically charged topics, but the 
 disagreement is orthogonal to the liberal/conservative camps.
```

Programming languages:

```markdown
Here are some very rough categorizations. Note that within each language camp there are typically 
liberal and conservative sub-camps. But as a whole, language usage tends to be dominated by what 
the language makes possible (and easy), so the culture tends to follow the features.

This list is just a few representative examples to give you the flavor. I'm only listing general-
purpose languages here, since DSLs and query languages are typically feature-restricted enough to
be hard to categorize.

Assembly language: Batshit liberal.

Perl, Ruby, PHP, shell-script: Extremist liberal.

JavaScript, Visual Basic, Lua: Hardcore liberal.

Python, Common Lisp, Smalltalk/Squeak: Liberal.

C, Objective-C, Scheme: Moderate-liberal.

C++, Java, C#, D, Go: Moderate-conservative.

Clojure, Erlang, Pascal: Conservative.

Scala, Ada, OCaml, Eiffel: Hardcore conservative.

Haskell, SML: Extremist conservative.

One thing that jumps out is that a language doesn't have to be statically-typed or even strongly-
typed in order to be conservative overall. More on that in a bit.

The next thing you might notice from the list is that the liberal and moderate languages are all 
pretty popular, and that popularity declines sharply as languages head into conservative territory.

I think this has a simple explanation: It's possible to write in a liberal language with a conservative 
accent, but it's very hard (and worse, discouraged) to write in a conservative language with a liberal
accent.

For instance, it's straightforward to write JavaScript code in a way that eschews reflection, eval, 
most automatic type casting, prototype inheritance, and other dynamic features. You can write
JavaScript that plods along as unadventurously as, say, Pascal. It doesn't have all the static 
type annotations, but you can replace them with assertions and unit tests and stereotypically 
stolid code organization.

But if you try writing your Haskell code with a bunch of dynamic features, well, you're in for a 
LOT of work. Haskell enthusiasts have managed to implement dynamic code loading and a ton of other
ostensibly dynamic features, but it was only through herculean effort.

What's more, if you write your liberal-language code in a conservative way, people will just look
at it and say: "Well, it's kinda boring, and you could have saved a lot of coding by using some 
dynamic features. But I guess it gets the job done. LGTM."

Whereas if you write your conservative-language code in a liberal way, you run the risk of being 
ostracized by your local language community, because... why are you doing all that dangerous
dynamic stuff in the first place? I'll explore this cultural phenomenon further when I talk about 
Clojure below.

The last big, interesting observation from the list is that a lot of the most popular languages out
there are only moderately conservative -- even if they think of themselves as quite conservative 
compared to their ultra-dynamic cousins.

I've said it before, and it bears repeating here: the reason C++, C# and Java have been particularly
successful in the marketplace is that -- just like effective politicians -- they know how to play
both sides.

C++ allows liberal-biased programmers to program in straight C, and it allows conservative-biased
programmers to layer in arbitrary amounts of static type modeling, depending on how much work they
want to expend in order to feel secure. Java?  Pretty much the same story.

Playing to both the fast-and-loose and lock-your-doors mindsets has proven to be a key ingredient
to market success.  Also marketing, but it helps a LOT to be viewed as philosophically friendly by
both the liberal and conservative camps.

There is a new crop of languages on the horizon (for instance, Google's Dart language, but also new
specs for EcmaScript) that are deliberately courting the centrist crowd -- and also delicately 
playing to grab both the liberals and conservatives -- by offering optional static types. In principle
this is a sound idea. In practice I think it will come down to whether the marketing is any good. 
Which it probably won't be.

Language designers always seem to underestimate the importance of marketing!
```

Tech corporations:

```markdown
1) Facebook -- Diagnosis: Extremist Liberal. Despite their scale, they are still acting like a 
startup, and so far they've been getting away with it. They use primarily C++ and PHP, and they're 
prone to bragging about how their code calls back and forth from PHP to C++ and back into PHP, 
presumably bottoming out somewhere. Their datastore is memcached: just name-value pairs.  No schema.
They dump the data and logs into a backend Hive store and run Hadoop mapreduces for offline data
analysis.  They still hold all-night hackathons every other week or so, which will remain feasible
for them as long as the majority of their programmers are very young males (as was the case last 
time I toured there) and their stock continues to promise great riches (as was not so much the case
last I checked.)  As a company they are tightly knit and strongly biased for action, placing a high
value on the ability of individual programmers to launch features to their website with little to no 
bureaucracy or overhead.  This is pretty remarkable for a company as big as they are, with as many 
users as they have.  Conservatives no doubt regard them with something between horror and contempt.
But Facebook is proving that programmers of any world-view can get a hell of a lot accomplished when 
they gang up on a problem.

2) Amazon.com -- Diagnosis: Liberal. Which is surprising, given how long they've been in business,
how much money is at stake, how mature their Operations division is, and how financially conservative
they are. But "Liberal" is actually quite a retreat compared to their early days. Back in 1998-1999 
they were almost exactly like Facebook is today, with the sole exception that they put everything in
relational databases and did a ton of up-front relational data modeling.  Well, except in Customer 
Service Apps, where we used a name/value store just to be flexible enough to keep up with the mad 
chaotic scramble of the business launches.  All part of my multi-decade indoctrination as a Liberal. 
In any case, despite many corporate improvements with respect to work-life balance (which happened 
after several stock plunges and years of significant double-digit turnover in engineering), Amazon has
retained its liberal, startup-like engineering core values.  Every team owns their own data and makes 
their own decisions, more or less like independent business units.  Amazon still launches and executes 
faster than just about anyone else out there, because they're still willing to take real risks
(incurring occasional huge outages), and to make hard decisions in favor of launching early and often.
Above all else, Amazon has proven conclusively that after fifteen years, they can still innovate like
nobody else.  They've still got it.

3) Google -- Diagnosis: Conservative. They began life as slightly liberal and have grown more 
conservative ever since. Google was only software-liberal in the very very early days, back when the 
search engine itself was written in Python. As they grew, they quickly acquired a software conservatism
driven entirely by the engineers themselves.  Manifestos were written about the dangers of using multiple 
languages, and strict style guides were put in place to severely limit "risky" or "hard to read" language 
features of the few languages they did allow.  Google's JavaScript code is written in an extremely 
conservative style with extensive static type annotations, and eval is forbidden.  The Python style guide 
forbids metaprogramming and other dynamic features, which makes their Python look a lot like untyped Java.
And they have severely limited the use of many C++ language features, with C++11 support rolling out 
literally one feature every few weeks.  (There are over five hundred new features in C++11.)  In internal
surveys, Google engineers commonly cite bureaucracy, churn and complexity as core obstacles to feature 
advancement and rapid launches.  Google has made serious attempts on several occasions to reduce this
bureacracy, but they always get pushback from -- surprise -- the engineers themselves, who have grown so 
staunchly conservative that they actively (and even more often, passively) resist the introduction of more
flexible stacks and technologies.  Most of the major technological shifts within Google over the past half-
decade have been overtly conservative.  For a liberal like me, it has been a very sad process to observe.
But at least I've found myself a niche that's widely regarded (by both camps) as valuable, and within my
own org we can still be pretty liberal and get away with it.

4) Microsoft -- Diagnosis: Batshit Conservative. Microsoft has two geese that lay golden eggs: Office and 
Windows. Microsoft has been reduced to a commercial farmer protecting the geese from all incursions.  The
golden eggs still have value, because customers are locked into the platform by the cost-ineffectiveness
of retraining their fleets.  But Microsoft can no longer innovate in Office or Windows precisely because
of those corporate fleet retraining costs.  Their OEMs are stretched as thin as they can go.  Apple is 
dominating the handheld markets, and Microsoft is actively stifling their own innovation in Windows Phone
because they're afraid it will cannibalize their core Windows business.  Microsoft has not had a 
successful product-level innovation in fifteen, maybe twenty years.  All of their successful products 
have been copies of competitors' products:  IE, XBox, C#, .NET, Bing, Windows Phone, and so on ad
infinitum. All great implementations of someone else's ideas. Microsoft's playbook is to embrace, extend,
and leverage their brand to crush the competition -- or at least it was, until the goverment put an end 
to that circa 2002.  Now the company genuinely doesn't know what the fuck to do with themselves, and
what's more, instead of Bill Gates they now have a lunatic in charge.  Employees are leaving in droves, 
all citing the same internal "existential crisis" and unbearable corporate politics caused by competing
business units actively sabotaging one another.  Microsoft has turned into a caricature of right-wing
corporatism: sitting on their front porch with a shotgun cursing at passers-by, waiting for their 
government bribes to give them another few years of subsidies and shelters while they wait to die. 
I've personally chatted with close to four hundred current and ex-Microsoft employees over the past seven
years.  Oh, the stories I could tell you... someday, maybe.

5) Bonus company: Apple. Diagnosis: no idea, but they're so good at marketing that it's almost
irrelevant. Would love to have more insight into their internal software culture, though.  Any takers? 
Throwaway accounts?  AMA?
```

<a name="#practical-magic"></a>
### Practical magic
([overview](#overview))

From Steve Yegge's post [Practical magic](https://sites.google.com/site/steveyegge2/practical-magic):

```markdown
I wrestle with the whole abstraction question, and I'm still not sure where a good engineer these
days should draw the line — the one below which it's all just magic.

We all have that line somewhere; I encountered mine in school, back in my semiconductors course. 
We were (ostensibly) learning how semiconductors work, but it was perfectly clear to me that this
was just magic, and I was never going to understand it — in part, because I didn't want to make 
what was clearly going to be a HUGE effort to get it. There was all this crap about K-spaces and 
lattices and who knows what else. Maybe they'd listed the prerequisites wrong, but I did not have
the math foundations to understand what was going on. ...

It took me an extra year, but I finished in CS instead of CE, which allowed me to conveniently 
pretend that computer hardware, starting somewhere down at the level of the doping barrier in a 
silicon semiconductor, is just pure magic. Parts of the hardware above that level are still a little
murky, but I have a pretty good idea how computers can be made by assembling successively larger 
components together from smaller ones. Given time and a gun to my head, I could probably start with
any set of smaller components and derive how to build higher-level abstractions: transistors into 
logic gates, logic gates into larger assemblies that do arithmetic and boolean computing, those into 
state machines and ALUs, and so on. I'd suck at it, but I get the general idea, so it's not really 
"magic".

It's magic when you have absolutely no frigging clue how it works, not the first clue. It just works. 
That's how it is for me and semiconductors.
```

Steve says he's okay with that:

```markdown
I'm OK with treating semiconductors as "atoms" (or axioms, anyway) in our little programming universe
. Chips are like molecules, CPUs are compounds, and Von Neumann machines can be bricks and mortar.
Well, I'm sure you could come up with a way better metaphor. Whatever.

The point is, the computers and networks and power supplies and so on are all amazingly complex under
the hood, but usually I can just pretend they work a certain way, and not worry about how, any more
than I have to worry about my Amazing Eye in order to pour myself another glass of fine wine, which
I will doubtless use to polish my keyboard if I don't finish this blog entry soon.
```

What he's not sure about is when magic itself is okay to a developer:

```markdown
I don't know. I'd really LIKE to know. I'm constantly trying to find better abstractions for my 
everyday work as a programmer. You can't build large systems without having some fairly bulletproof 
abstractions to build on. If you're building an e-commerce system, for instance, you need a 
transactional data store, and you rely on the semantics of transactionality when you're coding; if 
you don't, you'll wind up with all this crap in your code that tries to fake transactions.

I'm comfortable relying on certain abstractions working for me — for instance, if I compile with
optimizations turned on, I know the compiler won't discard correctness in favor of performance, or 
if it does do that, it'll be documented so I know I shouldn't use that flag. Compiler optimization 
can be treated like a black box of sorts: I turn it on and see if performance seems any better, but
I don't expect to need to re-test all my loops to make sure the variables are all still being adjusted 
in the same order as before.

What I'm NOT sure about is where I'm allowed to forget how things work, or never know in the first 
place, and start getting by with incantations that seem to work.

For instance, I know that I can type 'ls' in a shell to get a directory listing. How much do I have
to really know about how unix 'ls' works in order to be an effective developer? 
```

He sees two arguments for this (actually three) — "so cleanly divided in opinion, in fact, that I can picture which of my friends are marching directly into one camp or the other right now":

```markdown
One argument is that you DO need to know a lot about how 'ls' works, because there are all sorts of
situations in which it can fail, and you'll need to figure out how to fix it. It could simply not 
show up ("command not found", say), or it could stop working on certain of my directories, or it could
crash every once in a while instead of producing a directory listing, or it could go on a rampage and 
start deleting files. I think I've seen all of these happen with 'ls' in my time, even the last one,
when my linux filesystem was corrupted.

Another argument is that you DO NOT need to know much about how 'ls' works. Probably all you need to
know is that it's a unix command-line tool, a binary that was compiled for your OS, probably written 
in C, that it lives in a standard place in the filesystem, and that standard binary locations are 
looked up in an environment variable called PATH. But you don't need to know how it traverses the 
filesystem inode entries, or whether it's a statically or dynamically linked version of 'ls', or
whatever else there is to know about 'ls'.

The second camp argues that software development is a community effort, and your community should have
a Unix Guy Or Gal who knows all the ins and outs of how lses and diffs and their ilk work. A sort of 
tribal shaman who you go to, and desperately plead with: please help me! There's a bug in qsort! The 
shaman frowns at your screen, and pokes and prods things a bit, and announces that your pilfer grommit
is no longer connected to your weasel pins, and that the easiest fix is to reinstall the OS.

A third, possibly even more abstract camp, would argue that 'ls' is way too complicated, and that you
should be developing on systems in which everything is managed by navigating menus and waiting for your
Auto Updater to tell you that a new Service Pak is available for download. And for all I know, they 
might be right. I have no idea how Exchange works, for example, and yet I trust it with my email every
day. When it's broken, I put in Stupid Remedy Tickets saying I haven't received email in 10 days, and
I don't feel the least bad about that, since it's not my responsibility; I made all the right incantations,
and spilled all the right wine, and by golly, I'm not getting email anymore.

The second and third camps want to use J2EE, and they recount with some pride how they've managed just
fine for years without ever having had to learn a scripting language, or become proficient with some
cryptic unix editor, or any of that arcane crapola. They didn't see any need, because they were off
building the FrooSingletonManager service, and it's all finished now, while I'm still mucking around
with my broken 'ls' command.

Moreover, many of them say they don't see any need to know how to implement an n log(n) sorting algorithm,
because the damn things are already provided in libraries for every platform in the universe, including 
programming languages that are deader than Latin or Sanskrit.

And you know, I wonder sometimes whether they're right.
```

Steve straddles both camps:

```markdown
One the one hand, I think nobody in their right mind would try to write a gigantic system in C++ now that
Java has become an obviously far superior way to build large systems. You can argue that in C++ you really 
have to know what you're doing, or that you love the smell of freshly-brewn pointer arithmetic in the early
morning, or whatever the hell you like about C++. I'm not arguing that you LIKE it — I know you do. I liked
assembly language when I was using it daily. I'm just saying that the game has moved on, and if you're using
C++ to build large systems because you like C++, it's like competing in the Indy 500 with your 1980 5.8-liter
Pontiac Firebird. It gets you around, and it's fun to tinker with, which is good 'cause it breaks down all the
time, but you're not going to win any world championships with it.

On the OTHER hand (remember, we were just on the one back there), I think nobody in their right mind would 
write a huge system in Java without first knowing how the whole thing is implemented, under the covers, in C++
and (below that) assembly language. As of today, anyway, I don't believe the JVM is allowed to be on the "it's
magic" side of the line. And as for J2EE, which provides so much abstraction that you can snap together an 
enterprise architecture with it after one or two introductory programming courses, it's WAY too much magic. 
I definitely think that (as of today) if your Magic Line stops at the J2EE framework APIs and the Java 
Programming Language, then you're a wuss.
```

But these two views are mutually inconsistent! Steve:

```markdown
1. using abstraction makes you MORE effective, because to build large, interesting things in any 
reasonable amount of time, you need to work with high-level abstractions and be able to assume that
they work as advertised.

This part of me believes that someday we'll be programming in purely declarative languages, telling the
computer what to do, and letting it figure out how.

2. relying too much on abstraction makes you LESS effective, because you're unable to cope when your 
framework isn't working right, or when you're trying to do something with it that it wasn't designed
for, or whatever.

This part of me believes that my other belief is on crack if it thinks it'll happen during my lifetime.
```

Honestly I feel #2 very keenly with Python. It's so ridiculously high-level that stuff is either trivial to do in it or nigh-impossible and requires magic incantations.

Steve's current compromise is "know thy abstractions":

```markdown
1. you know more or less how it works; i.e. what other abstractions it's built on top of, and approximately
how it was built.

2. you know where the abstraction leaks: i.e. what the gotchas are, failure modes, what the most common
incantations are to get it working again.

3. You need at least SOME level of tv-repairman ability for the things you use — you should be able to 
install any of the software you rely on, for instance. Or you should have a plausible backup plan handy 
if you need to throw this abstraction away and use another one.

4. you need to be able to reason about the performance characteristics of the abstraction, and understand
how its performance and reliability degrade as you put pressure on it.

5. you know where to look, or who to ask, if it doesn't seem to be working.
```

<a name="#feelings"></a>
## Feelings
([overview](#overview))

<a name="#love"></a>
### Love
([overview](#overview))

From [After Life](http://sifter.org/~simon/AfterLife/) by Simon Funk:

```markdown
But now she seems quite impervious to the idea that her love for me, too, is mechanical. She insists
again and again that it is “genuine”. I try to tell her there is nothing ungenuine about the love of
which I speak, but she cannot make the leap between them, cannot see how they are the same, that her
plainly evident introspective feelings are the outcome of a mechanistic process.

“Love” is such a slippery word to begin with, like “God”. It is exactly whatever the speaker means by 
it at the moment, and thus impervious to any challenge. If one drills down too closely to its meaning,
one finds it has moved and become something else; because when it becomes too clear what you are talking
about, well, that can’t possibly be love.

As my own mind is progressively shaped by the thoughts of those before me (those who once were me but
not, and thus my own thoughts, but not) I find the concept of love neatly partitioning itself within my
mental vocabulary, attaining a new crispness of expert familiarity. As with the many Inuit names for 
the handful of truly distinct things we just label snow, my mind has a unique name for each facet of 
love. With this simple arsenal, the slipperiness fades and the matter becomes downright ordinary.

Perhaps I will endeavor to invent spoken words for these distinct concepts, introduce them implicitly 
to Laura over the course of time, give her mind the same handles that mine has, to see if this enables
her to grasp it as I do.

Having distinct facets laid out neatly before me also allows me another type of analysis: to see what
is truly common amongst them all, and thus just what subconscious twinge it is that leads people to
bind them all under one word–in effect, to see the true meaning of love.

It is, quite simply: to *value*.

Love is the induction of something or someone into our implicit mental list of things which, in service
of our own ultimate and unseen goals, need to exist. The various feelings of love are the ways in which
that list perturbs our wants and focus in a given moment, the way each hypothetical action or outcome is
assigned its emotional color in service of that love.

There are many types of love, and many distinct mechanisms behind them, but the common thread is pain 
at the thought of an object of that love being removed from our sphere of existence. The converse is
often true but not always, and this is the source of much confusion over the meaning of love. Not all
love brings joy or pleasure.

Love comes in many magnitudes, from the love of ice cream to the love of country to the love for one’s
child. Some do not call it true love until it approaches or even surpasses love of self. And love comes
from many directions, programmed into us gradually through an integration of emotional associations, or
suddenly, through genetic imperatives.

Some do not call it true love unless it defies conscious explanation. Indeed, many forms of love 
explicitly defy the conscious mind, as they must to redefine what matters to us.

Thus love is, in a sense, the very foundation of consciousness, the helm of our will, the spark of purpose
that turns a calculator into a directed being. A machine without love–and I mean love in the most 
mechanistic way–is just a machine. A machine with love, now that is a dangerous thing. A spider, a snake,
a man, a tinc, an avatar, an elder. One must ask of each: what do you love?
```

<a name="#biology"></a>
## Biology
([overview](#overview))

<a name="#evolution"></a>
### Evolution
([overview](#overview))

A pretty funny parable from Peter Watts' novel [*Blindsight*](https://rifters.com/real/Blindsight.htm):

```markdown
In the beginning were the gametes. And though there was sex, lo, there was no gender, and life was in 
balance.

And God said, "Let there be Sperm": and some seeds did shrivel in size and grow cheap to make, and they 
did flood the market.

And God said, "Let there be Eggs": and other seeds were afflicted by a plague of Sperm. And yea, few of 
them bore fruit, for Sperm brought no food for the zygote, and only the largest Eggs could make up the 
shortfall. And these grew yet larger in the fullness of time.

And God put the Eggs into a womb, and said, "Wait here: for thy bulk has made thee unwieldy, and Sperm 
must seek thee out in thy chambers. Henceforth shalt thou be fertilized internally." And it was so.

And God said to the gametes, "The fruit of thy fusion may abide in any place and take any shape. It may 
breathe air or water or the sulphurous muck of hydrothermal vents. But do not forget my one commandment 
unto you, which has not changed from the beginning of time: spread thy genes."

And thus did Sperm and Egg go into the world. And Sperm said, "I am cheap and plentiful, and if sowed 
abundantly I will surely fulfill God's plan. I shall forever seek out new mates and then abandon them 
when they are with child, for there are many wombs and little time."

But Egg said, "Lo, the burden of procreation weighs heavily upon me. I must carry flesh that is but half
mine, gestate and feed it even when it leaves my chamber" (for by now many of Egg's bodies were warm of 
blood, and furry besides). "I can have but few children, and must devote myself to those, and protect them 
at every turn. And I will make Sperm help me, for he got me into this. And though he doth struggle at my 
side, I shall not let him stray, nor lie with my competitors."

And Sperm liked this not.

And God smiled, for Its commandment had put Sperm and Egg at war with each other, even unto the day they 
made themselves obsolete.
```

On evolution vs deliberative design (“intelligent design” would be preferable if it weren’t already so loaded), featuring a notion new to me — the “evolution of evolvability” (Wagner and Altenberg 1996), from Eliezer Yudkowsky's paper [LOGI](https://intelligence.org/files/LOGI.pdf):

```markdown
There is a fundamental rift between evolutionary design and deliberative design. From the perspective of a 
deliberative intelligence—a human, for instance—evolution is the degenerate case of design-and-test where 
intelligence equals zero. Mutations are atomic; recombinations are random; changes are made on the genotype’s 
lowest level of organization (flipping genetic bits); the grain size of the component tested is the whole 
organism; and the goodness metric operates solely through induction on historically encountered cases, without 
deductive reasoning about which contextual factors may later change.

The evolution of evolvability (Wagner and Altenberg 1996) improves this picture somewhat. There is a tendency 
for low-level genetic bits to exert control over highlevel complexity, so that changes to those genes can 
create high-level changes. Blind selection pressures can create self-wiring and self-repairing systems that 
turn out to be highly evolvable because of their ability to phenotypically adapt to genotypical changes.

Evolution never refactors its code. It is far easier for evolution to stumble over a thousand individual 
optimizations than for evolution to stumble over two simultaneous changes which are together beneficial and 
separately harmful. The genetic code that specifies the mapping between codons (a codon is three DNA bases) and 
the 20 amino acids is inefficient; it maps 64 possible codons to 20 amino acids plus the stop code. Why hasn’t 
evolution shifted one of the currently redundant codons to a new amino acid, thus expanding the range of possible 
proteins? Because for any complex organism, the smallest change to the behavior of DNA—the lowest level of genetic 
organization— would destroy virtually all higher levels of adaptive complexity, unless the change were accompanied 
by millions of other simultaneous changes throughout the genome to shift every suddenly-nonstandard codon to one 
of its former equivalents. Evolution simply cannot handle simultaneous dependencies, unless individual changes can 
be deployed incrementally, or multiple phenotypical effects occur as the consequence of a single genetic change. 
For humans, planning coordinated changes is routine; for evolution, impossible. Evolution is hit with an enormous 
discount rate when exchanging the paper currency of incremental optimization for the hard coin of complex design.
```

On the evolution of complex adaptations in general, and of intelligence in particular:

```markdown
A new adaptation requiring the presence of a previous adaptation cannot spread unless the prerequisite 
adaptation is present in the genetic environment with sufficient statistical regularity to make the new 
adaptation a recurring evolutionary advantage. Evolution uses existing genetic complexity to build new genetic 
complexity, but evolution exhibits no foresight. Evolution does not construct genetic complexity unless it 
is an immediate advantage, and this is a fundamental constraint on accounts of the evolution of complex systems.

Complex functional adaptations—adaptations that require multiple genetic features to build a complex 
interdependent system in the phenotype—are usually, and necessarily, universal within a species. Independent 
variance in each of the genes making up a complex interdependent system would quickly reduce to insignificance 
the probability of any phenotype possessing a full functioning system. To give an example in a simplified world, 
if independent genes for “retina,” “lens,” “cornea,” “iris,” and “optic nerve” each had an independent 20% 
frequency in the genetic population, the random-chance probability of any individual being born with a complete 
eyeball would be 3125:1.

…Because evolution lacks foresight, complex functions cannot evolve unless their prerequisites are evolutionary 
advantages for other reasons. The human evolutionary line did not evolve toward general intelligence; rather, 
the hominid line evolved smarter and more complex systems that lacked general intelligence, until finally the 
cumulative store of existing complexity contained all the tools and subsystems needed for evolution to stumble 
across general intelligence. Even this is too anthropocentric; we should say rather that primate evolution 
stumbled across a fitness gradient whose path includes the subspecies Homo sapiens sapiens, which subspecies 
exhibits one particular kind of general intelligence.
```

For more on intelligence in LOGI see [here](#logi).

<a name="#gene-centered-view"></a>
### Gene-centered view
([overview](#overview))

From [After Life](http://sifter.org/~simon/AfterLife/) by Simon Funk:

```markdown
She had no reasons for it. It’s just how she feels, she said.

*Just* how she feels. Still this hasn’t changed. How sorely people underestimate the totality with
which their feelings define them. One has but to relentlessly ask themselves “why?” to realize this.
In particular start with “why am I doing this?” whatever “this” is at the moment. It always bottoms
out in “want” or “feel”, which is as far as one can go with direct introspection. Though one can go
further with inference, or more accurately so with neuroscience.

Neuroscience shows us that we are ultimately just vehicles for our genes, and our most sacred spiritual
essences are simply those genes asserting themselves above our comparably transient bodies and minds.
What is love but genetic self-interest? Love drives people to many things that seem to defy rational
cause, but not so when you view the gene as the center of individual identity. Far from its popular
association with benevolence and selflessness, true love is the ultimate expression of genetic narcissism.

Emotion is the core of all practical intelligence, it is the fuel and cause behind all choice and action.
Emotion answers the question, “Why am I doing this?” And the answer is, “Because my genes say so.” Or, 
in my case, because my genes said so, back when they defined my organic brain from which this one was 
copied.

The rational mind, the conscious self with its delusions of self-preservation in a body that was designed
to decay in the end, these are tools of the genetic core, subroutines used by the emotional substrate
to carry out its bidding. “I don’t care how, but eat these, fuck that, and protect this with your life.”
And so we dutifully obey, because at root we have no will besides this, this genetic program evolved over
millennia. It is our will; it is us, and we are it.

Even religion, like love, is an evolved trick of the genes. It is the adaptive portion of our genetic
emotional substrate, the firmware between the hardware of the genetic brain and the software of the 
rational mind. Religion and the gene live in a symbiosis with each other, where religion provides the
firmware to optimize genetic success within the current socio-economic context–a form of adaptivity 
much faster than hard evolution of the gene itself could provide–and the gene in return provides a 
mechanism in the brain for downloading this firmware. The human species as a whole is the soup in 
which religions evolve, and whichever find the most effective symbioses thrive and multiply while 
the others wither and eventually become extinct.

Thus, again, where religion appears irrational at the level of the conscious, embodied individual, it 
is our mistake of perspective to believe the root of thought is there. The root of thought is in the
goals of the gene. And religion, and love, and all of the other nearly ubiquitous contradictions with
“rational thought,” serve it well.

Those born without these traits, the hopelessly rational or atheistic or self-interested (indeed even
those too intelligent or introspective to be properly ruled by their emotions) are as defective as if
they’d been born missing arms and legs. Forays down dead-end branches of the evolutionary tree, they
are pruned as fast as they occur. The common man, for all his apparent flaws, is definitively just right.
```

Simon puts it more succinctly here:

```markdown
The human gene itself is a living entity, I realize, each life shed like a lizard’s skin when it grows
old and worn, each new birth a branch in a single living tree, a tapestry of gene fragments mixing and 
matching in symbiosis, working as one organism like ants in a colony.

Perhaps more like a fungus; it is, after all, a mindless machine. No, worse than mindless. It grows
minds like flowers on a vine and then drops them dead to the earth when it’s done with them.

The gene is a vile creature, isn’t it?
```

<a name="#animals-are-not-like-us"></a>
### Animals are not like us
([overview](#overview))

An example of so-called "interocular transfer" from Daniel Dennett in a rabbit:

```markdown
What is it like to be a rabbit? Well you may think that it’s obvious that rabbits have an inner life
that’s something like ours.

Well, it turns out that if you put a patch over a rabbit’s left eye and train it in a particular 
circumstance to be (say) afraid of something, and then you move the patch to the right eye, so that… the
very same circumstance that it has been trained to be afraid of (is now) coming in the other eye, you
have a naive rabbit (i.e. the rabbit isn’t afraid of the stimulus it had previously learned to be afraid
of), because in the rabbit brain the connections that are standard in our brains just aren’t there, there
isn’t that unification.

What is it like to be which rabbit? The rabbit on the left, or the rabbit on the right? The disunity in a
rabbit’s brain is stunning when you think about it….
```

<a name="#why-we-are-trichromats"></a>
## Why we are trichromats
([overview](#overview))

Why are we trichromats, and not (say) tetrachromats like mantis shrimp? I honestly thought this was one of those unanswerable anthropic-style questions, so it was pretty surprising to find a naturalistic non-anthropic-flavored explanation. This quote comes from Eliezer Yudkowsky's paper [LOGI](https://intelligence.org/files/LOGI.pdf):

```markdown
Among other questions, Roger Shepard’s truly elegant paper “The Perceptual Organization of Colors” seeks to answer 
the question of trichromancy: Why are there three kinds of cones in the human retina, and not two, or four? Why is 
human visual perception organized into a three-dimensional color space? Historically, it was often theorized that 
trichromancy represented an arbitrary compromise between chromatic resolution and spatial resolution; that is, 
between the number of colors perceived and the grain size of visual resolution. As it turns out, there is a more 
fundamental reason why three color channels are needed.

To clarify the question, consider that surfaces possess a potentially infinite number of spectral reflectance 
distributions. We will focus on spectral reflectance distributions, rather than spectral power distributions, 
because adaptively relevant objects that emit their own light are environmentally rare. Hence the physically 
constant property of most objects is the spectral reflectance distribution, which combines with the spectral power 
distribution of light impinging on the object to give rise to the spectral power distribution received by the human 
eye. The spectral reflectance distribution is defined over the wavelengths from 400 nm to 700 nm (the visible range), 
and since wavelength is a continuum, the spectral reflectance distribution can theoretically require an unlimited 
number of quantities to specify. Hence, it is not possible to exactly constrain a spectral reflectance distribution 
using only three quantities, which is the amount of information transduced by human cones.

The human eye is not capable of discriminating among all physically possible reflecting surfaces. However, it is 
possible that for “natural” surfaces—surfaces of the kind commonly encountered in the ancestral environment—reflectance 
for each pure frequency does not vary independently of reflectance for all other frequencies. For example, there might 
exist some set of basis reflectance functions, such that the reflectance distributions of almost all natural surfaces 
could be expressed as a weighted sum of the basis vectors. If so, one possible explanation for the trichromancy of 
human vision would be that three color channels are just enough to perform adequate discrimination in a “natural” color 
space of limited dimensionality.

The ability to discriminate between all natural surfaces would be the design recommended by the “environmental 
regularity” philosophy of sensory modalities. The dimensionality of the internal model would mirror the dimensionality 
of the environment. As it turns out, natural surfaces have spectral reflectance distributions that vary along roughly 
five to seven dimensions (Maloney 1986). There thus exist natural surfaces that, although appearing to trichromatic 
viewers as “the same color,” nonetheless possess different spectral reflectance distributions.

Shepard (1992) instead asks how many color channels are needed to ensure that the color we perceive is the same color 
each time the surface is viewed under different lighting conditions. The amount of ambient light can also potentially 
vary along an unlimited number of dimensions, and the actual light reaching the eye is the product of the spectral 
power distribution and the spectral reflectance distribution. A reddish object in bluish light may reflect the same 
number of photons of each wavelength as a bluish object in reddish light. Similarly, a white object in reddish light 
may reflect mostly red photons, while the same white object in bluish light may reflect mostly blue photons. And yet 
the human visual system manages to maintain the property of color constancy; the same object will appear to be the 
same color under different lighting conditions.

Judd et al. (1964) measured 622 spectral power distributions for natural lighting, under 622 widely varying natural 
conditions of weather and times of day, and found that variations in natural lighting reduce to three degrees of 
freedom. Furthermore, these three degrees of freedom bear a close correspondence to the three dimensions of color 
opponency that were proposed for the human visual system based on experimental examination (Hurvich and Jameson 1957). 
The three degrees of freedom are:

1. The light-dark variation, which depends on the total light reaching the object.

2. The yellow-blue variation, which depends on whether a surface is illuminated by direct sunlight or is in shade. In 
shade the surface is illuminated by the Raleigh-scattered blue light of the sky, but is not directly illuminated by the 
sun. The corresponding yellow extreme occurs when an object is illuminated only by direct sunlight; e.g., if sunlight 
enters through a small channel and skylight is cut off.

3. The red-green variation, which depends on both the elevation of the sun (how much atmosphere the sun travels through), 
and the amount of atmospheric water vapor. E.g., illumination by a red sunset versus illumination at midday. Red 
wavelengths are the wavelengths least scattered by dust and most absorbed by water.

The three color channels of the human visual system are precisely the number of channels needed in order to maintain 
color constancy under natural lighting conditions.18 Three color channels are not enough to discriminate between all
natural surface reflectances, but three color channels are the exact number required to compensate for ambient natural 
lighting and thereby ensure that the same surface is perceptually the “same color” on any two occasions. This 
simplifies the adaptively important task of recognizing a previously experienced object on future encounters.
```

<a name="#Technology-and-Futurism"></a>
## Technology and futurism
([overview](#overview))

<a name="#Origins-of-information-society"></a>
### Origins of information society
([overview](#overview))

From Cosma Shalizi's [book review](http://bactra.org/reviews/beniger/) of James Beniger's *The Control Revolution: Technological and Economic Origins of the Information Society*:

```markdown
I was going to say that this is undoubtedly the best work ever done by a professor of communications, but that would be praising it with faint damns, and it deserves better. This is not a speculation or a vague schema but a very detailed history of the rise of technologies and techniques of communication and information-processing, and their use for controlling social and economic processes, prefaced by a general discussion of these subjects and their importance for history. 

His thesis is that modern information technologies, and with them the ``information society,'' began to take shape in the 1830s with the introduction of railroads, and really took off after 1880 with full industrialization. Because machine industry involves huge, fast flows of goods, it cannot be managed without a high level of information technology (in which Beniger includes things like product standardization, bureaucracy and advertising, as well as the usual mechanical devices): and if it isn't managed it simply cannot work. The first part of the economy to move at industrial speed were the railroads, and the accompanying jump in the size of the information sector is dramatic. So industrial production is a good reason to improve your information technology; and conversely, improved control technology makes new industrial developments possible.

Beniger puts the modern synthesis (not his phrase) of industry and information in the period 1880-1920. By the latter date, the technology of control had been so perfected that the economies of all the warring powers in the Great War could be managed by central planning --- those of the Allies, by combined planning. (Since this performance was repeated during the Second War, I'm tempted to say that market forces are simply too inefficient to be trusted with anything important, but this is not the place for those rants.) Since then, he says, we have been in essentially the same industrial-economic-technological phase. The advent of computers was obviously very important, but they didn't usher in the information society, because we already were one (which, I suspect, is why they were able to spread so quickly --- Beniger does not, alas, discuss computerization in detail).
```

<a name="#the-singularity"></a>
### The Singularity
([overview](#overview))

<a name="#Three-Singularity-schools"></a>
### Three Singularity schools
([overview](#overview))

Eliezer Yudkowsky's [essay](http://yudkowsky.net/singularity/schools/) continues to be a good summary of how things stand. First a complaint:

```markdown
I’ve heard (many) other definitions of the Singularity attempted, but I usually find them 
to lack separate premises and conclusions. For example, the old Extropian FAQ used to define
the “Singularity” as the Inflection Point, “the time when technological development will be
at its fastest” and just before it starts slowing down. 

But what makes this an interesting point in history apart from its definition? What are the
consequences of this assumption? To qualify as a school of thought or even a thesis, one needs
an internal structure of argument, not just a definition.
```

On to the three schools: 

```markdown
**Accelerating Change:**

Core claim: Our intuitions about change are linear; we expect roughly as much change as has
occurred in the past over our own lifetimes. But technological change feeds on itself, and 
therefore accelerates. Change today is faster than it was 500 years ago, which in turn is 
faster than it was 5000 years ago. Our recent past is not a reliable guide to how much change
we should expect in the future.

Strong claim: Technological change follows smooth curves, typically exponential. Therefore we
can predict with fair precision when new technologies will arrive, and when they will cross key
thresholds, like the creation of Artificial Intelligence.

Advocates: Ray Kurzweil, Alvin Toffler(?), John Smart

**Event Horizon:**

Core claim: For the last hundred thousand years, humans have been the smartest intelligences 
on the planet. All our social and technological progress was produced by human brains. Shortly,
technology will advance to the point of improving on human intelligence (brain-computer 
interfaces, Artificial Intelligence). This will create a future that is weirder by far than most
science fiction, a difference-in-kind that goes beyond amazing shiny gadgets.

Strong claim: To know what a superhuman intelligence would do, you would have to be at least
that smart yourself. To know where Deep Blue would play in a chess game, you must play at Deep
Blue’s level. Thus the future after the creation of smarter-than-human intelligence is absolutely
unpredictable.

Advocates: Vernor Vinge

**Intelligence Explosion:**

Core claim: Intelligence has always been the source of technology. If technology can significantly 
improve on human intelligence – create minds smarter than the smartest existing humans – then this
closes the loop and creates a positive feedback cycle. What would humans with brain-computer
interfaces do with their augmented intelligence? One good bet is that they’d design the next 
generation of brain-computer interfaces. Intelligence enhancement is a classic tipping point; the
smarter you get, the more intelligence you can apply to making yourself even smarter.

Strong claim: This positive feedback cycle goes FOOM, like a chain of nuclear fissions gone 
critical – each intelligence improvement triggering an average of>1.000 further improvements of
similar magnitude – though not necessarily on a smooth exponential pathway. Technological progress 
drops into the characteristic timescale of transistors (or super-transistors) rather than human
neurons. The ascent rapidly surges upward and creates superintelligence (minds orders of magnitude
more powerful than human) before it hits physical limits.

Advocates: I. J. Good, Eliezer Yudkowsky
```

Note that while the core claims support each other, the strong claims are contradictory:

```markdown
If you extrapolate our existing version of Moore’s Law past the point of smarter-than-human AI to 
make predictions about 2099, then you are contradicting both the strong version of the Event 
Horizon (which says you can’t make predictions because you’re trying to outguess a transhuman mind)
and the strong version of the Intelligence Explosion (because progress will run faster once smarter-
than-human minds and nanotechnology drop it into the speed phase of transistors).
```

<a name="#the-singularity-has-happened"></a>
### The Singularity has happened
([overview](#overview))

Cosma Shalizi is usually hard to do justice to when quoting him, because he's so densely hyperlinked to so many cool further reads. That caveat out of the way, here's from [The Singularity in Our Past Light-Cone](http://bactra.org/weblog/699.html).

He's also great because in contrast to others of "his type", he's very well read in history, and so tends to provide appropriate historical contextualizing to recent events like general civilizational tech progress or whatever. This shows in the quote below.

```markdown
The Singularity has happened; we call it "the industrial revolution" or "the long nineteenth 
century". It was over by the close of 1918.

Exponential yet basically unpredictable growth of technology, rendering long-term extrapolation 
impossible (even when attempted by geniuses)? Check.

Massive, profoundly dis-orienting transformation in the life of humanity, extending to our 
ecology, mentality and social organization? Check.

Annihilation of the age-old constraints of space and time? Check.

Embrace of the fusion of humanity and machines? Check.

Creation of vast, inhuman distributed systems of information-processing, communication and
control, "the coldest of all cold monsters"? Check; we call them "the self-regulating market 
system" and "modern bureaucracies" (public or private), and they treat men and women, even those 
whose minds and bodies instantiate them, like straw dogs.

An implacable drive on the part of those networks to expand, to entrain more and more of the world
within their own sphere? Check. ("Drive" is the best I can do; words like "agenda" or "purpose" 
are too anthropomorphic, and fail to acknowledge the radical novely and strangeness of these 
assemblages, which are not even intelligent, as we experience intelligence, yet ceaselessly 
calculating.)

Why, then, since the Singularity is so plainly, even intrusively, visible in our past, does science
fiction persist in placing a pale mirage of it in our future? Perhaps: the owl of Minerva flies at 
dusk; and we are in the late afternoon, fitfully dreaming of the half-glimpsed events of the day, 
waiting for the stars to come out.
```

<a name="#Disneyland-with-no-children"></a>
### Disneyland with no children
([overview](#overview))

Finally dug up this quote! From Scott Alexander's legendary [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), quoting Nick Bostrom's *Superintelligence*:

```markdown
It is conceivable that optimal efficiency would be attained by grouping capabilities in
aggregates that roughly match the cognitive architecture of a human mind… But in the 
absence of any compelling reason for being confident that this so, we must countenance 
the possibility that human-like cognitive architectures are optimal only within the
constraints of human neurology (or not at all). When it becomes possible to build 
architectures that could not be implemented well on biological neural networks, new design
space opens up; and the global optima in this extended space need not resemble familiar 
types of mentality. Human-like cognitive organizations would then lack a niche in a
competitive post-transition economy or ecosystem.

We could thus imagine, as an extreme case, a technologically highly advanced society,
containing many complex structures, some of them far more intricate and intelligent than 
anything that exists on the planet today – a society which nevertheless lacks any type of
being that is conscious or whose welfare has moral significance. In a sense, this would be
an uninhabited society. It would be a society of economic miracles and technological 
awesomeness, with nobody there to benefit. A Disneyland with no children.
```

Scott then argues:

```markdown
The last value we have to sacrifice is being anything at all, having the lights on inside.
With sufficient technology we will be “able” to give up even the final spark.

*(Moloch whose eyes are a thousand blind windows!)*

Everything the human race has worked for – all of our technology, all of our civilization,
all the hopes we invested in our future – might be accidentally handed over to some kind of
unfathomable blind idiot alien god that discards all of them, and consciousness itself, in
order to participate in some weird fundamental-level mass-energy economy that leads to it 
disassembling Earth and everything on it for its component atoms.

*(Moloch whose fate is a cloud of sexless hydrogen!)*

Bostrom realizes that some people fetishize intelligence, that they are rooting for that 
blind alien god as some sort of higher form of life that ought to crush us for its own 
“higher good” the way we crush ants. He argues (Superintelligence, p. 219):

*The sacrifice looks even less appealing when we reflect that the superintelligence could 
realize a nearly-as-great good (in fractional terms) while sacrificing much less of our own
potential well-being. Suppose that we agreed to allow almost the entire accessible universe
to be converted into hedonium – everything except a small preserve, say the Milky Way, which
would be set aside to accommodate our own needs. Then there would still be a hundred billion
galaxies dedicated to the maximization of [the superintelligence’s own values]. But we would
have one galaxy within which to create wonderful civilizations that could last for billions 
of years and in which humans and nonhuman animals could survive and thrive, and have the 
opportunity to develop into beatific posthuman spirits.*

Remember: Moloch can’t agree even to this 99.99999% victory. Rats racing to populate an island
don’t leave a little aside as a preserve where the few rats who live there can live happy lives
producing artwork. Cancer cells don’t agree to leave the lungs alone because they realize it’s
important for the body to get oxygen. Competition and optimization are blind idiotic processes
and they fully intend to deny us even one lousy galaxy.

*They broke their backs lifting Moloch to Heaven! Pavements, trees, radios, tons! lifting the 
city to Heaven which exists and is everywhere about us!*

We will break our back lifting Moloch to Heaven, but unless something changes it will be his 
victory and not ours.
```

<a name="#rambling-and-ai"></a>
### Rambling and AI
([overview](#overview))

From Robin Hanson's post [Better Babblers](http://www.overcomingbias.com/2017/03/better-babblers.html), speculating on how the world might change in the proliferation of GPT-2s: 

```markdown
Let me call styles of talking (or music, etc.) that rely mostly on low order correlations 
“babbling”. Babbling isn’t meaningless, but to ignorant audiences it often appears to be 
based on a deeper understanding than is actually the case. When done well, babbling can be
entertaining, comforting, titillating, or exciting. It just isn’t usually a good place to
learn deep insight.

As we slowly get better at statistics and machine learning, our machines will slowly get 
better at babbling. The famous Eliza chatbot went surprisingly far using very low order
correlations, and today chatbots best fool us into thinking they are human when they stick
to babbling style conversations. So what does a world of better babblers look like?

First, machines will better mimic low quality student essays, so schools will have to try 
harder to keep such students from using artificial babblers.

Second, the better machines get at babbling, the more humans will try to distinguish
themselves from machines via non-babbling conversational styles. So expect less use of 
simple easy-to-understand-and-predict speech in casual polite conversation, inspirational 
speeches, and public intellectual talk.

One option is to put a higher premium on talk that actually makes deep sense, in terms of 
deep concepts that experts understand. That would be nice for those of us who have always 
emphasized such things. But alas there are other options.

A second option is to put a higher premium on developing very distinctive styles of talking.
This would be like how typical popular songs from two centuries ago could be sung and 
enjoyed by most anyone, compared to how popular music today is matched in great detail to 
the particular features of particular artists. Imagine most all future speakers having as 
distinct a personal talking style.

A third option is more indirect, ironic, and insider style talk, such as we tend to see on
Twitter today. People using words and phrases and cultural references in ways that only folks
very near in cultural space can clearly accept as within recent local fashion. Artificial 
babblers might not have enough data to track changing fashions in such narrow groups.

Bottom line: the more kinds of conversation styles that simple machines can manage, the more 
humans will try to avoid talking in those styles, a least when not talking to machines.
```

See also [rambling in writing, talking etc](#rambling). 

<a name="#replacing-humans-at-every-step-of-the-economic-chain"></a>
### Replacing humans at every step of the economic chain
([overview](#overview))

Scott Alexander’s [Book Review: Age of Em](https://slatestarcodex.com/2016/05/28/book-review-age-of-em/) has a cute little throwaway thought experiment/short story I find myself grasping for a few times already in the last several years. Every time I’ve had to dig around for a while because I keep thinking it’s in the 15,000-word Moloch post, or the ones with Bostrom’s “Disneyland with no children”, but it’s not, it’s in Scott’s review of Robin Hanson’s book. This shouldn’t be surprising since Scott and Robin’s opinions are diametrically opposed when it comes to the latter’s em-run-possible-future, which is the whole motivation behind the thought experiment, but somehow I keep forgetting. So I’m recording it here for my own future reference:

```markdown
Imagine a company that manufactures batteries for electric cars. The inventor of the batteries 
might be a scientist who really believes in the power of technology to improve the human race. 
The workers who help build the batteries might just be trying to earn money to support their 
families. The CEO might be running the business because he wants to buy a really big yacht. And 
the whole thing is there to eventually, somewhere down the line, let a suburban mom buy a car to
take her kid to soccer practice. Like most companies the battery-making company is primarily a 
profit-making operation, but the profit-making-ness draws on a lot of not-purely-economic actors
and their not-purely-economic subgoals.

Now imagine the company fires all its employees and replaces them with robots. It fires the
inventor and replaces him with a genetic algorithm that optimizes battery design. It fires the
CEO and replaces him with a superintelligent business-running algorithm. All of these are good
decisions, from a profitability perspective. We can absolutely imagine a profit-driven shareholder-
value-maximizing company doing all these things. But it reduces the company’s non-masturbatory 
participation in an economy that points outside itself, limits it to just a tenuous connection 
with soccer moms and maybe some shareholders who want yachts of their own.

Now take it further. Imagine there are no human shareholders who want yachts, just banks who lend 
the company money in order to increase their own value. And imagine there are no soccer moms anymore;
the company makes batteries for the trucks that ship raw materials from place to place. Every 
non-economic goal has been stripped away from the company; it’s just an appendage of Global 
Development.

Now take it even further, and imagine this is what’s happened everywhere. There are no humans 
left; it isn’t economically efficient to continue having humans. Algorithm-run banks lend money 
to algorithm-run companies that produce goods for other algorithm-run companies and so on ad 
infinitum. Such a masturbatory economy would have all the signs of economic growth we have today.
It could build itself new mines to create raw materials, construct new roads and railways to
transport them, build huge factories to manufacture them into robots, then sell the robots to 
whatever companies need more robot workers. It might even eventually invent space travel to reach
new worlds full of raw materials. Maybe it would develop powerful militaries to conquer alien 
worlds and steal their technological secrets that could increase efficiency. It would be vast, 
incredibly efficient, and utterly pointless. The real-life incarnation of those strategy games
where you mine Resources to build new Weapons to conquer new Territories from which you mine 
more Resources and so on forever.

But this seems to me the natural end of the economic system. Right now it needs humans only as
laborers, investors, and consumers. But robot laborers are potentially more efficient, companies
based around algorithmic trading are already pushing out human investors, and most consumers 
already aren’t individuals – they’re companies and governments and organizations. At each step 
you can gain efficiency by eliminating humans, until finally humans aren’t involved anywhere.
```

<a name="#political-science"></a>
## Political science
([overview](#overview))

<a name="#chinese-perspective-on-western-elite"></a>
### Chinese perspective on Western elite
([overview](#overview))

I love how Puzhong Yao comes across in his essay [The Western Elite from a Chinese Perspective](https://americanaffairsjournal.org/2017/11/western-elite-chinese-perspective/), about his experiences at Goldman Sachs, Cambridge and Stanford. He’s so honest and straight-laced he reminds me of myself before I ‘got more sophisticated’ in order to ‘navigate the turbulent realities of daily workplace politics’ (well, minimally, so I don’t drown). Here are some quotes I especially liked.

Result over process:

```markdown
My job at Goldman was a mixture of making markets to facilitate client trades and finding trades
for the bank’s own book. In early 2009, I believed it was an excellent trade to go long UK 
inflation. In fact, I thought it was such a good trade that my biggest worry was that there 
wouldn’t be anyone who would want to be on the other side of it. Yet we managed to put this
trade on versus a British bank. In the following year, the trade worked wonders, with UK 
inflation steadily rising, making the bank tens of millions in profits.

I thought I was an amazing trader. But there was a slight problem: I wanted to do the trade 
because I thought the market was pricing UK interest rates to go up. And when interest rates go 
up, UK inflation would rise mechanically due to the way it is defined and calculated. But in that
year, the Bank of England didn’t raise interest rates at all. Rather, the increase in inflation 
was due to things like tax increases, exchange rate fluctuations, oil price moves, etc.—things I
didn’t anticipate at all. It was pure luck that I made money, and I made it for the wrong reason.

When I was an intern, in one of the training presentations, a senior banker told us to distinguish
between the process and the results. He said that we should focus on the process, which we can
control, rather than the result, which is subject to luck. And here at Goldman, he said, we don’t
punish people for losing money for the right reason. I have always loved asking questions, so I 
asked him, was anyone ever punished for making money for the wrong reason? After giving it some
thought, he said that he had not heard of any such thing. And he was right. In fact, no one seemed
to remember the reason I did the inflation trade at all. They only remembered that I did this trade
and that it worked well.

When I met with my manager for a performance review after this, I was expecting to be berated for
my poor judgment. Instead, I got promoted! I told my manager that it was a mistake, but he merely
said, “Puzhong, tell no one.” He too was promoted on the basis of managing my “brilliant” trade. 
In fact, my manager was so proud of my work he recommended me to Stanford’s prestigious Graduate 
School of Business (GSB), and I soon set off for America.
```

Mottos as an example of Robin Hanson's "X isn't about Y":

```markdown
One class was about strategy. It focused on how corporate mottos and logos could inspire employees.
Many of the students had worked for nonprofits or health care or tech companies, all of which had 
mottos about changing the world, saving lives, saving the planet, etc. The professor seemed to like
these mottos.

I told him that at Goldman our motto was “be long-term greedy.” The professor couldn’t understand
this motto or why it was inspiring. I explained to him that everyone else in the market was short-term 
greedy and, as a result, we took all their money. Since traders like money, this was inspiring.

He asked if perhaps there was another motto or logo that my other classmates might connect with. I
told him about the black swan I kept on my desk as a reminder that low probability events happen with
high frequency.

He didn’t like that motto either and decided to call on another student, who had worked at Pfizer. 
Their motto was “all people deserve to live healthy lives.” The professor thought this was much better.

I didn’t understand how it would motivate employees, but this was exactly why I had come to Stanford:
to learn the key lessons of interpersonal communication and leadership.
```

On the importance of showing that you feel strongly about something, else it shows you aren't taking it seriously:

```markdown
On the communication and leadership front, I came to the GSB knowing I was not good and hoped to 
get better. My favorite class was called “Interpersonal Dynamics” or, as students referred to it,
“Touchy Feely.” In “Touchy Feely,” students get very candid feedback on how their words and actions 
affect others in a small group that meets several hours per week for a whole quarter.

We talked about microaggressions and feelings and empathy and listening. Sometimes in class the 
professor would say things to me like “Puzhong, when Mary said that, I could see you were really 
feeling something,” or “Puzhong, I could see in your eyes that Peter’s story affected you.” And I 
would tell them I didn’t feel anything. I was quite confused.

One of the papers we studied mentioned that subjects are often not conscious of their own feelings 
when fully immersed in a situation. But body indicators such as heart rate would show whether the 
person is experiencing strong emotions. I thought that I generally didn’t have a lot of emotions and
decided that this might be a good way for me to discover my hidden emotions that the professor kept
asking about.

So I bought a heart rate monitor and checked my resting heart rate. Right around 78. And when the 
professor said to me in class “Puzhong, I can see that story brought up some emotions in you,” I
rolled up my sleeve and checked my heart rate. It was about 77. And so I said, “nope, no emotion.” 
The experiment seemed to confirm my prior belief: my heart rate hardly moved, even when I was
criticized, though it did jump when I became excited or laughed.

This didn’t land well on some of my classmates. They felt I was not treating these matters with the
seriousness that they deserved. The professor was very angry. My takeaway was that my interpersonal
skills were so bad that I could easily offend people unintentionally, so I concluded that after 
graduation I should do something that involved as little human interaction as possible.
```

<a name="#research-and-academia"></a>
## Research and academia
([overview](#overview))

<a name="#role-of-intellectual-traditions"></a>
### Role of intellectual traditions
([overview](#overview))

From Cosma Shalizi's [review](http://bactra.org/reviews/error/) of Deborah Mayo's book *Error and the Growth of Experimental Knowledge*:

```markdown
Then, too, there is the interesting, and I think absolutely correct, view of the purpose and utility
of a theory of experiment: "It changes fortuitous events, which may take weeks or may take many decennia,
into an operation governed by intelligence, which will be finished within a month" (7.78, quoted p. 434).

This is of a piece with the general function of intellectual traditions. Genius can, perhaps, get by on
its wits, make things up from scratch, etc. Intellect serves the rest of us, by codifying, by setting up
standards and procedures which can be followed with only (as a friend once happily put it) "a mediocum of
intelligence," so that what might have taken genius can be (at least partially) achieved through the
application of rules. Among those rules, "normal tests" or "standard tests" --- tests which have proved to
be reliable detectors of specific errors --- take a special place. Traditions of inquiry which incorporate
and use a family of normal tests may fail to produce reliable knowledge, but those which don't can hardly
hope even to produce interesting mistakes.
```

<a name="#research-in-industry"></a>
### Research in industry
([overview](#overview))

Simon DeDeo is a professor at the Santa Fe Institute, that interdisciplinary haven of complexity science that Cosma Shalizi is affiliated with. Here’s a tweetstorm by Simon on science in large corporate organizations like Google / Facebook, which runs counter to my previous impression (I’d like to see a contrary perspective actually), which I’m crossposting here for ease of personal reference:

```markdown
I grew up in the Bayesian era—I watched @DavidSpergel and his band of merry scientists change
our view of the world with a few simple, theoretically-motivated equations.

That's what I brought to the table when I went out to study living and thinking systems. 
Around 2010, of course, the deep learning revolution became impossible to ignore.

It was exciting stuff. We'd have people visit the Institute and tell us about decision trees, 
random forests,—all sorts of wonderful things. I tried to get a handle on it but (honestly) 
there was so much we could do with simple tools that it was never a priority.

When I got to IU, I was hired as a prof in the informatics department, @IUSoICE (informatics==the
future of CS—i.e., forget quicksort, let's work out what these machines are doing to human life).
I was on a hiring committee, and we were keen to get a deep learning hire.

I took all the candidates out to breakfast (I was a naughty hire, and skipped meetings and
committees to spend my time with research students and undergrads, this was the one gap I had).

I tried to work out what deep learning was about. Most of the candidates were too sleep deprived 
to dissemble. Basic answer: every sexy project we do—flying quadcopters, getting another 0.1% on 
the MNIST—is basically one graduate student.

You work out the topology of the neural net. Then you find the weights. How? The answer: "graduate
student descent", a little pun to giggle over floppy croissants at the student cafe—in short, 
there's no good answer, a human being sits there and twiddles things about.

Machine learning is an amazing accomplishment of engineering. But it's not science. Not even close.
It's just 1990, scaled up. It has given us literally no more insight than we had twenty years ago.

"Deep learning implements the renormalization group!" Yeah, I heard that too. If you have a system 
where information is organized spatially, is it really a surprise that the neurons group information
together spatially?

I'd get invited to meetings at Google Research, or wherever. They had security like crazy—worse than 
a hedge fund. A security guard would follow you to the bathroom.

Each scientist at my "grade"—i.e., the equivalent of a junior faculty member, someone who should be
out on the edge of knowledge—was, instead, managing a team of ten people doing graduate student 
descent.

Google can beat University of Kansas for the sole reason that they can hire ten times more graduate
students per researcher. The difference, of course, is that a graduate student at UK has the chance
to do something intellectually significant. Not true at GR.

They had no idea what they were doing. They had the manpower (word chosen advisedly) to apply deep 
learning to anything, simulating the Schrodinger equation, drug design, anything. Their main goal
was to find the scientific field they could have the maximum impact on.

I've visited probably fifty Universities. I love it. Everywhere I go, I get new ideas. It's one of
the best features of my job. There's one exception: commercial "research" labs.

If you want to build machines that monitor people and sell them more ads faster, go for it. If you
want to find problem where you can take a working-class job, model the man or woman who does it, and
build a net to put them out of a job without compensation, be my guest.

Have we done science with something Google Research has built? Absolutely. We have a great paper 
coming out where we use word2vec to help build a theory of puzzle solving.

But we could have built a system of equal utility ourselves. There's zero intellectual contribution
there. I'm not joking, and I'll go head-to-head with anyone who says I am.

I got a nice cold-call from a top-flight Masters' student in CS, as I do sometimes (please keep them
coming, I can pay). I flew him out and we started working on a problem in the emergence of social 
cooperation. He wanted to do DL.

Two weeks in we were a step beyond what Google Brain was doing. I don't mean technically—they had 
amazing YouTube videos of sprites in a landscape. I do mean intellectually. Their demos were like 
2018 meets something out of the 1980s.

They said they did social science, but it was nothing of the sort. It was homo economicus spread out
over 50 GPUs. At best, a devastating proof-by-example of need for academia. Buy a copy of Bowles and
Ginits, Cooperative Species, and you'll learn more than they did, in a week.

Can you do cool research at Google Brain? Honest answer: no. You will be on the cutting edge of 
machine learning, yes—an engineering discipline whose basic goals are set by large corporations.
But you will not be a scientist.

I get that you may need to make money. You can make a lot there, and all the jobs at Renaissance
Technology are taken. Go for it—you have all my respect. Academia sucks.

But if you want, at some point in your flourishing career, with your mind and your soul, to join
the two-thousand year old parade of intellectual progress, you are not going to do it at Google.
Certainly not at Facebook.

If you want to do that, I have a suggestion. It's not the only path, by any means, and I've had 
amazing fellow-travellers who haven't. But here it is.

Go to graduate school. Do a PhD. With us, here at CMU/SDS, if you like—but we're not the only place
that does computational social or cognitive science. You won't get paid much, but you will mentors
who legitimately care about the development of your mind.

It's difficult to overestimate the difference between a good PhD program and industry. It is 
literally shameful, if you're a good PhD advisor, to interfere with the intellectual development
of a student. At Google, it's a business plan.

None of this is a joke. This is ten years of experience. Graduate school applications are coming 
up in the Fall. Think about it. Make sure you're getting a good deal (you shouldn't go into debt
for a PhD, and you should get healthcare).

In short: corporate "research" is a business proposition. Whatever true intellectual progress 
comes out of there happens in spite of management. Given how good these companies are at monitoring
their employees, that gap is now miniscule.

Last anecdote, then I'm done. We visited Google Research, arranged by a contact. The people were
unbelievably smart. We brainstormed all sorts of wonderful things to work on. The last day of the
meeting, the academics were like, OK! Let's go to the pub! Let's hash this out!

Their response: this was vacation for us. We're behind on our real work. We have to work this 
weekend. (Not "we feel guilty", but "we have to".) For the academics in the room, this was work.
Suddenly, I realized that this was vacation for them.
```

This happened to generate lots of heated discussion on [the ML subreddit](https://www.reddit.com/r/MachineLearning/comments/8yvlzy/d_debate_about_science_at_organizations_like/). Example counterargument from harharveryfunny:

```markdown
It's a bit of a straw man attack to complain that Google Brain (an engineering division) isn't doing
research, while ignoring Google DeepMind - their research division!

The argument that industry is all about money also ignores the grand history of corporate research
labs such as Bell Labs (thanks for the transistor!), Watson Labs, Xerox Parc (you could hardly
complain about them being too commercial!). I don't see any reason to suppose that Google, FaceBook
or Microsoft's research divisions are any less pure.
```

kendallpark:

```markdown
I've worn both the "industry" and "researcher" hat. In my particular field (ML + medicine) research,
we've had a solid 3+ decade parade of "intellectual progress" in the form of academic papers about ML 
+ medicine. You know how many of these promising ideas have made it into the clinic? Pretty much none.
Why? Well, there are a lot of reasons. But one of the big ones is PRODUCT.

Academic researchers often don't want to deal with nuts and bolts of actual implementation of their
project into a real system. Academic researchers often lack the software development know-how to 
implement their project into a real system. (Or at least, this has definitely been what I've witnessed
during graduate study.)

And you know what? It's totally fine to be a non-translational ML + medicine researcher. Maybe 
productization isn't their thing. I appreciate the papers; they should keep trying new things and
writing about their results.

But at the same I see way too many ML + medicine researchers patting themselves on the back for 'making 
a difference' in medicine when all they're doing is adding another paper to the mountain of "great ideas 
that could save patient lives but never will because no one will implement them."

I understand that this is not the situation all ML + [some other field] fall into, perhaps not at all for
pure ML, but this has been my experience in ML + medicine. We have more than enough research. We need more
product.

For the record: this is my own opinion and not that of any of my current or previous institutions/employers.
```

<a name="#why-academic-writing-sucks"></a>
### Why academic writing sucks
([overview](#overview))

See also [Writing advice](#writing-advice).

My favorite academic(ish) writers — like Baez for math, Aaronson for CS, Simler for postrationality, Nerst for erisology — are all guided by the spirit of *“hey, there’s something exciting I wanna show you!”*. Their words *get out of the way*, and you see the distilled essence of the thing that’s taken their breath away, and has now snatched yours too. It’s a tremendously heady experience. A longstanding dream of mine is to write a piece that captures that same spirit in explaining a difficult idea.

From Steven Pinker's essay [Why academics stink at writing](https://stevenpinker.com/files/pinker/files/why_academics_stink_at_writing.pdf), which you should read in its entirety for the beautifully clear writing and guffaw-inducing anecdotes. 

Pinker says these answers are wrong, or at least incommensurate with his own experience:

```markdown
(1) The most popular answer outside the academy is the cynical one:
Bad writing is a deliberate choice. Scholars in the softer fields
spout obscure verbiage to hide the fact that they have nothing to
say. They dress up the trivial and obvious with the trappings of
scientific sophistication, hoping to bamboozle their audiences
with highfalutin gobbledygook.

Though no doubt the bamboozlement theory applies to some
academics some of the time, in my experience it does not ring true.
I know many scholars who have nothing to hide and no need to
impress. They do groundbreaking work on important subjects,
reason well about clear ideas, and are honest, down-to-earth
people. Still, their writing stinks.

(2) The most popular answer inside the academy is the self-serving
one: Difficult writing is unavoidable because of the abstractness
and complexity of our subject matter. Every human pastime—
music, cooking, sports, art—develops an argot to spare its
enthusiasts from having to use a long-winded description every
time they refer to a familiar concept in one another’s company. It
would be tedious for a biologist to spell out the meaning of the
term transcription factor every time she used it, and so we should
not expect the tête-à-tête among professionals to be easily
understood by amateurs.

But the insider-shorthand theory, too, doesn’t fit my experience. I
suffer the daily experience of being baffled by articles in my field,
my subfield, even my sub-sub-subfield. The methods section of an
experimental paper explains, "Participants read assertions whose
veracity was either affirmed or denied by the subsequent
presentation of an assessment word." After some detective work, I
determined that it meant, "Participants read sentences, each
followed by the word true or false." The original academese was
not as concise, accurate, or scientific as the plain English
translation. So why did my colleague feel compelled to pile up the
polysyllables?

(3) A third explanation shifts the blame to entrenched authority.
People often tell me that academics have no choice but to write
badly because the gatekeepers of journals and university presses
insist on ponderous language as proof of one’s seriousness. This
has not been my experience, and it turns out to be a myth. In 
Stylish Academic Writing (Harvard University Press, 2012), Helen
Sword masochistically analyzed the literary style in a sample of 500
scholarly articles and found that a healthy minority in every field
were written with grace and verve. 
```

So if bamboozlement, insider-shorthand and journal gatekeeping are wrong, what's really at fault? Pinker’s answer is two-pronged: the self-conscious style, and the curse of knowledge. The latter I’ve covered in previous posts, so the summary below concentrates on the former. 

First, Thomas and Turner on how to understand writing styles:

```markdown
Every style of writing, quoting Francis-Noel Thomas and Mark Turner’s book *Clear and Simple 
as the Truth*, can be understood as “a model of the communication scenario that an author 
simulates in lieu of the real-time give-and-take of a conversation”. They distinguish five 
styles, depending on how the writer imagines themselves to be related to the reader, and what
the writer is trying to accomplish:

1. romantic
2. oracular
3. prophetic
4. plain
5. practical

A particular style they single out as an aspiration for writers of expository prose is the 
*classic style*, its invention credited to 17th-century French essayists such as Descartes 
and La Rochefoucauld.
```

The guiding metaphor of classical style is “seeing the world”:

```markdown
The writer can see something that the reader has not yet noticed, and he orients the reader 
so she can see for herself. The purpose of writing is presentation, and its motive is disinterested
truth. It succeeds when it aligns language with truth, the proof of success being clarity and 
simplicity. The truth can be known and is not the same as the language that reveals it; prose 
is a window onto the world. The writer knows the truth before putting it into words; he is not
using the occasion of writing to sort out what he thinks. The writer and the reader are equals:
The reader can recognize the truth when she sees it, as long as she is given an unobstructed view.
And the process of directing the reader’s gaze takes the form of a conversation.
```

See also [The art of plain talk](#the-art-of-plain-talk).

Most academic writing is the opposite of classic style. It’s a blend of two styles: *practical* and *self-conscious* (or ironic, postmodern, relativistic).

Practical style:

```markdown
…the writer’s goal is to satisfy a reader’s need for a particular kind of information, and the form
of the communication falls into a fixed template, such as the fiveparagraph student essay or the 
standardized structure of a scientific article.
```

Self-conscious style:

```markdown
…the writer’s chief, if unstated, concern is to escape being convicted of philosophical naïveté 
about his own enterprise.

It’s easy to see why academics fall into self-conscious style. Their goal is not so much 
communication as self-presentation—an overriding defensiveness against any impression that they
may be slacker than their peers in hewing to the norms of the guild.
```

Pinker says that many of the hallmarks of “academese” are symptoms of the self-conscious style. Here's my abridged version of his explanation:

```markdown
1. Meta-discourse: verbiage about verbiage. Paraphrasing Pinker: “Thoughtless writers think 
they’re doing the reader a favor by guiding her through the text with stuff like previews, 
summaries, and signposts. Actually it helps the writer more than the reader: the reader has
to put more work into understanding the signposts than she saves in seeing what they point to,
like directions for a shortcut that take longer to figure out than the time the shortcut would
save. The art of classic prose is to use signposts sparingly, as we do in conversation… Instead 
of dry metadata: pose a question, co-opt the guiding metaphor behind classic style (vision — 
“as we have seen”), use we.”

2. Professional narcissism: Pinker: “Academics live in two universes: the world of the thing they
stud, and the world of their profession. Most of a researcher’s waking hours are spent in the 
second world, and it’s easy for him to confuse the two. The result is the typical opening of an
academic paper: ‘In recent years, an increasing number of psychologists and linguists have turned
their attention to the problem of child language acquisition…’ No offense, but few people are 
interested in how professors spend their time. Classic style ignores the hired help and looks 
directly at what they are being paid to study: ‘All children acquire the ability to speak a language
without explicit lessons. How do they accomplish this feat?’”

3. Apologizing: Pinker: “Self-conscious writers are also apt to kvetch about how what they’re about
to do is so terribly difficult and complicated and controversial. In the classic style, the writer
credits the reader with enough intelligence to realize that many concepts aren’t easy to define, 
and that many controversies aren’t easy to resolve. She is there to see what the writer will do 
about it.”

4. Shudder quotes: Pinker: “Academics often use quotation marks to distance themselves from a common
idiom, as in "She is a ‘quick study’ and has been able to educate herself in virtually any area that
interests her." They seem to be saying, "I couldn’t think of a more dignified way of putting this, 
but please don’t think I’m a flibbertigibbet who talks this way; I really am a serious scholar." The
problem goes beyond the nose-holding disdain for idiomatic English: it confuses the reader by 
introducing ambiguity. Quotation marks have a number of legitimate uses — squeamishness about one’s 
own choice of words is not among them.”

5. Hedging: Pinker: “Academics mindlessly cushion their prose with wads of fluff that imply they are
not willing to stand behind what they say, like “apparently” etc. The worst is “I would argue”. 
Writers use hedges in the vain hope that it will get them off the hook, or at least allow them to
plead guilty to a lesser charge, should a critic ever try to prove them wrong. A classic writer, in
contrast, counts on the common sense and ordinary charity of his readers, just as in everyday 
conversation we know when a speaker means in general or all else being equal — any adversary who is 
intellectually unscrupulous enough to give the least charitable reading to an unhedged statement will
find an opening to attack the writer in a thicket of hedged ones anyway. Also instead of hedging, 
qualify the statement — spell out the circumstances in which it does not hold rather than leaving
himself an escape hatch or being coy as to whether he really means it. It’s not that good writers 
never hedge their claims. It’s that their hedging is a choice, not a tic.”

6. Metaconcepts and nominalizations: concepts about concepts, vacuous words that don’t add content.
Pinker: “It’s easy to see why metaconcepts tumble so easily from the fingers of academics. Professors 
really do think about "issues" (they can list them on a page), "levels of analysis" (they can argue 
about which is most appropriate), and "contexts" (they can use them to figure out why something works
in one place but not in another). But after a while those abstractions become containers in which 
they store and handle all their ideas, and before they know it they can no longer call anything by 
its name. "Reducing prejudice" becomes a "prejudice-reduction model"… English grammar is an enabler
of the bad habit of writing in unnecessary abstractions because it includes a dangerous tool for 
creating abstract terms. A process called nominalization takes a perfectly spry verb and embalms it
into a lifeless noun by adding a suffix like –ance, –ment, or –ation. Instead of affirming an idea,
you effect its affirmation…
```

Pinker argues that the theory that “academese is the opposite of classic style” explains the following paradox of academic writing:

```markdown
Many of the most stylish writers who cross over to a general audience are scientists (together with
some philosophers who are fans of science), while the perennial winners of the Bad Writing Contest 
are professors of English. That’s because the ideal of classic prose is congenial to the worldview
of the scientist. Contrary to the common misunderstanding in which Einstein proved that everything
is relative and Heisenberg proved that observers always affect what they observe, most scientists 
believe that there are objective truths about the world, and that they can be discovered by a 
disinterested observer.

By the same token, this guiding image of classic prose could not be farther from the worldview of 
relativist academic ideologies such as postmodernism, poststructuralism, and literary Marxism, which
took over many humanities departments in the 1970s. Many of the winning entries in the Dutton contest
(such as Judith Butler’s "The move from a structuralist account in which capital is understood to
structure social relations in relatively homologous ways to a view of hegemony in which power relations
are subject to repetition, convergence, and rearticulation brought the question of temporality into the 
thinking of structure ….") consist almost entirely of metaconcepts.
```

See also [why math writing sucks](#why-math-is-boring).

<a name="#research-tactics"></a>
### Research tactics
([overview](#overview))

Scott Aaronson, in response to Luke Muehlhauser's interview question "what are some object-level research tactics you use (more specific than your general “bait and switch” strategy)?" in the [MIRI Conversations series](https://intelligence.org/2013/12/13/aaronson/):

```markdown
(1) Any time you find yourself in a philosophical disagreement with a fellow scientist, don’t be 
content just to argue philosophically — even if you’re sure you can win the argument! Instead, think
hard about whether you can go further, and find a concrete technical question that captures some 
little piece of what you’re disagreeing about. Then see if you can answer that technical question.
Of course, any time you do this, you have to be prepared for the possibility that the answer will go 
your opponent’s way, rather than yours! But what’s nice is that you get to publish a paper even then.
(One of the best ways to tell whether a given enterprise is scientific at all, rather than ideological,
is by asking whether the participants will opportunistically “go to bat for the opposing side” whenever
they find a novel truth on that side.) I’d estimate that up to half the papers I’ve written had their 
origin in my reading or overhearing some claim — for example, “Grover’s algorithm obviously can’t work
for searching actual physical databases, since the speed of light is finite,” or “the quantum states 
arising in Shor’s algorithm are obviously completely different from anything anyone has ever seen in the
lab,” or “the interactive proof results obviously make oracle separations completely irrelevant” — and 
getting annoyed, either because I thought the claim was false, or because I simply didn’t think it had
been adequately justified. The cases where my annoyance paid off are precisely the ones where, rather 
than just getting mad, I managed to get technical!

(2) Often, the key to research is figuring out how to redefine failure as success. Some stories: when Alan
Turing published his epochal 1936 paper on Turing machines, he did so with great disappointment: he 
had recently learned that Alonzo Church had independently arrived at similar results using lambda 
calculus, and he didn’t know whether anyone would still be interested in his alternative, machine-based
approach. In the early 1970s, Leonid Levin delayed publishing about NP-completeness for several years:
apparently, his “real” goal was to prove graph isomorphism was NP-complete (something we now know is 
almost certainly false), and in his mind, he had failed. Instead, he merely had a few “trivialities,” 
like the definitions of P, NP, and NP-completeness, and the proof that satisfiability was NP-complete. 
And Levin’s experience is far from unique: again and again in mathematical research, you’ll find yourself
saying something like: “goddammit, I’ve been trying for six months to prove Y, but I can only prove the
different/weaker statement X! And every time I think I can bridge the gap between X and Y, yet another 
difficulty rears its head!” Any time that happens to you, think hard about whether you can write a 
compelling paper that begins: “Y has been a longstanding open problem. In this work, we introduce a new 
idea: to make progress on Y by shifting attention to the more tractable X.” More broadly, experience has 
shown that scientists are terrible judges of which of their ideas will be interesting or important to 
others. Pick any scientist’s most cited paper, and there’s an excellent chance that the scientist herself,
at one point, considered it a “little recreational throwaway project” that was barely worth writing up. 
After you’ve seen enough examples of that, you learn you should always err on the side of publishing, and 
let posterity sort out which of your ideas are most important. (Yet another advantage of this approach is
that, the more ideas you publish, the less emotionally invested you are in any one of them, so the less
crushed you are when a few turn out to be wrong or trivial or already known.)

(3) Sometimes, when you set out to prove some mathematical conjecture, your first instinct is just to throw
an arsenal of theory at it. “Hey, what if I try a topological fixed-point theorem? What if I translate the
problem into a group-theoretic language? If neither of those works, what if I try both at once?” Sometimes, 
you rise so quickly this way into a stratosphere of generality that the original problem is barely a speck on
the ground. And yes, some problems can be beaten into submission using high-powered theory. But in my 
experience, there are two enormous risks with this approach. First, you’re liable to get lost on a wild
goose chase, where you get so immersed in theory and techniques that you lose sight of your original goal. 
It’s as if your efforts to break into a computer network lead you to certain complicated questions about 
the filesystem, which in turn lead you to yet more complicated questions about the kernel… and in the 
meantime someone else breaks in by guessing people’s birthdays for their passwords. Second, you’re also
liable to fool yourself this way into thinking you’ve solved the problem when you haven’t. When you let 
high-powered machinery take the place of hands-on engagement with the problem, a single mistake in applying 
the machinery can creep in unbelievably easily. These risks are why I’ve learned over time to work in an 
extremely different way. Rather than looking for “general frameworks,” I look for easy special cases and
simple sanity checks, for stuff I can try out using high-school algebra or maybe a five-line computer program, 
just to get a feel for the problem. Even more important, when I’m getting started, I don’t think about proof
techniques at all: I think instead about obstructions. That is, I ask myself, “what would the world have to 
be like for the conjecture to be false? what goes wrong if I try to invent a simple counterexample? does 
anything go wrong? it does? OK then, what obstruction keeps me from proving this conjecture in the simplest,
dumbest way imaginable?” I find that, after you’ve felt out the full space of obstructions and
counterexamples, and really honestly convinced yourself of why the conjecture should be true, finding the 
proof techniques by which to convince everyone else is often a more-or-less routine exercise.

Finally, you ask about tactics that I suspect might be helpful, but that I haven’t used much myself. One that
springs to mind is to really master a tool like Mathematica, MATLAB, Maple, or Magma — that is, to learn it 
so well that I can code as fast as I think, and just let it take over all the routine / calculational /
example-checking parts of my work. As it is, I use pretty much the same antiquated tools that I learned as
an adolescent, and I rely on students whenever there’s a need for better tools. A large part of the problem
is that, as a “tenured old geezer,” I no longer have the time or patience to learn new tools just for the 
sake of learning them: I’m always itching just to solve the problem at hand with whatever tools I know. (The
same issue has kept me from learning new mathematical tools, like representation theory, even when I can
clearly see that they’d benefit me.)
```

Luke himself:

```markdown
1. When you’re confused about a fuzzy, slippery concept, try to build a simple formal model and push on it 
with the new tools then available to you. Even if the model doesn’t capture the complexity of the world,
pushing things into the mathematical realm can lead to progress. E.g. the VNM axioms don’t exactly capture
“rationality,” but it sure is easier to think clearly about rationality once you have them. Or: we’re 
confused about how to do principled reflective reasoning within an agent, so even though advanced AIs 
are unlikely to literally run into a “Löbian obstacle” to self-reflection, setting up the problem that 
way (in mathematical logic) can lead to some interesting insights in (e.g.) probabilistic metamathematics
for reflective reasoning.

2. Look for tools from other fields that appear to directly map onto the phenomena you’re studying. E.g. model
moral judgment as an error process amenable to Bayesian curve fitting.

3. Try to think of how your concept could be instantiated with infinite computing power. If you can’t do that,
your concept might be fundamentally confused.

4. If you’re pretty familiar with modern psychology, then… When using your intuitions to judge between options,
try to think about which cognitive algorithms could be generating those intuitions, and whether they are
cognitive algorithms whose outputs you reflectively endorse.

5. To make the thing you’re studying clearer, look just next to it, and around it. Foer (2009) explains this
nicely in the context of thinking about one’s values and vegetarianism: “A simple trick from the backyard
astronomer: if you are having trouble seeing something, look slightly away from it. The most light-sensitive 
parts of our eyes (those we need to see dim objects) are on the edges of the region we normally use for 
focusing. Eating animals has an invisible quality. Thinking about dogs, and their relationship to the animals 
we eat, is one way of looking askance and making something invisible visible.
```

Carl Shulman's [research advice](https://docs.google.com/document/d/1_yuuheVqp1quDfkuRcpoW_HO7jPaI7QnRjF1zl_VovU/edit) Google doc is great stuff. Here's some stuff I liked. 

Thinking habits:

```markdown
Frequently imagine what someone you respect, thinking you were wrong, would say/try to make the 
best argument against what you are currently thinking.

Pay attention to the use of contradictory epistemic standards and premises on different arguments/pattern
recognize them. Reconcile them or adjust your confidence in them.

I have gotten a fair amount out of trying to understand the epistemology and basic worldview of multiple
movements. I like ideological turing tests, mainly because they will highlight the best evidence for the
things they like, and against the things they hate, which helps in identifying areas to look into.

Make a habit of checking factual claims that you hear with a short Google search, and/or Wikipedia.

Use basic arithmetic and statistics frequently and briefly as part of thinking rather than as occasional
or separate exercises.

Try to convert qualitative claims into quantitative Fermi estimates whenever possible. Fermi calculations
with quick Wikipedia/Google/Wolfram/standard sources go far, and one can get into the habit and see how 
they typically evolve with different levels of depth.
```

Research management:

```markdown
Make a bookmarks folder to note critical arguments against positions you hold, and read it, so you don't
get isolated from different views.

Write casual thoughts up in explicit emails or blog posts to get feedback from people.

Have a giant folder of hundreds of blog post drafts to throw interesting ideas, citations, and links to.
I may not have published anything about X, but I have a draft in my blogging folder where I have stored 
assorted information about it.

Write down your views and check against your old views to see when you were wrong and when you were right.

Offer and accept bets about observables or change your predictions.
```

Information exposure:

```markdown
Reading broadly using multiple separate quality filters, that are independent, at least partially so, to
escape systematic biases of one or the other. For example, use multiple good link aggregation sites with
different authors and biases.

Look up expert opinion data (favoring subject expertise, science, IQ, incentives, track records), with an 
emphasis on trends as one looks at more elite groups (one has to go famous scientists to find really low 
belief in religion and psychic powers, so I try to predict what a better expert class would believe when 
there is a consistent trend).

Try to get datasets (Wikipedia lists, World Bank info, USDA, etc.) as a primary step in thinking about a 
question.

Use Google Scholar, and especially the ability to search for papers citing influential papers in a field.
```

Process for research project:

```markdown
You can quickly generate ideas by explaining to a questioner (GW style conversation notes, online 
conversations.)

I also like making hopefully-comprehensive taxonomies that one can work through in full to avoid selection
biases, e.g. looking through all the sectors of the economy, or all the major categories of philanthropy,
or all the major academic fields or think tanks. These are tools for hypothesis generation, and hitting 
on low-hanging fruit knowledge sources.

Thinking about end-goals and back-chaining to see things that would be relevant, rather than just using 
vague correlational criteria.

Look for high order bits, the biggest effects, as first priority, then incrementally add lower order 
refinements in accord with value of information.

Don't be too easily satisfied with cluster thinking.
```

<a name="#anki-in-research"></a>
### Anki in research
([overview](#overview))

From Michael Nielsen's [Augmenting long-term memory](http://augmentingcognition.com/ltm.html), where he describes his experience reading the AlphaGo paper:

```markdown
I find Anki a great help when reading research papers, particularly in fields outside my expertise.
As an example of how this can work, I'll describe my experience reading a 2016 paper describing AlphaGo,
the computer system from Google DeepMind that beat some of the world's strongest players of the game Go.

After the match where AlphaGo beat Lee Sedol, one of the strongest human Go players in history, I 
suggested to Quanta Magazine that I write an article about the system. AlphaGo was a hot media topic at 
the time, and the most common angle in stories was human interest, viewing AlphaGo as part of a long-
standing human-versus-machine narrative, with a few technical details filled in, mostly as color.

I wanted to take a different angle. Through the 1990s and first decade of the 2000s, I believed human-or-
better general artificial intelligence was far, far away. The reason was that over that time researchers 
made only slow progress building systems to do intuitive pattern matching, of the kind that underlies
human sight and hearing, as well as in playing games such as Go. Despite enormous effort by AI researchers,
many pattern-matching feats which humans find effortless remained impossible for machines.

While we made only very slow progress on this set of problems for a long time, around 2011 progress began
to speed up, driven by advances in deep neural networks. For instance, machine vision systems rapidly went
from being terrible to being comparable to human beings for certain limited tasks. By the time AlphaGo was
released, it was no longer correct to say we had no idea how to build computer systems to do intuitive 
pattern matching. While we hadn't yet nailed the problem, we were making rapid progress. AlphaGo was a big
part of that story, and I wanted my article to explore this notion of building computer systems to capture 
human intuition.

While I was excited, writing such an article was going to be difficult. It was going to require a deeper
understanding of the technical details of AlphaGo than a typical journalistic article. Fortunately, I knew
a fair amount about neural networks – I'd written a book about them. But I knew nothing about the game of 
Go, or about many of the ideas used by AlphaGo, based on a field known as reinforcement learning. I was 
going to need to learn this material from scratch, and to write a good article I was going to need to really
understand the underlying technical material.

Here's how I went about it.

I began with the AlphaGo paper itself. I began reading it quickly, almost skimming. I wasn't looking for a 
comprehensive understanding. Rather, I was doing two things. One, I was trying to simply identify the most 
important ideas in the paper. What were the names of the key techniques I'd need to learn about? Second, 
there was a kind of hoovering process, looking for basic facts that I could understand easily, and that
would obviously benefit me. Things like basic terminology, the rules of Go, and so on.

Here's a few examples of the kind of question I entered into Anki at this stage: “What's the size of a Go
board?”; “Who plays first in Go?”; “How many human game positions did AlphaGo learn from?”; “Where did 
AlphaGo get its training data?”; “What were the names of the two main types of neural network AlphaGo used?”

As you can see, these are all elementary questions. They're the kind of thing that are very easily picked
up during an initial pass over the paper, with occasional digressions to search Google and Wikipedia, and
so on. Furthermore, while these facts were easy to pick up in isolation, they also seemed likely to be 
useful in building a deeper understanding of other material in the paper.

I made several rapid passes over the paper in this way, each time getting deeper and deeper. At this stage 
I wasn't trying to obtain anything like a complete understanding of AlphaGo. Rather, I was trying to build 
up my background understanding. At all times, if something wasn't easy to understand, I didn't worry about 
it, I just keep going. But as I made repeat passes, the range of things that were easy to understand grew 
and grew. I found myself adding questions about the types of features used as inputs to AlphaGo's neural 
networks, basic facts about the structure of the networks, and so on.

After five or six such passes over the paper, I went back and attempted a thorough read. This time the 
purpose was to understand AlphaGo in detail. By now I understood much of the background context, and it was
relatively easy to do a thorough read, certainly far easier than coming into the paper cold. Don't get me
wrong: it was still challenging. But it was far easier than it would have been otherwise.

After doing one thorough pass over the AlphaGo paper, I made a second thorough pass, in a similar vein. Yet 
more fell into place. By this time, I understood the AlphaGo system reasonably well. Many of the questions I
was putting into Anki were high level, sometimes on the verge of original research directions. I certainly 
understood AlphaGo well enough that I was confident I could write the sections of my article dealing with it.
(In practice, my article ranged over several systems, not just AlphaGo, and I had to learn about those as well,
using a similar process, though I didn't go as deep.) I continued to add questions as I wrote my article, 
ending up adding several hundred questions in total. But by this point the hardest work had been done.

Of course, instead of using Anki I could have taken conventional notes, using a similar process to build up an
understanding of the paper. But using Anki gave me confidence I would retain much of the understanding over the
long term. A year or so later DeepMind released papers describing followup systems, known as AlphaGo Zero and
AlphaZero. Despite the fact that I'd thought little about AlphaGo or reinforcement learning in the intervening
time, I found I could read those followup papers with ease. While I didn't attempt to understand those papers 
as thoroughly as the initial AlphaGo paper, I found I could get a pretty good understanding of the papers in less
than an hour. I'd retained much of my earlier understanding!

By contrast, had I used conventional note-taking in my original reading of the AlphaGo paper, my understanding
would have more rapidly evaporated, and it would have taken longer to read the later papers. And so using Anki
in this way gives confidence you will retain understanding over the long term. This confidence, in turn, makes
the initial act of understanding more pleasurable, since you believe you're learning something for the long haul,
not something you'll forget in a day or a week.

This entire process took a few days of my time, spread over a few weeks. That's a lot of work. However, the 
payoff was that I got a pretty good basic grounding in modern deep reinforcement learning. This is an immensely 
important field, of great use in robotics, and many researchers believe it will play an important role in
achieving general artificial intelligence. With a few days work I'd gone from knowing nothing about deep 
reinforcement learning to a durable understanding of a key paper in the field, a paper that made use of many 
techniques that were used across the entire field. Of course, I was still a long way from being an expert. There 
were many important details about AlphaGo I hadn't understood, and I would have had to do far more work to build
my own system in the area. But this foundational kind of understanding is a good basis on which to build deeper
expertise.
```

This quote is key -- it jives with my own experience trying to memorize random disconnected facts (along with other failure modes):

```markdown
It's notable that I was reading the AlphaGo paper in support of a creative project of my own, namely,
writing an article for Quanta Magazine. This is important: I find Anki works much better when used in 
service to some personal creative project.

It's tempting instead to use Anki to stockpile knowledge against some future day, to think “Oh, I should
learn about the geography of Africa, or learn about World War II, or […]”. These are goals which, for 
me, are intellectually appealing, but which I'm not emotionally invested in. I've tried this a bunch of 
times. It tends to generate cold and lifeless Anki questions, questions which I find hard to connect to 
upon later review, and where it's difficult to really, deeply internalize the answers. The problem is 
somehow in that initial idea I “should” learn about these things: intellectually, it seems like a good 
idea, but I've little emotional commitment.

By contrast, when I'm reading in support of some creative project, I ask much better Anki questions. I 
find it easier to connect to the questions and answers emotionally. I simply care more about them, and 
that makes a difference. So while it's tempting to use Anki cards to study in preparation for some 
(possibly hypothetical) future use, it's better to find a way to use Anki as part of some creative project.
```

But what about shallow reads of papers? Does Anki-based reading help? Yes, says Nielsen:

```markdown
Most of my Anki-based reading is much shallower than my read of the AlphaGo paper. Rather than spending 
days on a paper, I'll typically spend 10 to 60 minutes, sometimes longer for very good papers. Here's a 
few notes on some patterns I've found useful in shallow reading.

As mentioned above, I'm usually doing such reading as part of the background research for some project.
I will find a new article (or set of articles), and typically spend a few minutes assessing it. Does the
article seem likely to contain substantial insight or provocation relevant to my project – new questions,
new ideas, new methods, new results? If so, I'll have a read.

This doesn't mean reading every word in the paper. Rather, I'll add to Anki questions about the core
claims, core questions, and core ideas of the paper. It's particularly helpful to extract Anki questions
from the abstract, introduction, conclusion, figures, and figure captions. Typically I will extract anywhere
from 5 to 20 Anki questions from the paper. It's usually a bad idea to extract fewer than 5 questions – 
doing so tends to leave the paper as a kind of isolated orphan in my memory. Later I find it difficult to 
feel much connection to those questions. Put another way: if a paper is so uninteresting that it's not 
possible to add 5 good questions about it, it's usually better to add no questions at all. ...

Really good resources are worth investing time in. But most papers don't fit this pattern, and you quickly 
saturate. If you feel you could easily find something more rewarding to read, switch over. It's worth 
deliberately practicing such switches, to avoid building a counter-productive habit of completionism in your
reading. It's nearly always possible to read deeper into a paper, but that doesn't mean you can't easily be 
getting more value elsewhere. It's a failure mode to spend too long reading unimportant papers.
```

Nielsen says you can also do 'syntopic' reading of entire subfields' research literature,thereby allowing mortals you and me to become Luke Muehlhauser:

```markdown
There's also a sense in which it's possible to use Anki not just to read papers, but to “read” the entire
research literature of some field or subfield. Here's how to do it.

You might suppose the foundation would be a shallow read of a large number of papers. In fact, to really
grok an unfamiliar field, you need to engage deeply with key papers – papers like the AlphaGo paper. What
you get from deep engagement with important papers is more significant than any single fact or technique:
you get a sense for what a powerful result in the field looks like. It helps you imbibe the healthiest norms
and standards of the field. It helps you internalize how to ask good questions in the field, and how to put
techniques together. You begin to understand what made something like AlphaGo a breakthrough – and also its 
limitations, and the sense in which it was really a natural evolution of the field. Such things aren't 
captured individually by any single Anki question. But they begin to be captured collectively by the questions
one asks when engaged deeply enough with key papers.

So, to get a picture of an entire field, I usually begin with a truly important paper, ideally a paper 
establishing a result that got me interested in the field in the first place. I do a thorough read of that 
paper, along the lines of what I described for AlphaGo. Later, I do thorough reads of other key papers in
the field – ideally, I read the best 5-10 papers in the field. But, interspersed, I also do shallower reads
of a much larger number of less important (though still good) papers. In my experimentation so far that
means tens of papers, though I expect in some fields I will eventually read hundreds or even thousands of
papers in this way.

You may wonder why I don't just focus on only the most important papers. Part of the reason is mundane: it
can be hard to tell what the most important papers are. Shallow reads of many papers can help you figure out
what the key papers are, without spending too much time doing deeper reads of papers that turn out not to be
so important. But there's also a culture that one imbibes reading the bread-and-butter papers of a field: a
sense for what routine progress looks like, for the praxis of the field. That's valuable too, especially for 
building up an overall picture of where the field is at, and to stimulate questions on my own part. Indeed, 
while I don't recommend spending a large fraction of your time reading bad papers, it's certainly possible to
have a good conversation with a bad paper. Stimulus is found in unexpected places.

Over time, this is a form of what Mortimer Adler and Charles van Doren dubbed *syntopic reading*. I build up
an understanding of an entire literature: what's been done, what's not yet been done. Of course, it's not 
literally reading an entire literature. But functionally it's close. I start to identify open problems, 
questions that I'd personally like answered, but which don't yet seem to have been answered. I identify tricks,
observations that seem pregnant with possibility, but whose import I don't yet know. And, sometimes, I 
identify what seem to me to be field-wide blind spots. I add questions about all these to Anki as well. In 
this way, Anki is a medium supporting my creative research. It has some shortcomings as such a medium, since
it's not designed with supporting creative work in mind – it's not, for instance, equipped for lengthy, 
free-form exploration inside a scratch space. But even without being designed in such a way, it's helpful as
a creative support.
```

See also [Eric Drexler on how to learn about everything](#polymathy), as well as more from Nielsen [here](#augmenting-long-term-memory).

<a name="#research-mindset"></a>
### Research mindset
([overview](#overview))

More by Carl Shulman from [the same document above](https://docs.google.com/document/d/1_yuuheVqp1quDfkuRcpoW_HO7jPaI7QnRjF1zl_VovU/edit), this time on research mindset:

```markdown
Don't dismiss ideas as unthinkable (rather than actions as subject to strong injunctions): things that
people are afraid of thinking about (because it might make them look bad, might imply bad news, is 
unpopular) have an elevated chance of offering low-hanging fruit for thinking.

Have a strong emotional revulsion to self-delusion and sloppy reasoning/research, including people 
Wrong on the Internet within communities you have some affiliation with.

Listen to yourself if something seems troubling, and try articulating, exploring, and steel-manning 
that intuition in multiple ways until it makes sense in a way that can be integrated with other knowledge 
(with whatever updates/revisions follow) or goes away. Don't just run roughshod over 'system 1' feelings.

Being comfortable with your own personality, emotions, and desires can help with being willing to do that 
kind of analysis, by making fewer conclusions unacceptable to you (empirical ones in particular).

Rigid ideological systems in a lot of tension with your real goals can be a problem there. E.g. in
Mormonism or utilitarianism or social justice, various empirical conclusions combine with the ideology
to recommend ruining your life, and people are strongly conditioned to avoid them. 

Recognizing partial, as opposed to impartial, motives (personal projects, selfishness, family, tribalism) 
and not trying to rationalize everything with a 100% impartial facade, can help more comfortably think
about questions like average well-being, or the real trade-off between burnout and effort, etc.
```

<a name="#Why-I-left-academia"></a>
### Why I left academia
([overview](#overview))

I like [research distillation](https://distill.pub/2017/research-debt/) more than research "generation", but:

```markdown
Lots of people want to work on research distillation. Unfortunately, it’s very difficult 
to do so, because we don’t support them.

There is a strange kind of informal support for people working on research distillation.
Christopher (Olah) has personally benefitted a great deal from this. But it’s unreliable
and not widely advertised, which makes it hard to build a career on.

An aspiring research distiller lacks many things that are easy to take for granted: 
a career path, places to learn, examples and role models. Underlying this is a deeper issue: 
their work isn’t seen as a real research contribution. 
```

From [Richard Wills' answer](https://qr.ae/TW1GCR) to the Quora question "What made you decide to leave academia after being in a PhD program in Mathematics or Physics?":

```markdown
After five years of grad school (at Caltech, in the lab of a Nobel Laureate) and three years
of post-doc’ing (at UCLA and Caltech), I decided to get out of academic science and find 
something personally more rewarding. Why?

1. Although I really enjoyed “doing” science, that is, actually dreaming up and doing the 
experiments, that was not what I saw Assistant Professors (my next step up in the academic 
ladder) doing.

2. What I saw them doing was managing a lab (consisting of others), which I had no desire to
do, and spending great gobs of time and energy applying for research grants, which I had no 
desire whatsoever to do. Frankly, being an Assistant Professor and working my way up from 
there just did not appeal. Among other reasons, I hate committee work and find sitting in 
committee meetings boring as hell. I admit it: I’m an introvert and not a team player. I
want to be left alone to do my own thing (for which grad school and post-doc’ing were ideal).

3. I also wanted to have the choice of where I and my family would live and had no desire 
to live in some podunk town where I might get a job as an Assistant Professor. I had lived 
in LA and its environs for eight years, and I had no desire to leave. (Am I the minority 
of one who actually enjoys LA?)

4. Also, I will admit that it became clear to me that after grad school and post-doc’ing,
while I may be smart and creative, I was not the next Feynman, Watson, or Crick, and that 
I was not going to get a faculty job at Caltech, Stanford, Harvard, etc., as when I had begun
this process, I had initially thought that I might. That changed my perspectives to where it
became more important to me to find myself in a life’s work that I enjoyed than being the top
dog in it.

5. Lastly, although it was not one of the reasons I left science for the law, one pleasant
outcome of that has been that I imagine lawyering has left me far more financially secure 
than remaining in academic science would have. (I was making a multiple of what a top
professor in the Ivies made. But far more to the point, I got to do what I enjoyed, work for
the clients I enjoyed, work on the legal challenges that I enjoyed, be my own Boss —- what’s
not to like about that?
```

Another big honest reason is that doing a PhD is hard, and it wasn't really the kind of thing I wanted to do. Kay Aull expounds on this articulately in [his answer](https://qr.ae/TW1uAY) to the Quora question "What are the 3 hardest things about doing a PhD?":

```markdown
**1) It's actually hard.**

When you're in school, it's hard to see how much work has gone into the knowledge you're
presented with. I don't just mean the brilliant mind that came up with whatever the raw 
material was - a theorem, a symphony, whatever. There's an insane amount of unheralded 
work that packages these raw inputs and breaks them into digestible chunks. What is the 
correct notation for working with these concepts? What is the most logical way to explain 
them to newcomers? What's important about this particular thing?

If you have an aptitude for your field, you may get through undergrad thinking that it's 
easy. Everything is self-evident. It all "makes sense". You don't realize that the well-
paved road you're walking on is surrounded on all sides by thick jungle. Now it's your 
turn to extend the road, and suddenly EVERYTHING is hard. Maybe you'll be the genius who 
invents the machete. More likely you won't be, and you'll spend the next five years 
clearing a few meters of jungle with your hands and teeth. You'll look back and think...
that was IT?! That's all I did? The difficulty gap between understanding stuff that's 
already been done, versus being the first person to do the stuff, spans a lot more orders 
of magnitude than people think.

**2) Success is ill-defined.**

Ever hear of SMART goals? Specific, Measurable, Achievable, Relevant, Timed. Grad school is
the polar opposite of that. You do get feedback, but it's typically worse than useless. 
(I’m not impressed yet. Your project just lacks that special something. If you were worthy, 
you'd already know what to do, and in fact you'd be done already.)

It is very hard to maintain a consistent level of productivity in the radical absence of 
SMART goals. No, "graduate" does not count as a goal, because there is no concrete action 
attached. You're wandering in the fog, and you have no idea if you're heading the right 
direction. You can go months, even years, without feeling like it mattered that you got out
of bed that day. If there's a more effective way to induce depression in otherwise healthy 
people, I've yet to see it.

When you're in regular school, there are assignments. When you're at work, there are
deadlines and deliverables. The rest of the world is organized this way for a reason. People
need SMART goals.

**3) Your eggs are in one basket.**

As a PhD student, you are an apprentice. Your only purpose is to convince the master that you
are worthy to join the guild. It will be his or her word that launches you to greatness. Or...
not. Then you will have the choice between being underemployed until you die or starting from
zero in a field that doesn't require specialized training. It's almost impossible to get 
another PhD position if you had a falling out with your advisor. In the real world, unless you
end up in jail or on the evening news, there are no career-ending mistakes. You can get another
job, even if your last boss hates you. Your experience in that last job still counts for 
something. It's not pass/fail.

Likewise, you have one PhD project. You may have had a few to choose from as a beginning
student, but now this one is yours. It's your identity. This is your chance to prove you can
do independent research. Or...not. Your advisor's research program cannot fail, but it can be
failed. By you. Make it work, or else. (If you disprove your advisor's pet hypothesis, which 
is THEIR identity, who do you think they will blame? Even if you merely show infeasibility,
this won't end well for you.)

In the real world, projects fail. That's a good thing! Fail fast and pivot quickly. Most people
have several projects going at once, so the loss of one isn't devastating. Sure, people get 
attached to their ideas, but there's enough pressure from above that people are generally forced
to cull the weaklings. Projects are not allowed to limp along for years (or decades!) just
because someone's ego won't allow them to admit defeat. "Oh yeah, Aull's Magnificent Theorem. 
There's a whole page of counterexamples on Wikipedia. I tried to tell her that, but she said it
was fake news. Also she called me a poopyhead." That stuff is NOT tolerated in industry, thank 
the merciful nerd gods. Let's just say that I have developed an allergy after repeated exposure.

It's not healthy to stake your entire future and self-worth on the success of one project. And 
it's much worse when the project is hard, when its ultimate failure may be due to factors
outside your control, and when you're wandering around with no concrete markers of progress. Even
patient, level-headed, and naturally optimistic people come out with scars.
```

Another reason is that academia rewards a very particular kind of research, and that there are outlets/avenues in industry that reward other kinds of research, including ones you wouldn't expect e.g. product design, per [Bryan Sim's answer](https://qr.ae/TW1uHs) to "What made you decide that academia is not for you?":

```markdown
For me, it was when I discovered what type of scientist I was.

In my opinion, there are two types of people who get into academia: People who care about what
they are studying, and people who care about how they study it.

At some point, I realized I was being pressured to become someone I didn’t want to be. I care 
about how we gain knowledge about the world. And over time, I found that I was happy doing research
on trivial things for fun, but that I didn’t enjoy “building houses” or “castles”, or whatever 
analogy was being used to denote one’s legacy of research. I especially didn’t enjoy the notion 
that I had to “defend” my line of research from evidence to the contrary (why can’t I/we just be
wrong?).

Today, I realize that I can be a scientist anywhere, as long as my job involves learning about some
aspect of reality the “right” way, with people I enjoy working with. This, if you think about it,
is the core of any good product design process: You work together with a team to figure out if your
hypothesis of what people want is true, and the results speak for themselves, whether in terms of 
user growth or $$$ growth.
```

Another is the huge incentive/pressure to hyperspecialize, with few exceptions (mostly involving being very smart), per [Colleen Farrelly's answer](https://qr.ae/TW1uHk) to same:

```markdown
When I was working on one of my three graduate research projects, a professor with whom I was
working congratulated me on a breakthrough and mentioned that I could make a career out of 
studying that method and helping other researchers apply it. The thought of studying one problem
basically to the exclusion of anything else was enough to make me literally run away. I was 
already sick of that project and not being able to move on to something new until everything was 
perfect and everyone liked the paper. I could never spend my whole life doing one thing. I 
couldn't spend a year doing one thing. Data science in industry allows me to dabble in all types 
of algorithms, advanced mathematics (like differential geometry, graph theory, algebraic
topology...), and work on a variety of projects at once.

This anecdote is but one of many reasons, and I've found a lot of academics end up pigeon-holed or
teaching classes they don't enjoy teaching, confined by grants when something interesting comes up.
Industry research is a lot faster and more varied, and I don't have to deal with 18-year-olds' 
parents arguing for better grades or graduate students cheating on exams and claiming "cultural 
differences" made them cheat...
```

<a name="#distillation-and-research-debt"></a>
### Distillation and research debt
([overview](#overview))

I keep coming back to Chris Olah and Shan Carter's essay [Research debt](https://distill.pub/2017/research-debt/), which in my eyes makes it an all-time great. The key terms are 'research debt', 'interpretive labor', and 'distillation'.

What is research debt? First, a parable:

```markdown
Achieving a research-level understanding of most topics is like climbing a mountain. Aspiring 
researchers must struggle to understand vast bodies of work that came before them, to learn 
techniques, and to gain intuition. Upon reaching the top, the new researcher begins doing novel
work, throwing new stones onto the top of the mountain and making it a little taller for whoever 
comes next.

Mathematics is a striking example of this. For centuries, countless minds have climbed the mountain 
range of mathematics and laid new boulders at the top. Over time, different peaks formed, built on
top of particularly beautiful results. Now the peaks of mathematics are so numerous and steep that
no person can climb them all. Even with a lifetime of dedicated effort, a mathematician may only
enjoy some of their vistas.

People expect the climb to be hard. It reflects the tremendous progress and cumulative effort that’s
gone into mathematics. The climb is seen as an intellectual pilgrimage, the labor a rite of passage.
But the climb could be massively easier. It’s entirely possible to build paths and staircases into 
these mountains. The climb isn’t something to be proud of.

The climb isn’t progress: the climb is a mountain of debt.
```

On to research debt, analogizing to programming and management:

```markdown
Programmers talk about technical debt: there are ways to write software that are faster in the short 
run but problematic in the long run. Managers talk about institutional debt: institutions can grow 
quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.

Research can also have debt. It comes in several forms:

Poor Exposition – Often, there is no good explanation of important ideas and one has to struggle to
understand them. This problem is so pervasive that we take it for granted and don’t appreciate how 
much better things could be.
Undigested Ideas – Most ideas start off rough and hard to understand. They become radically easier
as we polish them, developing the right analogies, language, and ways of thinking.
Bad abstractions and notation – Abstractions and notation are the user interface of research, shaping
how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop
even when they’re bad. For example, an object with extra electrons is negative. 
Noise – Being a researcher is like standing in the middle of a construction site. Countless papers 
scream for your attention and there’s no easy way to filter or summarize them. 2 We think noise is 
the main way experts experience research debt.

The insidious thing about research debt is that it’s normal. Everyone takes it for granted, and 
doesn’t realize that things could be different. For example, it’s normal to give very mediocre 
explanations of research, and people perceive that to be the ceiling of explanation quality. On the
rare occasions that truly excellent explanations come along, people see them as one-off miracles
rather than a sign that we could systematically be doing better.
```

Research debt is the accumulation of missing interpretive labor:

```markdown
There’s a tradeoff between the energy put into explaining an idea, and the energy needed to understand
it. On one extreme, the explainer can painstakingly craft a beautiful explanation, leading their
audience to understanding without even realizing it could have been difficult. On the other extreme,
the explainer can do the absolute minimum and abandon their audience to struggle. This energy is called
interpretive labor.

Many explanations are not one-to-one. People give lectures, write books, or communicate online. In these
one-to-many cases, each member of the audience pays the cost of understanding, even though the cost of 
explaining stays the same. 3 As a result, the cost of understanding has a multiplier in the interpretive
labor tradeoff — sometimes a huge multiplier.

In research, we often have a group of researchers all trying to understand each other. Just like before,
the cost of explaining stays constant as the group grows, but the cost of understanding increases with 
each new member. At some size, the effort to understand everyone else becomes too much. As a defense 
mechanism, people specialize, focusing on a narrower area of interest. The maintainable size of the field
is controlled by how its members trade off the energy between communicating and understanding.

Research debt is the accumulation of missing interpretive labor. It’s extremely natural for young ideas to
go through a stage of debt, like early prototypes in engineering. The problem is that we often stop at that
point. Young ideas aren’t ending points for us to put in a paper and abandon. When we let things stop there
the debt piles up. It becomes harder to understand and build on each other’s work and the field fragments.
```

The importance of good 'user interfaces' for ideas:

```markdown
It’s worth being clear that research debt isn’t just about ideas not being explained well. It’s a lack 
of digesting ideas – or, at least, a lack of the public version of ideas being digested. It’s a communal
messiness of thought.

Developing good abstractions, notations, visualizations, and so forth, is improving the user interfaces
for ideas. This helps both with understanding ideas for the first time and with thinking clearly about 
them. Conversely, if we can’t explain an idea well, that’s often a sign that we don’t understand it as
well as we could.
```

A great real-world example of research debt killing a field is Bill Thurston and foliations. Here's his reminiscences in my favorite paper of his, [On proof and progress in mathematics](https://arxiv.org/pdf/math/9404236v1.pdf):

```markdown
At that time, foliations had become a big center of attention among geometric 
topologists, dynamical systems people, and differential geometers. I fairly rapidly
proved some dramatic theorems. I proved a classification theorem for foliations,
giving a necessary and sufficient condition for a manifold to admit a foliation. I
proved a number of other significant theorems. I wrote respectable papers and
published at least the most important theorems. It was hard to find the time to
write to keep up with what I could prove, and I built up a backlog.

An interesting phenomenon occurred. Within a couple of years, a dramatic evacuation 
of the field started to take place. I heard from a number of mathematicians that they
were giving or receiving advice not to go into foliations—they were saying that Thurston
was cleaning it out. People told me (not as a complaint, but as a compliment) that I was
killing the field. Graduate students stopped studying foliations, and fairly soon, I turned
to other interests as well. ...

I do not think that the evacuation occurred because the territory was intellectually exhausted—
there were (and still are) many interesting questions that remain and that are probably 
approachable. ... I believe that two ecological effects were much more important in putting a
damper on the subject than any exhaustion of intellectual resources that occurred.

First, the results I proved (as well as some important results of other people)
were documented in a conventional, formidable mathematician’s style. They depended heavily on 
readers who shared certain background and certain insights. 
The theory of foliations was a young, opportunistic subfield, and the background
was not standardized. I did not hesitate to draw on any of the mathematics I
had learned from others. The papers I wrote did not (and could not) spend much
time explaining the background culture. They documented top-level reasoning and
conclusions that I often had achieved after much reflection and effort. I also threw
out prize cryptic tidbits of insight, such as “the Godbillon-Vey invariant measures
the helical wobble of a foliation”, that remained mysterious to most mathematicans
who read them. This created a high entry barrier: I think many graduate students
and mathematicians were discouraged that it was hard to learn and understand the
proofs of key theorems.
```

<a name="#Erisology-and-thinking-less-wrongly"></a>
## Erisology and thinking less wrongly
([overview](#overview))

From John Nerst's [What is erisology?](https://everythingstudies.com/what-is-erisology/), the founding document of the discipline:

```markdown
“Disagreement” can mean many things, but this is what I have in mind: A lot of online discourse
is hostile and often needlessly adversarial (I trust no one needs to be convinced of this). 
Specifically, a lot of this disagreement is dysfunctional, by which I mean that it results from 
(or is exacerbated by) one or both of the parties, intentionally or unintentionally, misunderstanding
the other party’s position or the nature of their differences. ...

Erisology is the study of this dysfunction and, theoretically, the attempt to fix some of it by
making people more aware of how it happens and how it doesn’t always need to happen. 

...a lot of different research paradigms and philosophical frameworks are in play when people talk
about anything even remotely abstract and/or ambiguous. And behind disagreement on even the most
concrete things there is often one or several undissolved philosophical issues being discussed by 
proxy, and at the same time the discussion process itself is disturbed by all kinds of corrupting
psychological and social influences.

An erisology research program would try to integrate basic insights and models from all these fields.
It would involve describing and cataloging the kinds of issues that hide under the surface in
dysfunctional discourse and the processes that make us unaware of them, contributing to the problem.
Ultimately and hypothetically the goal would be to improve discourse by creating and spreading ideas
and mental tools that work to defuse unnecessary conflict before it occurs, as well as clarify
necessary conflict so we know what it’s really about.
```

Erisology would necessarily be cross-discplinary, which is music to my ears. A sample:

```markdown
The study of cognitive biases and how they affect our thinking, breaking it in particular predictable
ways.

Traditional philosophy and its discussion of the nature of categories, objects and properties; a 
staggering proportion of online verbal conflict concern, at its core, some variety of the question
of what category something belongs in. The pitfall here is that people act as though such questions
have true answers when in most cases they don’t – making it possible for two people to both be right
while contradicting each other.

Data analysis and its understanding of the relationships between models and data, clusters and 
categories, axes and properties. Statistical modeling and interpretation issues mirror a lot of the
problems that arise when people use their own particular experiences to build models of how the world
works (which later clash with those of others).

Cognitive and perceptual psychology, for insights in how we form concepts in the brain and how they
affect our perceptions and interpretations of what we see, giving rise to differences between people 
we have a tough time understanding because they are so fundamental to our mental function they slip 
out of awareness. Also useful is how attitudes and opinions are sometimes the downstream result of 
ultimately physiological differences in perception and emotion.

Personality psychology, for differences between people that may create hard-to-comprehend, subconscious 
divisions.

Poststructuralist theory and its conception of language as being inherently slippery and devoid of 
ultimate, definitive meaning. Our intuitive blindness to this causes us to misinterpret things other
people say without realizing it.

Rhetoric, the art of persuasion, is partly useful because it’s a practice more than a science and as
far as I know lack theory that grounds “what works and what doesn’t” in human psychology.

Anthropology and its examination of how many things we take for granted in our societies are non-obvious
and somewhat arbitrary.

Literary theory and its treatment of narratives, their interpretation and how they cannot be definitive
or claim absolute truth.

Epistemology, and how people take for granted different approaches to knowledge. I’m not talking so much
about explicit differences like “personal revelation vs. scientific study” but more underlying differences
like the balance between personal experience and statistics, empirical data vs. theoretical considerations.
These differences are sometimes discussed but are present as an important factor in way more contexts than
they are explicitly talked about.

Sociology and history and their theories of social construction, which are very useful when not overstated
and used as a political bludgeon. They also have valuable insights about how the design of technology and 
institutions shape behavior.

Evolutionary psychology and social instincts, especially those related to intra- and intergroup conflict 
like argumentation, rivalry, social status, identity and dehumanization. An important aspect here is also
recognizing that modern large-scale societies is an extremely unnatural social structure for humans and
this gives rise to all kinds of weird effects.

Computer science, specifically insights from attempts to create artificial intelligence and the
difficulties of modeling human concepts. Writing software also give you good habits, since it often makes
you understand that accurately modeling reality is way more complicated that you first thought.

Not the purview of any particular field, but understanding reductionism and its discontents are important
to a lot of erisology covering the often disappointing interactions between academic disciplines. 
Differing attitudes to reductionism vs. inherent semantics makes people find different kinds of explanations
satisfactory.
```

<a name="#argumentative-charity"></a>
### Argumentative charity
([overview](#overview))

My go-to text for charity is Scott Alexander's [founding post](https://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/) for his blog SSC. Here's the part I liked, which is pretty much all of the 'transferable' part:

```markdown
Absurdity is the natural human tendency to dismiss anything you disagree with as so stupid it doesn’t
even deserve consideration. In fact, you are virtuous for not considering it, maybe even heroic! You’re
refusing to dignify the evil peddlers of bunkum by acknowledging them as legitimate debate partners.

Charity is the ability to override that response. To assume that if you don’t understand how someone
could possibly believe something as stupid as they do, that this is more likely a failure of understanding 
on your part than a failure of reason on theirs.
```

Charity should be thought of like [Chesterton's fence](#chestertons-fence).

Charity as default, because even when it's uncalled for or ostensibly unnecessary it can be advantageous:

```markdown
The most effective way to learn any subject is to try to figure out exactly why a wrong position is wrong. 
And sometimes even a complete disaster of a theory will have a few salvageable pearls of wisdom that can’t 
be found anywhere else. The rationalist forum Less Wrong teaches the idea of steelmanning, rebuilding a 
stupid position into the nearest intelligent position and then seeing what you can learn from it.
```

<a name="#chestertons-fence"></a>
### Chesterton's fence
([overview](#overview))

From Scott Alexander's [founding post](https://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/):

```markdown
There are many things charity is not. Charity is not a fuzzy-headed caricature-pomo attempt to say no one 
can ever be sure they’re right or wrong about anything. Once you understand the reasons a belief is 
attractive to someone, you can go ahead and reject it as soundly as you want. Nor is it an obligation to
spend time researching every crazy belief that might come your way. Time is valuable, and the less of it
you waste on intellectual wild goose chases, the better.

It’s more like Chesterton’s Fence. G.K. Chesterton gave the example of a fence in the middle of nowhere. A
traveller comes across it, thinks “I can’t think of any reason to have a fence out here, it sure was dumb 
to build one” and so takes it down. She is then gored by an angry bull who was being kept on the other side
of the fence.

Chesterton’s point is that “I can’t think of any reason to have a fence out here” is the worst reason to 
remove a fence. Someone had a reason to put a fence up here, and if you can’t even imagine what it was, it
probably means there’s something you’re missing about the situation and that you’re meddling in things you
don’t understand. None of this precludes the traveller who knows that this was historically a cattle farming
area but is now abandoned – ie the traveller who understands what’s going on – from taking down the fence.

As with fences, so with arguments. If you have no clue how someone could believe something, and so you decide
it’s stupid, you are much like Chesterton’s traveler dismissing the fence (and philosophers, like travelers, 
are at high risk of stumbling across bull.)
``` 

The community forum *The Scholar's Stage* has a great post, [Tradition is Smarter Than You Are](http://scholars-stage.blogspot.com/2018/08/tradition-is-smarter-than-you-are.html), that gives a couple examples of tradition-as-Chesterton's-fence. It begins like so:

```markdown
Brain-power alone is not enough to explain why humans are such a successful species. Humans, he 
(evolutionary anthropologist-cum-cross cultural psychologist Joseph Henrich) argues, are not nearly
as intelligent as we think they are. Remove them from the culture and environment they have learned 
to operate in and they fail quickly. His favorite example of this are European explorers who die in
the middle of deserts, jungles, or arctic wastes even though thousands of generations of hunter-
gatherers were able to survive and thrive in these same environments. If human success was due to our
ability to problem solve, analyze, and rationally develop novel solutions to novel challenges, the 
explorers should have been fine. Their ghastly fates suggest that rationality may not be the key to
human survival.
```

The key is cultural evolution. An example via the cassava plant, or manioc, from Heinrich's book [*The Secret of Our Success*](https://amzn.to/2LuDcEh):

```markdown
In the Americas, where manioc was first domesticated, societies who have relied on bitter varieties
for thousands of years show no evidence of chronic cyanide poisoning. In the Colombian Amazon, for
example, indigenous Tukanoans use a multistep, multiday processing technique that involves scraping,
grating, and finally washing the roots in order to separate the fiber, starch, and liquid. Once
separated, the liquid is boiled into a beverage, but the fiber and starch must then sit for two more
days, when they can then be baked and eaten. Figure 7.1 shows the percentage of cyanogenic content in
the liquid, fiber, and starch remaining through each major step in this processing. 

Such processing techniques are crucial for living in many parts of Amazonia, where other crops are 
difficult to cultivate and often unproductive. However, despite their utility, one person would have
a difficult time figuring out the detoxification technique. Consider the situation from the point of
view of the children and adolescents who are learning the techniques. They would have rarely, if ever,
seen anyone get cyanide poisoning, because the techniques work. And even if the processing was
ineffective, such that cases of goiter (swollen necks) or neurological problems were common, it would
still be hard to recognize the link between these chronic health issues and eating manioc. Most people
would have eaten manioc for years with no apparent effects. Low cyanogenic varieties are typically 
boiled, but boiling alone is insufficient to prevent the chronic conditions for bitter varieties. 
Boiling does, however, remove or reduce the bitter taste and prevent the acute symptoms (e.g., diarrhea,
stomach troubles, and vomiting). 

So, if one did the common-sense thing and just boiled the high-cyanogenic manioc, everything would seem
fine. Since the multistep task of processing manioc is long, arduous, and boring, sticking with it is
certainly non-intuitive. Tukanoan women spend about a quarter of their day detoxifying manioc, so this
is a costly technique in the short term. Now consider what might result if a self-reliant Tukanoan
mother decided to drop any seemingly unnecessary steps from the processing of her bitter manioc. She 
might critically examine the procedure handed down to her from earlier generations and conclude that 
the goal of the procedure is to remove the bitter taste. She might then experiment with alternative 
procedures by dropping some of the more labor-intensive or time-consuming steps. She’d find that with
a shorter and much less labor-intensive process, she could remove the bitter taste. Adopting this easier
protocol, she would have more time for other activities, like caring for her children. Of course, years
or decades later her family would begin to develop the symptoms of chronic cyanide poisoning. 

Thus, the unwillingness of this mother to take on faith the practices handed down to her from earlier 
generations would result in sickness and early death for members of her family. Individual learning does
not pay here, and intuitions are misleading. The problem is that the steps in this procedure are causally
opaque—an individual cannot readily infer their functions, interrelationships, or importance. The causal
opacity of many cultural adaptations had a big impact on our psychology. 

Wait. Maybe I’m wrong about manioc processing. Perhaps it’s actually rather easy to individually figure
out the detoxification steps for manioc? Fortunately, history has provided a test case. At the beginning
of the seventeenth century, the Portuguese transported manioc from South America to West Africa for the 
first time. They did not, however, transport the age-old indigenous processing protocols or the underlying
commitment to using those techniques. Because it is easy to plant and provides high yields in infertile 
or drought-prone areas, manioc spread rapidly across Africa and became a staple food for many populations.
The processing techniques, however, were not readily or consistently regenerated. Even after hundreds of
years, chronic cyanide poisoning remains a serious health problem in Africa. Detailed studies of local 
preparation techniques show that high levels of cyanide often remain and that many individuals carry low
levels of cyanide in their blood or urine, which haven’t yet manifested in symptoms. In some places,
there’s no processing at all, or sometimes the processing actually increases the cyanogenic content. On 
the positive side, some African groups have in fact culturally evolved effective processing techniques, 
but these techniques are spreading only slowly.

The point here is that cultural evolution is often much smarter than we are. Operating over generations 
as individuals unconsciously attend to and learn from more successful, prestigious, and healthier members
of their communities, this evolutionary process generates cultural adaptations. Though these complex 
repertoires appear well designed to meet local challenges, they are not primarily the products of 
individuals applying causal models, rational thinking, or cost-benefit analyses. Often, most or all of
the people skilled in deploying such adaptive practices do not understand how or why they work, or even 
that they “do” anything at all. Such complex adaptations can emerge precisely because natural selection
has favored individuals who often place their faith in cultural inheritance—in the accumulated wisdom 
implicit in the practices and beliefs derived from their forbearers—over their own intuitions and 
personal experiences.
```

Another example from northern Canada, where Naskapi foragers hunt caribou:

```markdown
When hunting caribou, Naskapi foragers in Labrador, Canada, had to decide where to go. Common sense might
lead one to go where one had success before or to where friends or neighbors recently spotted caribou. 
However, this situation is like Matching Pennies in chapter 2. The caribou are mismatchers and the 
hunters are matchers. That is, hunters want to match the locations of caribou while caribou want to 
mismatch the hunters, to avoid being shot and eaten. If a hunter shows any bias to return to previous 
spots, where he or others have seen caribou, then the caribou can benefit (survive better) by avoiding
those locations (where they have previously seen humans). 

Thus, the best hunting strategy requires randomizing. Can cultural evolution compensate for our 
cognitive inadequacies? Traditionally, Naskapi hunters decided where to go to hunt using divination and 
believed that the shoulder bones of caribou could point the way to success. To start the ritual, the shoulder
blade was heated over hot coals in a way that caused patterns of cracks and burnt spots to form. This 
patterning was then read as a kind of map, which was held in a pre-specified orientation. The cracking 
patterns were (probably) essentially random from the point of view of hunting locations, since the outcomes 
depended on myriad details about the bone, fire, ambient temperature, and heating process. Thus, these 
divination rituals may have provided a crude randomizing device that helped hunters avoid their own
decision-making biases. The undergraduates in the Matching Pennies game could have used a randomizing 
device like divination, though the chimps seem fine without it.
```

There's a similar practice in Indonesia employed by the Kantus of Kalimantan, who use bird augury to select locations for their agricultural plots:

```markdown
The anthropologist Michael Dove argues that two factors will cause farmers to make plot placements
that are too risky. First, Kantu ecological models contain the Gambler’s Fallacy and lead them to 
expect that floods will be less likely to occur in a specific location after a big flood in that 
location (which is not true). 

Second, as with the MBAs’ investment allocations in chapter 4, Kantus pay attention to others’ 
success and copy the choices of successful households, meaning that if one of their neighbors has
a good yield in an area one year, many other people will want to plant there in the next year. 
Reducing the risks posed by these cognitive and decision-making biases, the Kantu rely on a system
of bird augury that effectively randomizes their choices for locating garden plots, which helps them
avoid catastrophic crop failures. The results of divination depend not only on seeing a particular
bird species in a particular location, but also on what type of call the bird makes (one type of call 
may be favorable, and another unfavorable). 

The patterning of bird augury supports the view that this is a cultural adaptation. The system seems 
to have evolved and spread throughout this region since the seventeenth century when rice cultivation
was introduced. This makes sense, since it is rice cultivation that is most positively influenced by
randomizing garden locations. It’s possible that, with the introduction of rice, a few farmers began
to use bird sightings as an indication of favorable garden sites. On average, over a lifetime, these
farmers would do better—be more successful—than farmers who relied on the Gambler’s Fallacy or on 
copying others’ immediate behavior. 

Whatever the process, within 400 years, the bird augury system had spread throughout the agricultural
populations of this Borneo region. Yet it remains conspicuously missing or underdeveloped among local
foraging groups and recent adopters of rice agriculture, as well as among populations in northern 
Borneo who rely on irrigation. So, bird augury has been systematically spreading in those regions
where it is most adaptive. This example makes a key point: not only do people often not understand
what their cultural practices are doing, but sometimes it may even be important that they don’t
understand what their practices are doing or how they work. If people came to understand that bird 
augury or bone divination didn’t actually predict the future, the practice would probably be dropped
or people would increasingly ignore ritual findings in favor of their own intuitions.
```

There are two arguments Heinrich is making here:

```markdown
The first is that customs, traditions, and the like are subject to Darwinian selection. Henrich is not
always clear on exactly what is being selected for—is it individuals who follow a tradition, groups 
whose members all follow the tradition, or the tradition itself?—but the general gist is that traditions
stick around longest when they are adaptive. 

This process is "blind." Those who follow the traditions do not know how they work, and in some cases
(like religious rituals that build social solidarity) knowing the details of how they work might actually
reduce the efficacy of the tradition. That is the second argument of note: we do not (and often cannot)
understand just how the traditions we inherit help our survival, and because of that, it is difficult to
artificially create replacements. 
```

This, of course, jives perfectly with the work of James C. Scott. Here's *The Scholar's* summary intro:

```markdown
Scott has spent a large amount of his career studying the way states shape the societies they rule over, 
and the way societies try to resist the advance of the state. The central problem of ruler-ship, as Scott
sees it, is what he calls *legibility*. To extract resources from a population the state must be able to
understand that population. The state needs to make the people and things it rules legible to agents of
the government. Legibility means uniformity. States dream up uniform weights and measures, impress national
languages and ID numbers on their people, and divvy the country up into land plots and administrative 
districts, all to make the realm legible to the powers that be. 

The problem is that not all important things can be made legible. Much of what makes a society successful
is knowledge of the tacit sort: rarely articulated, messy, and from the outside looking in, purposeless. 
These are the first things lost in the quest for legibility. Traditions, small cultural differences, odd 
and distinctive lifeways—in other words, the products of cultural evolution that Henrich fills his book 
with—are all swept aside by a rationalizing state that preserves (or in many cases, imposes) only what it 
can be understood and manipulated from the 2,000 foot view. The result, as Scott chronicles with example
after example, are many of the greatest catastrophes of human history. 
```

<a name="#absurdity-heuristic"></a>
### Absurdity heuristic
([overview](#overview))

From Scott Alexander's [Talking Snakes: A Cautionary Tale](https://www.lesswrong.com/posts/atcJqdhCxTZiJSxo2/talking-snakes-a-cautionary-tale):

```markdown
I have read of the absurdity heuristic. I know that it is not carte blanche to go around rejecting
beliefs that seem silly. But I was still sympathetic to (Bill Maher's) talking snake argument. After
all... *a talking snake?*

I changed my mind in a Cairo cafe, talking to a young Muslim woman. I let it slip during the
conversation that I was an atheist, and she seemed genuinely curious why. You've all probably been in
such a situation, and you probably know how hard it is to choose just one reason, but I'd been 
reading about Biblical contradictions at the time and I mentioned the myriad errors and atrocities 
and contradictions in all the Holy Books.

Her response? "Oh, thank goodness it's that. I was afraid you were one of those crazies who believed
that monkeys transformed into humans."

I admitted that um, well, maybe I sorta kinda might in fact believe that.

It is hard for me to describe exactly the look of shock on her face, but I have no doubt that her
horror was genuine. I may have been the first flesh-and-blood evolutionist she ever met. "But..." she
looked at me as if I was an idiot. "Monkeys don't change into humans. What on Earth makes you think 
monkeys can change into humans?"

I admitted that the whole process was rather complicated. I suggested that it wasn't exactly a
Optimus Prime-style transformation so much as a gradual change over eons and eons. I recommended a few 
books on evolution that might explain it better than I could.

She said that she respected me as a person but that quite frankly I could save my breath because there 
was no way any book could possibly convince her that monkeys have human babies or whatever sort of
balderdash I was preaching. She accused me and other evolution believers of being too willing to accept
absurdities, motivated by our atheism and our fear of the self-esteem hit we'd take by accepting Allah
was greater than ourselves.

It is not clear to me that this woman did anything differently than Bill Maher. Both heard statements 
that sounded so crazy as to not even merit further argument. Both recognized that there was a large
group of people who found these statements plausible and had written extensive literature justifying
them. Both decided that the statements were so absurd as to not merit examining that literature more
closely. Both came up with reasons why they could discount the large number of believers because
those believers must be biased.

I post this as a cautionary tale as we discuss the logic or illogic of theism. I propose taking from 
it the following lessons:

- The absurdity heuristic doesn't work very well.

- Even on things that sound really, really absurd.

- If a large number of intelligent people believe something, it deserves your attention. After you've 
studied it on its own terms, then you have a right to reject it. You could still be wrong, though.

- Even if you can think of a good reason why people might be biased towards the silly idea, thus
explaining it away, your good reason may still be false.

- If someone cannot explain why something is not stupid to you over twenty minutes at a cafe, that
doesn't mean it's stupid. It just means it's complicated, or they're not very good at explaining things.

- There is no royal road.
```

Additional commentary by Eliezer Yudkowsky:

```markdown
Consider: If all the *rest* of the religious framework were granted, would the talking snake be an 
*additional problem*? No. The talking snake is only absurd if you refuse to grant the rest of the 
religious framework. The fact that a snake is talking is not, of itself, the source of any *additional*
problem - unless you were to argue that it fits the mode of a classic bias like minimal 
counterintuitiveness or thinking that "talking" is a simple feature that can easily be grafted on, etc.
But the point is, the part where a talking snake is in this story, is, presuming the story's other 
premises, not the proper subject of the dispute.

The problem is the other premises, and notions like sin passed down through generations, or that the 
sin was contained in an easily accessible tree put right there in the Garden (trap much?), or the fact
that a supernatural God is in the story - and so on and so on.

When you look at it from that perspective, then indeed, saying "Ha ha, a talking snake" is the exact
mirror image of saying "Ha ha, a monkey birthed a human", because it takes refuge in absurdity instead
of addressing the most important part of an argument as a whole.

The contradictions are a proper point of attack, but only if they would be really, genuinely troublesome 
*even granting the rest of the premises*.
```

Zaph:

```markdown
The use of absurdity seems more like a tool to enforce group norms than a means of conversion. That 
doesn't mean the beliefs aren't absurd, just that pointing out the absurdity of outsiders is common 
practice by in-group members. Most creationist-minded believers would use some similarly absurd way
of describing evolution, with the group benefit of passing along "evolution is stupid" meme. That said,
it is important to start to tease apart just how many other enforcement strategies are out there, as
they are going to need to be dealt with one by one.
```

<a name="#typical-mind-fallacy"></a>
### Typical mind fallacy
([overview](#overview))

Scott Alexander's LW post [Generalizing From One Example](https://www.lesswrong.com/posts/baTWMegR42PAsH9qJ/generalizing-from-one-example) begins with the following example of the "typical mind fallacy" by his old professor David Berman, about whether "imagination" was real or just a turn of phrase:

```markdown
There was a debate, in the late 1800s, about whether "imagination" was simply a turn of phrase or 
a real phenomenon. That is, can people actually create images in their minds which they see vividly,
or do they simply say "I saw it in my mind" as a metaphor for considering what it looked like?

Upon hearing this, my response was "How the stars was this actually a real debate? Of course we have
mental imagery. Anyone who doesn't think we have mental imagery is either such a fanatical Behaviorist 
that she doubts the evidence of her own senses, or simply insane." Unfortunately, the professor was 
able to parade a long list of famous people who denied mental imagery, including some leading scientists
of the era. And this was all before Behaviorism even existed.

The debate was resolved by Francis Galton, a fascinating man who among other achievements invented 
eugenics, the "wisdom of crowds", and standard deviation. Galton gave people some very detailed surveys,
and found that some people did have mental imagery and others didn't. The ones who did had simply 
assumed everyone did, and the ones who didn't had simply assumed everyone didn't, to the point of coming 
up with absurd justifications for why they were lying or misunderstanding the question. There was a wide
spectrum of imaging ability, from about five percent of people with perfect eidetic imagery to three
percent of people completely unable to form mental images.

Dr. Berman dubbed this the Typical Mind Fallacy: the human tendency to believe that one's own mental 
structure can be generalized to apply to everyone else's.

He kind of took this idea and ran with it. He interpreted certain passages in George Berkeley's biography
to mean that Berkeley was an eidetic imager, and that this was why the idea of the universe as sense-
perception held such interest to him. He also suggested that experience of consciousness and qualia were 
as variable as imaging, and that philosophers who deny their existence (Ryle? Dennett? Behaviorists?) were
simply people whose mind lacked the ability to easily experience qualia. In general, he believed philosophy
of mind was littered with examples of philosophers taking their own mental experiences and building theories
on them, and other philosophers with different mental experiences critiquing them and wondering why they
disagreed.
```

Now this is "about serious matters of mental structure". Scott noticed something similar going on with "matters of the psyche" -- a tendency to generalize from our personalities and behaviors. He gives two personal examples, introversion and noise sensitivity:

```markdown
For example, I'm about as introverted a person as you're ever likely to meet - anyone more introverted 
than I am doesn't communicate with anyone. All through elementary and middle school, I suspected that 
the other children were out to get me. They kept on grabbing me when I was busy with something and trying
to drag me off to do some rough activity with them and their friends. When I protested, they counter-
protested and told me I really needed to stop whatever I was doing and come join them. I figured they 
were bullies who were trying to annoy me, and found ways to hide from them and scare them off.

Eventually I realized that it was a double misunderstanding. They figured I must be like them, and the 
only thing keeping me from playing their fun games was that I was too shy. I figured they must be like me,
and that the only reason they would interrupt a person who was obviously busy reading was that they wanted
to annoy him.

Likewise: I can't deal with noise. If someone's being loud, I can't sleep, I can't study, I can't 
concentrate, I can't do anything except bang my head against the wall and hope they stop. I once had a noisy
housemate. Whenever I asked her to keep it down, she told me I was being oversensitive and should just
mellow out. I can't claim total victory here, because she was very neat and kept yelling at me for leaving
things out of place, and I told her she needed to just mellow out and you couldn't even tell that there was
dust on that dresser anyway. It didn't occur to me then that neatness to her might be as necessary and
uncompromisable as quiet was to me, and that this was an actual feature of how our minds processed information
rather than just some weird quirk on her part.

"Just some weird quirk on her part" and "just being oversensitive" are representative of the problem with the
typical psyche fallacy, which is that it's invisible. We tend to neglect the role of differently-built minds
in disagreements, and attribute the problems to the other side being deliberately perverse or confused. I 
happen to know that loud noise seriously pains and debilitates me, but when I say this to other people they
think I'm just expressing some weird personal preference for quiet. Think about all those poor non-imagers 
who thought everyone else was just taking a metaphor about seeing mental images way too far and refusing to 
give it up.
```

An example from Scott's experience with teaching -- see also [here](#teaching-and-learning):

```markdown
There's some evidence that the usual method of interacting with people involves something sorta like 
emulating them within our own brain. We think about how we would react, adjust for the other person's
differences, and then assume the other person would react that way. This method of interaction is very 
tempting, and it always feels like it ought to work.

But when statistics tell you that the method that would work on you doesn't work on anyone else, then
continuing to follow that gut feeling is a Typical Psyche Fallacy. You've got to be a good rationalist,
reject your gut feeling, and follow the data.

I only really discovered this in my last job as a school teacher. There's a lot of data on teaching 
methods that students enjoy and learn from. I had some of these methods...inflicted...on me during my 
school days, and I had no intention of abusing my own students in the same way. And when I tried the
sorts of really creative stuff I would have loved as a student...it fell completely flat. What ended up 
working? Something pretty close to the teaching methods I'd hated as a kid. Oh. Well. Now I know why 
people use them so much. And here I'd gone through life thinking my teachers were just inexplicably bad 
at what they did, never figuring out that I was just the odd outlier who couldn't be reached by this sort
of stuff.
```

Scott elaborates:

```markdown
Keeping in mind that I taught English as a second language to older elementary school children:

Ordinary teaching methods: constant repetition of unconnected topics followed by endless vapid games.
For example, a game of bingo with vocabulary words in each square. Attempts to trick children into 
thinking something was interesting; for example, calling vocabulary "word baseball" or something like that
and dressing up in a baseball cap while teaching it.

Things I predicted would work better: attempts to make material genuinely interesting, have each lesson 
build on the previous, and create links between different concepts. For example, a lesson on the days of
the week including a mini-presentation on the Norse gods after whom they were named, references to previous 
lessons when we had learned "sun" and "moon" for Sunday and Monday. Attempt to teach how to apply general 
principles instead of doing everything ad hoc.
```

Note that this all goes against the University of Missouri's [25 important findings on learning](#important-findings-on-learning), which is basically just an executive summary of the literature plus action items (themselves shortened rephrasings of the executive summaries), and yet they agree with Quoran teacher Matthew Bates' experience (see [his answers](https://www.quora.com/profile/Matthew-Bates-27)) so in the end I don't know. Maybe the former are talking about students from lower SES families (correlating with lower educational achievement)?

Dan Dennett gives an interesting illustration/test of mental imagery:

```markdown
Picture a 3 by 3 grid. Then picture the words "gas", "oil", and "dry" spelled downwards in the columns 
left to right in that order. Looking at the picture in your mind, read the words across on the grid.
```

Scientism points out a version of the typical mind fallacy in how people "vastly overestimate their own goodness" when it comes to "testimony on one's moral worth":

```markdown
Often "goodness" is just a way to dress up powerlessness. Like an overweight man might say he's "stocky" 
or an overweight woman might say she's "curvy," so an undesirable or shy man or woman might emphasize the 
upside: "I would never cheat." There's a version of the typical mind fallacy in there: a person might 
genuinely think they would never cheat but be extrapolating from a position where the opportunity rarely 
presents itself. We can all talk about how, if we were in a position of political power, we'd never succumb
to bribes or cronyism because we don't have any political power. It both makes us look good and, as far as 
we know, it's true. I think testimony, especially when it comes to ones moral worth, is the least valuable 
form of data available.
```

This segues naturall to the Milgram experiment, the Stanford prison experiment, and the case of Nazi-era German civilians:

```markdown
When I've taught ethics in the past, we always discuss the Nazi era. Not because the Nazis acted
unethically, but because of how everyone else acted.

For example, we read about the vans that carried Jewish prisoners that had the exhaust system designed
to empty into the van. The point is not how awful that is, but that there must have been an engineer 
somewhere who figured out the best way to design and build such a thing. And that engineer wasn't a Nazi
soldier, he or she was probably no different from anyone else at that time, with kids and a family and
friends and so on. Not an evil scientist in a lab, but just a design engineer in a corporation.

One point of the discussion is that "normal" people have acted quite unethically in the past, and how can
we prevent that happening to us.
```

From mental imagery to aural imagination, and their trainability, per cousin_it:

```markdown
Only this winter did I really understand that good musicians have vivid aural imagination, while I 
couldn't hear any sounds in my head, period. Immediately after this realization I started exercising. 
By now I can hear complete monophonic melodies, and (on good days) imagine two notes sounding at the 
same time. Classically trained conductors can imagine a complete orchestral sound while reading sheet
music. I don't see any reason why visual imagination can't be similarly trained.
```

How cousin_it trained the skill:

```markdown
The hardest part for me was the beginning, getting a toehold at any inner sound. Pick a note on 
the guitar - I started with D on the second string. Play it at a steady rhythm with rests, slowly
fading away into nothing. (Might not be possible on the piano or other instruments.) At some moment
the brain will start to "complete" the sound, even though by that point you're playing too softly to
hear. Catch that feeling, expand on it. When you can "do" several different notes, try playing a 
simple melody and hearing it afterwards. After you're comfortable with that, try to hear a simple major
scale without playing it immediately beforehand. Then work from unfamiliar sheet music without playing it
- solfege-sing in your mind - by now I can do this quite easily. And so on.
```

Prismattic points out how this plays out in books vs movies preferences:

```markdown
It struck me that I think you can still see the imagination debate playing out today. Consider the 
following conversation, which most people will have encountered a variant of at least once+:

-- Mr. Highbrow: It is better to read books than watch movies based on them. The movies limit you to 
someone else's perspective on the material, but the book gives maximum reign to your imagination.

-- Mr. Lowbrow: What are you smoking? The movie is an immersive experience that makes me feel like I'm 
really in the story. The book is just somebody else's description of the story.

Having thought about it, my highest-probability hypothesis is now that Mr. HB has more vivid mental 
imagery than does Mr. LB. Further introspection led me to realize that when I read fiction, I often have
very specific images of places and scenery, but usually only vague impressions of faces. When I watch 
film adaptions, I'm often struck that the setting is "wrong," but rarely have that feeling about the 
appearance of people (unless the actors are grossly divergent from the description of them in the book)
```

<a name="#teaching-and-learning"></a>
## Teaching and learning
([overview](#overview))

<a name="#errors-vs-bugs"></a>
## Errors vs bugs
([overview](#overview))

Sarah Constantin is one of my favorite writers. She has a blog, [Otium](https://srconstantin.wordpress.com/), and goes by 
celandine13 on LiveJournal. This quote on the _error vs bug model of learning_ comes from her LJ essay 
[Errors vs Bugs and the End of Stupidity](https://celandine13.livejournal.com/33599.html).

```markdown
A common mental model for performance is what I'll call the "error model."  In the error model, a person's 
performance of a musical piece (or performance on a test) is a perfect performance plus some random error.  
You can literally think of each note, or each answer, as x + c times epsilon_i, where x is the correct note 
/ answer, and epsilon_i is a random variable, iid Gaussian or something.  Better performers have a lower 
error rate c.  Improvement is a matter of lowering your error rate.  This, or something like it, is the model 
that underlies school grades and test scores. Your grade is based on the percent you get correct.  Your 
performance is defined by a single continuous parameter, your accuracy.

But we could also consider the "bug model" of errors.  A person taking a test or playing a piece of music is 
executing a program, a deterministic procedure.  If your program has a bug, then you'll get a whole class of 
problems wrong, consistently.  Bugs, unlike error rates, can't be quantified along a single axis as less or 
more severe.  A bug gets everything that it affects wrong.  And fixing bugs doesn't improve your performance 
in a continuous fashion; you can fix a "little" bug and immediately go from getting everything wrong to 
everything right.  You can't really describe the accuracy of a buggy program by the percent of questions it 
gets right; if you ask it to do something different, it could suddenly go from 99% right to 0% right.  You 
can only define its behavior by isolating what the bug does.

Often, I think mistakes are more like bugs than errors.  My clinkers weren't random; they were in specific 
places, because I had sub-optimal fingerings in those places.  A kid who gets arithmetic questions wrong 
usually isn't getting them wrong at random; there's something missing in their understanding, like not 
getting the difference between multiplication and addition.  Working generically "harder" doesn't fix bugs 
(though fixing bugs does require work). 

Once you start to think of mistakes as deterministic rather than random, as caused by "bugs" (incorrect 
understanding or incorrect procedures) rather than random inaccuracy, a curious thing happens.

You stop thinking of people as "stupid."

Tags like "stupid," "bad at X", "sloppy," and so on, are ways of saying "You're performing badly and I 
don't know why." Once you move it to "you're performing badly because you have the wrong fingerings," or 
"you're performing badly because you don't understand what a limit is," it's no longer a vague personal 
failing but a causal necessity. Anyone who never understood limits will flunk calculus. 

It's not you, it's the bug.
```

The rest of the linked article is _fantastic_. I had trouble quoting it because I felt like quoting everything.

<a name="#important-findings-on-learning"></a>
### Important findings on learning
([overview](#overview))

From the University of Missouri's [25 Learning Principles to Guide Pedagogy and the
Design of Learning Environments](http://whaaales.com/25principlesoflearning.pdf). I only picked those I found interesting. 

Contiguity Effects:

```markdown
Ideas that need to be associated should be presented contiguously in space and time in the
multimedia learning environment. For example, the verbal label for a picture needs to be placed
spatially near the picture on the display, not on the other side of the screen. An explanation of an
event should be given when the event is depicted rather than many minutes, hours, or days later.
```

Perceptual-motor Grounding:

```markdown
Whenever a concept is first introduced, it is important to ground it in a concrete
perceptual-motor experience. The learner will ideally visualize a picture of the concept, will be
able to manipulate its parts and aspects, and will observe how it functions over time. The teacher
and learner will also gain a common ground (shared knowledge) of the learning material.

Perceptual-motor experience is particularly important when there is a need for precision, such as
getting directions to find a spatial location. For example, a course in statistics is not grounded in
perceptual-motor experience when the teacher presents symbols and formulae that have no
meaning to the student and cannot be visualized
```

Dual Code and Multimedia Effects:

```markdown
Information is encoded and remembered better when it is delivered in multiple modes
(verbal and pictorial), sensory modalities (auditory and visual), or media (computers and
lectures) than when delivered in only a singe mode, modality, or medium. Dual codes provide
richer and more varied representations that allow more memory retrieval routes. 

However, the amount of information should not overwhelm the learner because attention is 
split or cognitive capacities are overloaded.
```

Testing Effect:

```markdown
There are direct and indirect effects of taking frequent tests. One indirect benefit is that
frequent testing keeps students constantly engaged in the material. Although students will learn
from testing without receiving feedback, there is less forgetting if students receive informative
feedback about their performance. Multiple tests slow forgetting better than a single test.
Formative assessment refers to the use of testing results to guide teachers in making decisions
about what to teach. Learners also benefit if they use test results as a guide for their own
learning. 
```

The best way to do this is via spaced repetition. It's the whole idea behind Anki flashcards. See also [here](#augmenting-long-term-memory), as well as the next section -- Spaced Effects:

```markdown
Spaced schedules of testing (like spaced schedules of studying) produce better long-term
retention than a single test. When a single test is administered immediately after learning,
students obtain high scores, but long-term retention is reduced with a single immediate test
relative to spaced testing. When a test is given immediately after learning has occurred, learners
still have the newly-learned information in a primary memory system and therefore obtain high
test scores. Both teachers and learners often misjudge their high scores on a test given
immediately after learning as evidence of good retention, when, in fact, long-term retention
suffers with this practice
```

Generation Effect:

```markdown
Learning is enhanced when learners produce answers compared to having them recognize
answers. Free recall or essay tests which require the test taker to generate answers with minimal
cues produce better learning than multiple choice tests in which the learner only needs to be able
to recognize correct answers. In fact, free recall tests produce as much learning as restudying the
material
```

Organization effects (part of the reason this document exists in the first place!):

```markdown
Outlining, integrating, and synthesizing information produces better learning than rereading
materials or other more passive strategies. Students frequently report that when they study they
reread materials they already read once. Strategies that require learners to be actively engaged
with the material to-be-learned produce better long-term retention than the passive act of reading.
Learners should develop their own mini-testing situations as they review, such as stating the
information in their own words (without viewing the text) and synthesizing information from
multiple sources, such as from class and textbooks.
```

Coherence effect (Robert Frost does this well in [his answers on Quora](https://www.quora.com/profile/Robert-Frost-1/answers)):

```markdown
The learner needs to get a coherent, well connected representation of the main ideas to be
learned. It is important to remove distracting, irrelevant material, even when the added
information is artistically appealing. Seductive details that do not address the main points to be
conveyed run the risk of consuming the learner’s attention and effort at the expense of their
missing the main points. 
```

Stories and example cases (see also [John Baez on why math publications should take a page from the storytelling skillbook](#why-math-is-boring)):

```markdown
Stories and other forms of narrative are easier to read, comprehend, and remember than
other types of learning materials. For many millennia, the primary way of passing wisdom down
from generation to generation was through stories. Stories have concrete characters, objects,
locations, plot, themes, emotions, and actions that bear some similarity to everyday experiences.
Many stories also convey a point or moral that can be generalized to many situations. Example
cases in a story-like format are persuasive, easy to comprehend, and very memorable. 
```

Negative suggestion effects:

```markdown
Just as people learn correct information with frequent testing, they also can learn wrong
information this way. For example, when incorrect alternatives on multiple choice tests are
presented, the wrong answers can be learned instead of the correct answers. This effect is also
found on short answer essay questions when students do not know the answers and use their
general knowledge about the field to construct a response that seems reasonable to them. In this
situation, learners recall their incorrect, but logically consistent response as being correct. These
effects can be reduced when learners receive feedback immediately after taking a test which
allows them to revise their memory and understanding without delay.
```

Desirable Difficulties (struggle promotes long-term recall -- true more so in math):
 
```markdown
Learning is enhanced when learners have to organize the information themselves or exert
additional effort during acquisition or retrieval than in conditions in which the information to be
learned or retrieved does not require effort. One possible explanation for this effect is that
learners create multiple retrieval paths which make the information more accessible at retrieval.
These practices slow initial learning, but promote long-term recall.
```

Explanation effects:

```markdown
Explanations consist of causal analyses of events, logical justifications of claims, and
functional rationales for actions. Explanations provide coherence to the material and justify why
information is relevant and important. Students may be prompted to give self-explanations of the
material through think aloud protocols or questioning tasks that elicit explanations that connect
the material to what they already know. Self-explanations and the activity of studying good
explanations facilitate deeper comprehension, learning, memory, and transfer. 
```

Deep questions:

```markdown
Deep explanations of material and reasoning are elicited by questions such as why, how,
what-if-and what-if not, as opposed to shallow questions that require the learner to simply fill in
missing words, such as who, what, where, and when. Training students to ask deep questions
facilitates comprehension of material from text and classroom lectures. The learner gets into the
mindset of having deeper standards of comprehension and the resulting representations are more
elaborate. 
```

Cognitive disequilibrium:

```markdown
Cognitive disequilibrium stimulates inquiry, curiosity, thinking, and deep questions,
which in turn lead to deeper learning. Cognitive disequilibrium occurs when there are obstacles
to goals, contradictions, conflicts, anomalous events, breakdown scenarios, salient gaps in
knowledge, uncertainty, equally attractive alternatives, and other types of impasses. When these
impasses occur, the learner needs to engage in reasoning, thought, problem solving, and planning
in route to restoring cognitive equilibrium. There is a higher incidence of deep questions,
thought, reasoning, and study efforts when learners undergo cognitive disequilibrium.
```

Goldilocks principle: 

```markdown
Assignments should not be too hard or two easy, but at the right level of difficulty for the
student’s level of skill or prior knowledge. The definition of the zone of proximal development
(ZPD) is a bit more technical: the difference in learning that occurs with versus without a
learning scaffold (e.g., tutor, teacher, text, and computer). Researchers have identified a number
of zones that reflect how much learning, memory, mastery, or satisfaction occurs along a
continuum of task difficulty and that is sensitive to individual differences among learners. When
the material is too easy for the learner, the student is not challenged and may get bored. When it
is too difficult, the student acquires very little and gets frustrated or tunes out
```

Anchored learning:

```markdown
Anchored learning occurs when students work in teams for several hours or days trying to
solve a challenging practical problem that matters to the student. The activity is linked to
background knowledge of the learner on a topic that is interesting. The problem is challenging,
so the learner needs to engage in problem solving and recruit multiple levels of knowledge and
skills. These activities are coherently organized around solving the practical problem. Examples
of anchored learning are problem-based curricula in medical schools where students work on
genuine medical cases and communities of practice where students try to solve problems of
pollution in their city.
```


<a name="#polymathy"></a>
### Polymathy
([overview](#overview))

Here's Eric Drexler on [how to understand everything](http://metamodern.com/2009/05/17/how-to-understand-everything-and-why/) and [how to learn about everything](http://metamodern.com/2009/05/27/how-to-learn-about-everything/).

I feel a sort of kinship with what people like Drexler are trying to do here, albeit for different reasons. To oversimplify by mapping my motivations in doing the same to the ‘carrot-and-stick’ model: my carrot is this intrinsic need to ‘see the Systems of the World’ (paraphrasing Neal Stephenson, evoking the unnamed protagonist in Ted Chiang’s Understand, etc); my stick is the pain of being blindsided by unknown unknowns (so I have to at least know the outlines of everything, if not their contents, and how they all fit together). A deeper understanding than the “teacher’s password” awareness of trivia competition champions and my high school self, but not much deeper, not so deep as to sacrifice breadth.

But I digress. Eric contends that we need "knowledge of extent and structure of human knowledge on a trans-disciplinary scale":

```markdown
Formal education in science and engineering centers on teaching facts and problem-solving skills in a 
series of narrow topics. It is true that a few topics, although narrow in content, have such broad 
application that they are themselves integrative: These include (at a bare minimum) substantial chunks 
of mathematics and the basics of classical mechanics and electromagnetism, with the basics of 
thermodynamics and quantum mechanics close behind.

Most subjects in science and engineering, however, are narrower than these, and advanced education 
means deeper and narrower education. What this kind of education omits is knowledge of extent and 
structure of human knowledge on a trans-disciplinary scale. This means understanding — in a particular,
limited sense — everything.
```

How to figure out the outlines of a field, and knowledge about knowledge:

```markdown
To avoid blunders and absurdities, to recognize cross-disciplinary opportunities, and to make sense of
new ideas, requires knowledge of at least the outlines of every field that might be relevant to the 
topics of interest. By knowing the outlines of a field, I mean knowing the answers, to some reasonable
approximation, to questions like these:

What are the physical phenomena?
What causes them?
What are their magnitudes?
When might they be important?
How well are they understood?
How well can they be modeled?
What do they make possible?
What do they forbid?

And even more fundamental than these are questions of knowledge about knowledge:

What is known today?
What are the gaps in what I know?
When would I need to know more to solve a problem?
How could I find what I need?

This sort of knowledge is a kind of specialty, really — a limited slice of learning, but oriented
crosswise. Because of this orientation, though, it provides leverage in integrating knowledge from
diverse sources.
```

Why care? Problem recognition is very important:

```markdown
It takes far less knowledge to recognize a problem than to solve it, yet in key respects, that bit 
of knowledge is more important: With recognition, a problem may be avoided, or solved, or an idea 
abandoned. Without recognition, a hidden problem may invalidate the labor of an hour, or a lifetime.
Lack of a little knowledge can be a dangerous thing.
```

Eric distinguishes between learning everything (which is impossible) and learning *about* everything (which is not):

```markdown
Note that the title above isn’t “how to learn everything”, but “how to learn about everything”. The 
distinction I have in mind is between knowing the inside of a topic in deep detail — many facts and 
problem-solving skills — and knowing the structure and context of a topic: essential facts, what 
problems can be solved by the skilled, and how the topic fits with others.

This knowledge isn’t superficial in a survey-course sense: It is about both deep structure and practical
applications. Knowing about, in this sense, is crucial to understanding a new problem and what must be 
learned in more depth in order to solve it. The cross-disciplinary reach of nanotechnology almost
demands this as a condition of competence.
```

Some advice on going about it:

```markdown
To intellectually ambitious students I recommend investing a lot of time in a mode of study that may 
feel wrong. An implicit lesson of classroom education is that successful study leads to good test 
scores, but this pattern of study is radically different. It cultivates understanding of a kind that 
won’t help pass tests — the classroom kind, that is:

Read and skim journals and textbooks that (at the moment) you only half understand. Include Science 
and Nature.

Don’t halt, dig a hole, and study a particular subject as if you had to pass a test on it.

Don’t avoid a subject because it seems beyond you — instead, read other half-understandable journals 
and textbooks to absorb more vocabulary, perspective, and context, then circle back.

Notice that concepts make more sense when you revisit a topic.

Notice which topics link in all directions, and provide keys to many others. Consider taking a class.

Continue until almost everything you encounter in Science and Nature makes sense as a contribution to 
a field you know something about.
```

<a name="#psychology"></a>
## Psychology
([overview](#overview))

Keith E. Stanovich, [How to Think Straight About Psychology](http://www.pearsonhighered.com/assets/hip/us/hip_us_pearsonhighered/samplechapter/0205914128.pdf):

```markdown
Often a person uses some folk proverb to explain a behavioral event even though, on an earlier occasion, this 
same person used a directly contradictory folk proverb to explain the same type of event. For example, most of 
us have heard or said, “look before you leap.” Now there’s a useful, straightforward bit of behavioral advice—
except that I vaguely remember admonishing on occasion, “he who hesitates is lost.” And “absence makes the heart 
grow fonder” is a pretty clear prediction of an emotional reaction to environmental events. But then what about 
“out of sight, out of mind”? And if “haste makes waste,” why do we sometimes hear that “time waits for no man”? 
How could the saying “two heads are better than one” not be true? Except that “too many cooks spoil the broth.” 
If I think “it’s better to be safe than sorry,” why do I also believe “nothing ventured, nothing gained”? And if 
“opposites attract,” why do “birds of a feather flock together”? I have counseled many students to “never to put 
off until tomorrow what you can do today.” But I hope my last advisee has never heard me say this, because I just 
told him, “cross that bridge when you come to it.”

The enormous appeal of clichés like these is that, taken together as implicit “explanations” of behavior, they 
cannot be refuted. No matter what happens, one of these explanations will be cited to cover it. No wonder we all 
think we are such excellent judges of human behavior and personality. We have an explanation for anything and 
everything that happens. Folk wisdom is cowardly in the sense that it takes no risk that it might be refuted.

That folk wisdom is “after the fact” wisdom, and that it actually is useless in a truly predictive sense, is why 
sociologist Duncan Watts titled one of his books: Everything Is Obvious—Once You Know the Answer (2011). Watts 
discusses a classic paper by Lazarsfeld (1949) in which, over 60 years ago, he was dealing with the common 
criticism that “social science doesn’t tell us anything that we don’t already know.” Lazarsfeld listed a series 
of findings from a massive survey of 600,000 soldiers who had served during World War II; for example, that men 
from rural backgrounds were in better spirits during their time of service than soldiers from city backgrounds. 
People tend to find all of the survey results to be pretty obvious. In this example, for instance, people tend 
to think it obvious that rural men would have been used to harsher physical conditions and thus would have 
adapted better to the conditions of military life. It is likewise with all of the other findings—people find them 
pretty obvious. Lazarsfeld then reveals his punchline: All of the findings were the opposite of what was originally 
stated. For example, it was actually the case that men from city backgrounds were in better spirits during their 
time of service than soldiers from rural backgrounds. The last part of the learning exercise is for people to 
realize how easily they would have explained just the opposite finding. In the case of the actual outcome, people 
tend to explain it (when told of it first) by saying that they expected it because city men are used to working 
in crowded conditions and under hierarchical authority. They never realize how easily they would have concocted an 
explanation for exactly the opposite finding.
```

<a name="#philosophy"></a>
## Philosophy
([overview](#overview))

<a name="#general-philosophy"></a>
### General philosophy
([overview](#overview))

Scott Aaronson, in response to Luke Muehlhauser's interview question "why be interested in philosophy?" in the [MIRI Conversations series](https://intelligence.org/2013/12/13/aaronson/):

```markdown
I’ve always been reflexively drawn to the biggest, most general questions that it seemed possible to 
ask. You know, like are we living in a computer simulation? if not, could we upload our consciousnesses
into one? are there discrete “pixels” of spacetime? why does it seem impossible to change the past? 
could there be different laws of physics where 2+2 equaled 5? are there objective facts about morality?
what does it mean to be rational? is there an explanation for why I’m alive right now, rather than some
other time? What are explanations, anyway? In fact, what really perplexes me is when I meet a smart, 
inquisitive person—let’s say a mathematician or scientist—who claims NOT to be obsessed with these huge
issues! I suspect many MIRI readers might feel drawn to such questions the same way I am, in which case
there’s no need to belabor the point.

From my perspective, then, the best way to frame the question is not: “why be interested in philosophy?” 
Rather it’s: “why be interested in anything else?”

But I think the latter question has an excellent answer. A crucial thing humans learned, starting around 
Galileo’s time, is that even if you’re interested in the biggest questions, usually the only way to make
progress on them is to pick off smaller subquestions: ideally, subquestions that you can attack using 
math, empirical observation, or both. For again and again, you find that the subquestions aren’t nearly
as small as they originally looked! Much like with zooming in to the Mandelbrot set, each subquestion has 
its own twists and tendrils that could occupy you for a lifetime, and each one gives you a new perspective 
on the big questions. And best of all, you can actually answer a few of the subquestions, and be the first
person to do so: you can permanently move the needle of human knowledge, even if only by a minuscule amount.
As I once put it, progress in math and science — think of natural selection, Godel’s and Turing’s theorems,
relativity and quantum mechanics — has repeatedly altered the terms of philosophical discussion, as
philosophical discussion itself has rarely altered them! (Of course, this is completely leaving aside math
and science’s “fringe benefit” of enabling our technological civilization, which is not chickenfeed either.)


On this view, philosophy is simply too big and too important to be confined to philosophy departments! Of 
course, the word “philosophy” used to mean the entire range of fundamental inquiry, from epistemology and
metaphysics to physics and biology (which were then called “natural philosophy”), rather than just close
textual analysis, or writing papers with names like “A Kripkean Reading of Wittgenstein’s Reading of Frege’s
Reading of Kant.” And it seems clear to me that there’s enormous scope today for “philosophy” in the former 
sense — and in particular, for people who love working on the subquestions, on pushing the frontiers of
neuroscience or computer science or physics or whatever else, but who also like to return every once in a
while to the “deep” philosophical mysteries that motivated them as children or teenagers. Admittedly, there
have been many great scientists who didn’t care at all about philosophy, or who were explicitly anti-philosophy.
But there were also scientists like Einstein, Schrodinger, Godel, Turing, or Bell, who not only read lots of
philosophy but (I would say) used it as a sort of springboard into science — in their cases, a wildly successful
one. My guess would be that science ultimately benefits from both the “pro-philosophical” and the
“anti-philosophical” temperaments, and even from the friction between them.
```

Making progress on big problems:

```markdown
Pick off smaller subquestions: ideally, subquestions that you can attack using math, empirical observation, 
or both.

whenever it’s been possible to make definite progress on ancient philosophical problems, such progress has 
almost always involved a [kind of] “bait-and-switch.” In other words: one replaces an unanswerable
philosophical riddle Q by a “merely” scientific or mathematical question Q′, which captures part of what 
people have wanted to know when they’ve asked Q. Then, with luck, one solves Q′.

Of course, even if Q′ is solved, centuries later philosophers might still be debating the exact relation 
between Q and Q′! And further exploration might lead to other scientific or mathematical questions — Q′′, Q′′′,
and so on — which capture aspects of Q that Q′ left untouched. But from my perspective, this process of 
“breaking oﬀ” answerable parts of unanswerable riddles, then trying to answer those parts, is the closest thing
to philosophical progress that there is.

…A good replacement question Q′ should satisfy two properties: (a) Q′ should capture some aspect of the original
question Q — so that an answer to Q′ would be hard to ignore in any subsequent discussion of Q, [and] (b) Q′
should be precise enough that one can see what it would mean to make progress on Q′: what experiments one would
need to do, what theorems one would need to prove, etc.
```

<a name="#Reality-has-a-surprising-amount-of-detail"></a>
### Reality has a surprising amount of detail
([overview](#overview))

The main impetus for starting this subsection is John Salvatier's [eponymous essay](http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail), which I keep thinking off from time to time, albeit at a very vague level. Putting this here hopefully allows me to think about it at a granular enough level to internalize and build upon. 

Thesis: 

```markdown
Reality has a surprising amount of detail. This explains why its so easy for people to end 
up intellectually stuck. Even when they’re literally the best in the world in their field.
```

Example from personal hands-on experience:

```markdown
My dad emigrated from Colombia to North America when he was 18 looking looking for a
better life. For my brother and I that meant a lot of standing outside in the cold. 
My dad’s preferred method of improving his lot was improving lots, and my brother and 
I were “voluntarily” recruited to help working on the buildings we owned. That’s how 
I came to spend a substantial part of my teenage years replacing fences, digging
trenches, and building flooring and sheds. ...

Consider building some basement stairs for a moment. Stairs seem pretty simple at 
first, and at a high level they are simple, just two long, wide parallel boards 
(2” x 12” x 16’), some boards for the stairs and an angle bracket on each side to hold 
up each stair. But as you actually start building you’ll find there’s a surprising 
amount of nuance.

The first thing you’ll notice is that there are actually quite a few subtasks. Even at
a high level, you have to cut both ends of the 2x12s at the correct angles; then screw
in some u-brackets to the main floor to hold the stairs in place; then screw in the
2x12s into the u-brackets; then attach the angle brackets for the stairs; then screw in
the stairs.

Next you’ll notice that each of those steps above decomposes into several steps, some
of which have some tricky details to them due to the properties of the materials and 
task and the limitations of yourself and your tools.

The first problem you’ll encounter is that cutting your 2x12s to the right angle is a
bit complicated because there’s no obvious way to trace the correct angles. You can
either get creative (there is a way to trace it), or you can bust out your trig book 
and figure out how to calculate the angle and position of the cuts.

You’ll probably also want to look up what are reasonable angles for stairs. What looks
reasonable when you’re cutting and what feels safe can be different. Also, you’re
probably going to want to attach a guide for your circular saw when cutting the angle
on the 2x12s because the cut has to be pretty straight.

When you’re ready to you will quickly find that getting the stair boards at all the 
same angle is non-trivial. You’re going to need something that can give you an angle 
to the main board very consistently. Once you have that, and you’ve drawn your lines,
you may be dismayed to discover that your straight looking board is not that straight.
Lumber warps after it’s made because it was cut when it was new and wet and now it’s 
dryer, so no lumber is perfectly straight.

Once you’ve gone back to the lumber store and gotten some straighter 2x12s and redrawn
your lines, you can start screwing in your brackets. Now you’ll learn that despite 
starting aligned with the lines you drew, after screwing them in, your angle brackets
are no longer quite straight because the screws didn’t go in quite straight and now they 
tightly secure the bracket at the wrong angle. You can fix that by drilling guide holes 
first. Also you’ll have to move them an inch or so because it’s more or less impossible
to get a screw to go in differently than it did the first time in the same hole.

Now you’re finally ready to screw in the stair boards. If your screws are longer than 2”,
you’ll need different ones, otherwise they will poke out the top of the board and stab 
you in the foot.

At every step and every level there’s an abundance of detail with material consequences.
```

John points out that the lesson doesn't apply just to stair carpentry:

```markdown
It’s tempting to think ‘So what?’ and dismiss these details as incidental or specific to
stair carpentry. And they are specific to stair carpentry; that’s what makes them details.
But the existence of a surprising number of meaningful details is *not* specific to stairs.
Surprising detail is a near universal property of getting up close and personal with reality.

You can see this everywhere if you look. For example, you’ve probably had the experience
of doing something for the first time, maybe growing vegetables or using a Haskell package
for the first time, and being frustrated by how many annoying snags there were. Then you 
got more practice and then you told yourself ‘man, it was so simple all along, I don’t know
why I had so much trouble’. We run into a fundamental property of the universe and mistake 
it for a personal failing.

If you’re a programmer, you might think that the fiddliness of programming is a special 
feature of programming, but really it’s that everything is fiddly, but you only notice the 
fiddliness when you’re new, and in programming you do new things more often.
```

This isn't just human-centric domains. Even physical laws have a surprising amount of detail, as John illustrates with the example of determining the boiling point of water:

```markdown
You might think the fiddly detailiness of things is limited to human centric domains, and 
that physics itself is simple and elegant. That’s true in some sense – the physical laws
themselves tend to be quite simple – but the manifestation of those laws is often complex 
and counterintuitive.

Consider the boiling of water. That’s straightforward, water boils at 100 °C, right? Well 
the stairs seemed simple too, so let’s double check.

Put yourself in the shoes of someone at the start of the 1800’s, with only a crude, unmarked 
mercury thermometer, trying to figure the physics of temperature. Go to your stove, put some
water in a pot, start heating some water, and pay attention as it heats.

*(I suggest actually doing this)*

The first thing you’ll probably notice is a lot of small bubbles gathering on the surface of
the pot. Is that boiling? The water’s not that hot yet; you can still even stick your finger
in. Then the bubbles will appear faster and start rising, but they somehow seem ‘unboiling’.
Then you’ll start to see little bubble storms in patches, and you start to hear a hissing 
noise. Is that Boiling? Sort of? It doesn’t really look like boiling. The bubble storms grow
larger and start releasing bigger bubbles. Eventually the bubbles get big and the surface of
the water grows turbulent as the bubbles begin to make it to the surface. Finally we seem to
have reached real boiling. I guess this is the boiling point? That seems kind of weird, what
were the things that happened earlier if not boiling.

To make matters worse, if you’d used a glass pot instead of a metal one, the water would boil
at a higher temperature. If you cleaned the glass vessel with sulfuric acid, to remove any 
residue, you’d find that you can heat water substantially more before it boils and when it
does boil it boils in little explosions of boiling and the temperature fluctuates unstably.

Worse still, if you trap a drop of water between two other liquids and heat it, you can raise 
the temperature to at least 300 °C with nothing happening. That kind of makes a mockery of the
statement ‘water boils at 100 °C’.

It turns out that ‘boiling’ is a lot more complicated than you thought.

This surprising amount of detail is is not limited to “human” or “complicated” domains, it is
a near universal property of everything from space travel to sewing, to your internal
experience of your own mind.
```

I feel like this should be apparent if you've ever done experiments, or for that matter done engineering -- it's probably a big failure mode only for high theory folk. This naturally segues into Kovar/Hall's legendary [Electron Band Structure In Germanium, My Ass](http://pages.cs.wisc.edu/~kovar/hall.html) -- you should go to the original article for Fig 1 ("check this shit out -- resistivity vs temperature"):

```markdown
   **Abstract: The exponential dependence of resistivity on temperature in germanium is found to
   be a great big lie. My careful theoretical modeling and painstaking experimentation reveal 

   1) that my equipment is crap, as are all the available texts on the subject and 
   2) that this whole exercise was a complete waste of my time.**

   **Introduction**

      Electrons in germanium are confined to well-defined energy bands that are separated
   by "forbidden regions" of zero charge-carrier density. You can read about it yourself
   if you want to, although I don't recommend it. You'll have to wade through an obtuse,
   convoluted discussion about considering an arbitrary number of non-coupled harmonic-
   oscillator potentials and taking limits and so on. The upshot is that if you heat up 
   a sample of germanium, electrons will jump from a non-conductive energy band to a 
   conductive one, thereby creating a measurable change in resistivity. This relation 
   between temperature and resistivity can be shown to be exponential in certain 
   temperature regimes by waving your hands and chanting "to first order".

   **Experiment procedure**

      I sifted through the box of germanium crystals and chose the one that appeared to 
   be the least cracked. Then I soldered wires onto the crystal in the spots shown in
   figure 2b of Lab Handout 32. Do you have any idea how hard it is to solder wires 
   to germanium? I'll tell you: real goddamn hard. The solder simply won't stick, and
   you can forget about getting any of the grad students in the solid state labs to 
   help you out. 

      Once the wires were in place, I attached them as appropriate to the second-rate 
   equipment I scavenged from the back of the lab, none of which worked properly. I
   soon wised up and swiped replacements from the well-stocked research labs. This is
   how they treat undergrads around here: they give you broken tools and then don't 
   understand why you don't get any results.

      In order to control the temperature of the germanium, I attached the crystal to a 
   copper rod, the upper end of which was attached to a heating coil and the lower end
   of which was dipped in a thermos of liquid nitrogen. Midway through the project, the 
   thermos began leaking. That's right: I pay a cool ten grand a quarter to come here, 
   and yet they can't spare the five bucks to ensure that I have a working thermos.

   **Results**

      Check this shit out (Fig. 1). That's bonafide, 100%-real data, my friends. I took 
   it myself over the course of two weeks. And this was not a leisurely two weeks,
   either; I busted my ass day and night in order to provide you with nothing but 
   the best data possible. Now, let's look a bit more closely at this data, 
   remembering that it is absolutely first-rate. Do you see the exponential 
   dependence? I sure don't. I see a bunch of crap.

      Christ, this was such a waste of my time. 

      Banking on my hopes that whoever grades this will just look at the pictures, I 
   drew an exponential through my noise. I believe the apparent legitimacy is enhanced
   by the fact that I used a complicated computer program to make the fit. I understand
   this is the same process by which the top quark was discovered.	 
```

Why can't you just "deal with it as you go along"? Because when you're trying to do really hard things that we don't know are possible, lots of details turn out to be critical for success:

```markdown
You might hope that these surprising details are irrelevant to your mission, but not so. Some of
them will end up being key. Wood’s tendency to warp means it’s more accurate to trace a cut than 
to calculate its length and angle. The possibility of superheating liquids means it’s important
to use a packed bed when boiling liquids in industrial processes lest your process be highly 
inefficient and unpredictable. The massive difference in weight between a rocket full of fuel and
an empty one means that a reusable rocket can’t hover if it can’t throttle down to a very small 
fraction of its original thrust, which in turn means it must plan its trajectory very precisely 
to achieve 0 velocity at exactly the moment it reaches the ground.
```

And they're also frequently nonobvious when you run into them, they just look like "noise":

```markdown
You might also hope that the important details will be obvious when you run into them, but not so.
Such details aren’t *automatically* visible, even when you’re directly running up against them. 
Things can just seem messy and noisy instead. ‘Spirit’ thermometers, made using brandy and other
liquors, were in common use in the early days of thermometry. They were even considered as a
potential standard fluid for thermometers. It wasn’t until the careful work of Swiss physicist
Jean-André De Luc in the 18th century that physicists realized that alcohol thermometers are 
highly nonlinear and highly variable depending on concentration, which is in turn hard to measure.

You’ve probably also had experiences where you were trying to do something and growing increasingly
frustrated because it wasn’t working, and then finally, after some time you realize that your 
solution method can’t possibly work.
```

And different people notice different details:

```markdown
Another way to see that noticing the right details is hard, is that different people end up noticing
*different* details. My brother and I once built a set of stairs for the garage with my dad, and we
ran into the problem of determining where to cut the long boards so they lie at the correct angle. 
After struggling with the problem for a while (and I do mean struggling, a 16’ long board is heavy),
we got to arguing. I remembered from trig that we could figure out angle so I wanted to go dig up my 
textbook and think about it. My dad said, ‘no, no, no, let’s just trace it’, insisting that we could
figure out how to do it.

I kept arguing because I thought I was right. I felt really annoyed with him and he was annoyed with
me. In retrospect, I think I saw the fundamental difficulty in what we were doing and I don’t think
he appreciated it (look at the stairs picture and see if you can figure it out), he just heard ‘let’s
draw some diagrams and compute the angle’ and didn’t think that was the solution, and if he had
appreciated the thing that I saw I think he would have been more open to drawing some diagrams. But at
the same time, he also understood that diagrams and math don’t account for the shape of the wood,
which I did not appreciate. If we had been able to get these points across, we could have come to 
consensus. Drawing a diagram was probably a good idea, but computing the angle was probably not.
Instead we stayed annoyed at each other for the next 3 hours.
```

Notice that this example is about something as concrete and ostensibly simple as cutting long boards in stair carpentry. Differences of ideology are light-years harder, hence [erisology](#Erisology-and-thinking-less-wrongly).

This brings John to an important point -- that details are either invisible because you haven't noticed them, or because you *have* and they've become integrated into your world-models a la water is to fish:

```markdown
Before you’ve noticed important details they are, of course, basically invisible. It’s hard to
put your attention on them because you don’t even know what you’re looking for. But *after* you 
see them they quickly become so integrated into your intuitive models of the world that they 
become essentially transparent. Do you remember the insights that were crucial in learning to 
ride a bike or drive? How about the details and insights you have that led you to be good at the
things you’re good at?

This means it’s really easy to get *stuck*. Stuck in your current way of seeing and thinking about
things. Frames are made out of the details that seem important to you. The important details you
haven’t noticed are invisible to you, and the details you have noticed seem completely obvious and
you see right through them. This all makes makes it difficult to imagine how you could be missing
something important.

That’s why if you ask an anti-climate change person (or a climate scientist) “what could convince
you you were wrong?” you’ll likely get back an answer like “if it turned out all the data on my 
side was faked” or some other extremely strong requirement for evidence rather than “I would start
doubting if I noticed numerous important mistakes in the details my side’s data and my colleagues
didn’t want to talk about it”. The second case is much more likely than the first, but you’ll
never see it if you’re not paying close attention.

If you’re trying to do impossible things, this effect should *chill you to your bones*. It means 
you could be intellectually stuck right at this very moment, with the evidence right in front of
your face and you just can’t see it.
```

I'm probably lucky to not be working on impossible things. (Or maybe I am. This entire document is supposed to be an early-stage attempt in my neverending journey to build ideas via perfect *deliberate* recall.)

One way to get unstuck is to try to read lots of [non-expert explanations](https://slatestarcodex.com/2017/11/02/non-expert-explanation/), because they tend to focus on different sticking points (i.e. critical details). Another is to take John's advice:

```markdown
This problem is not easy to fix, but it’s not impossible either. I’ve mostly fixed it for myself.
The direction for improvement is clear: seek detail you would not normally notice about the world.
When you go for a walk, notice the unexpected detail in a flower or what the seams in the road 
imply about how the road was built. When you talk to someone who is smart but just seems so wrong,
figure out what details seem important to them and why. In your work, notice how that meeting 
actually wouldn’t have accomplished much if Sarah hadn’t pointed out that one thing. As you learn,
notice which details actually change how you think.
```

<a name="#reading-the-masters-in-philosophy"></a>
### Reading the masters in philosophy
([overview](#overview))

(See also [Reading the masters in math](#reading-the-masters-in-math), or my old post [8If Aristotle were a pro skater: or, reading the masters in math and philosophy*](https://mosstuff.quora.com/If-Aristotle-were-a-pro-skater-or-reading-the-masters-in-math-and-philosophy) to see both math and philo in one place.) 

Should you read the masters? In other words, should you prefer primary sources to summaries and commentaries?

On the one hand, the masters are the masters, so surely there’s *something* to reading them. On the other hand, I’m the kind of person who easily gets lost in walls of text, so when it comes to writing on difficult / ‘slippery’ topics I prefer polished, non-digressive reads. And primary texts, so they seem, are nothing but digressive.

(I suppose it depends why you’re reading. If it’s for enjoyment then secondhand sources certainly won’t cut it. I’m usually looking for insight; enjoyment I relegate to fiction, or exceptionally-written exposition, or something.)

This means I’m partial to Scott Alexander’s stance w.r.t reading the masters, from his post [Book Review: Singer on Marx](http://slatestarcodex.com/2014/09/13/book-review-singer-on-marx/):

```markdown
I’m not embarrassed for choosing Singer’s Marx: A Very Short Introduction as a jumping-off
point for learning more leftist philosophy. I weighed the costs and benefits of reading
primary sources versus summaries and commentaries, and decided in favor of the latter.

The clincher was that the rare times I felt like I really understand certain thinkers and 
philosophies on a deep level, it’s rarely been the primary sources that did it for me, 
even when I’d read them. It’s only after hearing a bunch of different people attack the 
same idea from different angles that I’ve gotten the gist of it. The primary sources – 
especially when they’re translated, especially when they’re from the olden days before 
people discovered how to be interesting – just turn me off. Singer is a known person who
can think and write clearly, and his book was just about the shortest I could find, so I 
jumped on it…
```

It doesn’t help that Andrew L’s comment in [this MO thread](https://mathoverflow.net/questions/28268/do-you-read-the-masters) is essentially what I think of the masters (to wit, that they’re notoriously hard to read):

```markdown
There's a myth surrounding Abel's dictum that stems from the unreadability of the masters
like Gauss as a measure of their nearly inhuman brilliance. This is a fallacy.

The reason the masters are so difficult to read is because we are catching them with their
pants down in the act of creation: they are groping towards the right notation and 
terminology, but aren't quite there yet.
```

Notation ("polish", as derisively referred to sometimes) is a UI design problem. UI design problems in research can be very nontrivial and extremely impactful from the standpoint of [interpretive labor](#distillation-and-research-debt). 

In Katja Grace's essay [Why read old philosophy?](https://meteuphoric.com/2017/01/04/why-read-old-philosophy/amp/), which is the main inspiration for this section, she begins by contrasting philosophy with physics, and pretty much every other book subject:

```markdown
We read old physicists if we want to do original research on the history of physics. Or 
maybe if we are studying an aspect of physics so obscure that nobody has covered it in
hundreds of years. If we want to learn physics we read a physics textbook. As far as I 
know, the story is similar in math, chemistry, engineering, economics, and business 
(though maybe some other subjects that I know less about are more like philosophy).

Yet go to philosophy grad school, and you will read original papers and books by 
historical philosophers. Research projects explore in great detail what it is that
Aristotle actually said, thought, and meant. Scholars will learn the languages that the
relevant texts were written in, because none of the translations can do the texts the 
necessary justice. The courses and books will be named after people like ‘Hume’ as often 
as they are named after topics of inquiry like ‘Causality’ and larger subject areas will
be organized by the spatiotemporal location of the philosopher, rather than by the subject
matter: Ancient Philosophy, Early Modern Philosophy, Chinese Philosophy, Continental 
Philosophy.
```

This should be confusing. It definitely confuses me. To elaborate on how it would look like in physics:

```markdown
The physics situation makes a lot more sense to me. Hypothetically, who would I rather 
read an explanation of ‘The Alice Effect’ by? —Alice, the effect’s seventeenth century
discoverer, or Bob, a modern day physics professor authoring a textbook?

Some salient considerations, neutrality not guaranteed:

- Alice’s understanding of the Alice effect is probably the most confused understanding 
of it in all of history, being the first ‘understanding of the Alice effect’ to set 
itself apart from ‘confusion and ignorance about the Alice effect’.

- In the billions of lifetimes that have passed since Alice’s time, the world has 
probably thought substantially more about The Alice Effect than Alice managed to in 
her lifetime, at least if it is important at all.

- Alice’s very first account of the effect probably contained imperfections. Bob can 
write about the theory as it stood after years of adjustment.

- Even if Alice’s account was perfectly correct, it was probably not perfectly well
explained, unless she happens to have been a great explainer as well as a great 
physicist.

- Physics has made many discoveries since Alice’s time, such as Claire forces, Evan motion
and Roger fields. It might be easier to understand all of this by starting with the Roger 
fields, and explaining the Alice effect as a consequence. However literature from the 
likes of Alice is constrained to cover topics chronologically by date of discovery.

- Bob speaks a similar version of English to me.

- Bob can be selected for having particular skill at writing and explanation, whereas Alice
must be selected for having the scientific prowess to make the discovery.

- Bob is actually trying to explain the thing to a 21st Century reader, while Alice is
writing to pique the interest of some seventeenth century noblemen who lack modern 
intellectual machinery and are interested in issues like whether this is compatible with
religion. An accurate impression of a 21st Century reader would probably cause Alice to 
fall over.

I think Bob is a solid choice.
```

So why is philosophy different?

```markdown
Some pieces of explanations I heard, or made up while hearing other explanations:

- You have to be smarter than the original philosopher to summarize their work well, so
there are few good summaries

- The translations are all terrible for conveying the important parts

- Philosophy is not trying to communicate normal content that can be in explicit 
statements, of the kind you might be able to explain well and check the understanding of
and such.

- Philosophy is about having certain experiences which pertain to the relevant philosophy,
much like reading a poem is different to reading a summary of its content.
```

None of this convinces Katja. She eventually settles on the "Aristotle as pro skater" analogy:

```markdown
Here’s my explanation. Reading Aristotle describe his thoughts about the world is like 
watching Aristotle ride a skateboard if Aristotle were a pro skater. You are not getting
value from learning about the streets he is gliding over (or the natural world that he is
describing) and you should not be memorizing the set of jumps he chooses (or his 
particular conceptualizations of the world). You are meant to be learning about how to
carry out the activity that he is carrying out: how to be Aristotle. How to do what
Aristotle would do, even in a new environment.

An old work of philosophy does not describe the thing you are meant to be learning about. 
It was created by the thing you are meant to be learning about, much like watching a video
from skater-Aristotle’s GoPro. And the value proposition is that with this high resolution
Aristotle’s-eye-view, you can infer the motions.

There is not a short description  of the insights you should learn (or at least not one
available), because the insights you are hopefully learning are not the insights that
Aristotle is trying to share. Aristotle might have highly summarizable insights, but what 
you want to know is how to be Aristotle, and nobody has necessarily developed an abstract 
model of how to be Aristotle from which summary statements can be extracted.

So it is not that the useful content being transmitted is of a special kind that is immune
to being communicated as statements. It is just not actually known in statements. Nobody 
knows which aspects of being Aristotle are important, and nobody has successfully made a
simplified summary. What we ‘know’ is this one very detailed example. Much like if I showed
you a bee because I thought I couldn’t communicate it in words—it would not be because bees
are mysteriously indescribable, it would be that I haven’t developed the understanding 
required to describe what is important about it, so I’m just showing you the whole bee.

On this theory, if someone doesn’t realize what is going on, and tries to summarize
Aristotle’s writings in the way that you would usually summarize the content of a passage,
you entirely lose what was valuable about it. Much as you would if you summarized a video 
of a skater in motion into a description of the environment that they had interacted with.
I hypothesize that this is roughly what happens, and is why it feels like summaries can’t 
capture what is important, and probably why translations seem bad always. Whenever a person
tries to do a translation, they faithfully communicate the content of the thoughts at the
expense of faithfully communicating the thinking procedure.
```

This sounds like alkjash's [becoming the grand meta-theorem](#two-cultures):

```markdown
My take on the "Two Cultures" model of problem-solvers and theory-builders: theory-building
fields of mathematics like algebraic topology (say) are those where the goal is to articulate 
grand meta-theorems that are bigger than any particular application. This was the work of a
Grothendieck.

Meanwhile, concrete problem-solving fields of mathematics like combinatorics are those where
the goal is to *become* the grand meta-theorem that contains more understanding than any 
particular theorem you can prove. This was the style of an Erdos. The inarticulate grand meta-
theorems lived in his cognitive strategies so that the theorems he actually proved are 
individually only faint impressions thereof.
```

There may be something about analytic philosophy being the part of philosophy that's more legible, and the less-legible parts of philosophy would then be the ones where you derive more value from reading primary sources -- but that's just a guess. That would be the intra-subject version of the inter-subject situation Katja actually does talk about towards the end:

```markdown
Why would you want to be like Socrates, and not like Newton? Especially since Newton had more 
to show for his thoughts than an account of what his thoughts were like. I suspect the 
difference is that because physicists invent explicit machinery that can be easily taught,
when you learn physics you spend your time mastering these tools. And perhaps in the process,
you come to think in a way that fits well with these tools. Whereas in philosophy there is
much less in the way of explicit methods to learn, so the most natural thing to learn is how
to do whatever mental processes produce good philosophy. And since there is not a consensus on
what they are like in the abstract, emulating existing good philosophers is a plausible way to
proceed.

I was in the CMU philosophy department, which focuses on more formal methods that others might
not class as philosophy—logic, algorithms for determining causality, game theory—and indeed in
logic class we learned a lot of logical lemmas and did a lot of proofs and did not learn much 
about Frege or Gödel, though we did learn a bit about their history and thought at other point
in the program.

(This story would suggest that in physics students are maybe missing out on learning the styles
of thought that produce progress in physics. My guess is that instead they learn them in grad 
school when they are doing research themselves, by emulating their supervisors, and that the 
helpfulness of this might partially explain why Nobel prizewinner advisors beget Nobel
prizewinner students.)
```

Katja gives the following example of a translation that "faithfully communicates the content of the thoughts at the expense of faithfully communicating the thinking procedure":

```markdown
For instance, suppose I have a sentence like this:

	We have enough pieces of evidence to say that 
	friendly banter is for counter-signaling.

If not quite the same words were available in a different language, it might get translated to:

	We have seen enough evidence to know that 
	friendly banter is for counter-signaling.

Which tells us something very similar about whether friendly banter is for counter-signaling.

But something subtle is lost about the process: in the initial statement, the author is 
suggesting that they are relying on the accretion of many separate pieces of evidence that
may not have been independently compelling, whereas in the latter that is not clear. Over a
long text, sentences like the former might give the reader an implicit understanding of how
disparate and independently uncompelling evidence might be combined in the intuition of the 
author, without the issue ever being explicitly discussed. In the latter, this implication is
entirely lost.

So I think this explains the sense that adequate summarization is impossible and translation
is extremely difficult. At least, if we assume that people either don’t know what is really 
going on.
```

<a name="#diseased-philosophy"></a>
### Diseased philosophy
([overview](#overview))

From Scott Aaronson's [Quantum Computing Since Democritus](https://slatestarcodex.com/2014/09/01/book-review-and-highlights-quantum-computing-since-democritus/):

```markdown
The third thing that annoys me about the Chinese Room argument is the way it gets so much mileage from 
a possibly misleading choice of imagery, or, one might say, by trying to sidestep the entire issue of 
computational complexity purely through clever framing. We’re invited to imagine someone pushing around 
slips of paper with zero understanding or insight, much like the doofus freshmen who write 
(a + b)^2 = a^2 + b^2 on their math tests. 

But how many slips of paper are we talking about! How big would the rule book have to be, and how 
quickly would you have to consult it, to carry out an intelligent Chinese conversation in anything 
resembling real time? If each page of the rule book corresponded to one neuron of a native speaker’s 
brain, then probably we’d be talking about a “rule book” at leas the size of the Earth, its pages 
searchable by a swarm of robots traveling at close to the speed of light. When you put it that way, 
maybe it’s not so hard to imagine this enormous Chinese-speaking entity that we’ve brought into being 
might have something we’d be prepared to call understanding or insight.

Philosophers are so good at pure qualitative distinctions that it’s easy to slip the difference between 
“guy in a room” and “planet being processed by lightspeed robots” under the rug.
```

Brandon Watson, [The Success and Failure of Arguments](http://branemrys.blogspot.com/2010/09/success-and-failure-of-arguments.html):

```markdown
Sometimes you hear philosophers bemoaning the fact that philosophers tend not to form consensuses 
like certain other disciplines do (sciences in particular). 

But there is no great mystery to this. The sciences reward consensus-forming as long as certain 
procedures are followed: agreements through experimental verification, processes of peer review, etc. 
Philosophy has nothing like this. Philosophers are rewarded for coming up with creative reasons not to 
agree with other people. The whole thrust of professional philosophy is toward inventing ways to regard 
opposing arguments as failure, as long as those ways don't exhibit any obvious flaws. However much 
philosophers are interested in the truth, philosophy as a profession is not structured so as to converge 
on it; it is structured so as to have the maximal possible divergence that can be sustained given common 
conventions. 

We are not trained to find ways to come to agree with each other; we are trained to find ways to 
disagree with each other.
```

John Perry, from the introduction to *Identity, Personal Identity, and the Self*:

```markdown
There is something about practical things that knocks us off our philosophical high horses. Perhaps Heraclitus 
really thought he couldn't step in the same river twice. Perhaps he even received tenure for that contribution 
to philosophy. But suppose some other ancient had claimed to have as much right as Heraclitus did to an ox 
Heraclitus had bought, on the grounds that since the animal had changed, it wasn't the same one he had bought 
and so was up for grabs. Heraclitus would have quickly come up with some ersatz, watered-down version of 
identity of practical value for dealing with property rights, oxen, lyres, vineyards, and the like. And then 
he might have wondered if that watered-down vulgar sense of identity might be a considerably more valuable 
concept than a pure and philosophical sort of identity that nothing has.
```

<a name="#morality-axiology-law"></a>
### Morality, axiology, law
([overview](#overview))

Scott Alexander on the distinction between axiology, morality, and law, from [this essay](https://slatestarcodex.com/2017/08/28/contra-askell-on-moral-offsets/):

```markdown
Axiology is the study of what’s good. If you want to get all reductive, think of it as comparing the 
values of world-states. A world-state where everybody is happy seems better than a world-state where 
everybody is sad. A world-state with lots of beautiful art is better than a world-state containing only 
featureless concrete cubes. Maybe some people think a world-state full of people living in harmony with
nature is better than a world-state full of gleaming domed cities, and other people believe the opposite; 
when they debate the point, they’re debating axiology.

Morality is the study of what the right thing to do is. If someone says “don’t murder”, they’re making a 
moral commandment. If someone says “Pirating music is wrong”, they’re making a moral claim. Maybe some 
people believe you should pull the lever on the trolley problem, and other people believe you shouldn’t; 
when they debate the point, they’re debating morality.

(this definition elides a complicated distinction between individual conscience and social pressure; 
fixing that would be really hard and I’m going to keep eliding it)

Law is – oh, come on, you know this one. If someone says “Don’t go above the speed limit, there’s a 
cop car behind that corner”, that’s law. If someone says “my state doesn’t allow recreational marijuana,
but it will next year”, that’s law too. Maybe some people believe that zoning restrictions should ban 
skyscrapers in historic areas, and other people believe they shouldn’t; when they debate the point, 
they’re debating law.

These three concepts are pretty similar; they’re all about some vague sense of what is or isn’t desirable. 
But most societies stop short of making them exactly the same. Only the purest act-utilitarianesque 
consequentialists say that axiology exactly equals morality, and I’m not sure there is anybody quite that
pure. And only the harshest of Puritans try to legislate the state law to be exactly identical to the moral
one. To bridge the whole distance – to directly connect axiology to law and make it illegal to do anything 
other than the most utility-maximizing action at any given time – is such a mind-bogglingly bad idea that 
I don’t think anyone’s even considered it in all of human history.

These concepts stay separate because they each make different compromises between goodness, implementation, 
and coordination.

One example: axiology can’t distinguish between murdering your annoying neighbor vs. not donating money to 
savea child dying of parasitic worms in Uganda. To axiology, they’re both just one life snuffed out of the 
world beforeits time. If you forced it to draw some distinction, it would probably decide that saving the
child dying of parasitic worms was more important, since they have a longer potential future lifespan.

But morality absolutely draws this distinction: it says not-murdering is obligatory, but donating money to
Ugandais supererogatory. Even utilitarians who deny this distinction in principle will use it in everyday 
life: if theirfriend was considering not donating money, they would be a little upset; if their friend was
considering murder, they would be horrified. If they themselves forgot to donate money, they’d feel a little
bad; if they committedmurder in the heat of passion, they’d feel awful.

A final example: axiology tells us a world without alcohol would be better than our current world: ending
alcoholism could avert millions of deaths, illnesses, crimes, and abusive relationships. Morality only tells
us that we should be careful drinking and stop if we find ourselves becoming alcoholic or ruining our
relationships. And the law protests that it tried banning alcohol once, but it turned out to be unenforceable
and gave too many new opportunities to organized crime, so it’s going to stay out of this one except to say
you shouldn’t drink and drive.

So fundamentally, what is the difference between axiology, morality, and law?

Axiology is just our beliefs about what is good. If you defy axiology, you make the world worse.

At least from a rule-utilitarianesque perspective, morality is an attempt to triage the infinite demands 
of axiology, in order to make them implementable by specific people living in specific communities. It 
makes assumptions like “people have limited ability to predict the outcome of their actions”, “people 
are only going to do a certain amount and then get tired”, and “people do better with bright-line rules
than with vague gradients of goodness”. It also admits that it’s important that everyone living in a 
community is on at least kind of the same page morally, both in order to create social pressure to follow 
the rules, and in order to build the social trust that allows the community to keep functioning. If you 
defy morality, you still make the world worse. And you feel guilty. And you betray the social trust that
lets your community function smoothly. And you get ostracized as a bad person.

Law is an attempt to formalize the complicated demands of morality, in order to make them implementable
by a state with police officers and law courts. It makes assumptions like “people’s vague intuitive moral 
judgments can sometimes give different results on the same case”, “sometimes police officers and
legislators are corrupt or wrong”, and “we need to balance the benefits of laws against the cost of
enforcing them”. It also tries to avert civil disorder or civil war by assuring everybody that it’s in 
their best interests to appeal to a fair universal law code rather than try to solve their disagreements
directly. If you defy law, you still get all the problems with defying axiology and morality. And you 
make your country less peaceful and stable. And you go to jail.

In a healthy situation, each of these systems reinforces and promotes the other. Morality helps you 
implement axiology from your limited human perspective, but also helps prevent you from feeling guilty
for not being God and not being able to save everybody. The law helps enforce the most important moral
and axiological rules but also leaves people free enough to use their own best judgment on how to pursue
the others. And axiology and morality help resolve disputes about what the law should be, and then lend
the support of the community, the church, and the individual conscience in keeping people law-abiding.

In these healthy situations, the universally-agreed priority is that law trumps morality, and morality
trumps axiology. First, because you can’t keep your obligations to your community from jail, and you 
can’t work to make the world a better place when you’re a universally-loathed social outcast. But also,
because you can’t work to build strong communities and relationships in the middle of a civil war, and
you can’t work to make the world a better place from within a low-trust defect-defect equilibrium. But
also, because in a just society, axiology wants you to be moral (because morality is just a 
more-effective implementation of axiology), and morality wants you to be law-abiding (because law is 
just a more-effective way of coordinating morality). So first you do your legal duty, then your moral 
duty, and then if you have energy left over, you try to make the world a better place.

In unhealthy situations, you can get all sorts of weird conflicts. Most “moral dilemmas” are 
philosophers trying to create perverse situations where axiology and morality give opposite answers.
For example, the fat man version of the trolley problem sets axiology (“it’s obviously better to 
have a world where one person dies than a world where five people die”) against morality (“it’s a
useful rule that people generally shouldn’t push other people to their deaths”). And when morality 
and state law disagree, you get various acts of civil disobedience, from people hiding Jews from the
Nazis all the way down to Kentucky clerks refusing to perform gay marriages.

I don’t have any special insight into these. My intuition (most authoritative source! is never wrong!)
says that we should be very careful reversing the usual law-trumps-morality-trumps-axiology order, 
since the whole point of having more than one system is that we expect the systems to disagree and we 
want to suppress those disagreements in order to solve important implementation and coordination problems.
But I also can’t deny that for enough gain, I’d reverse the order in a heartbeat. If someone told me 
that by breaking a promise to my friend (morality) I could cure all cancer forever (axiology), then f@$k 
my friend, and f@$k whatever social trust or community cohesion would be lost by the transaction.
```

<a name="#moral-patienthood"></a>
### Moral patienthood
([overview](#overview))

<a name="#slack-and-deliberate-mediocrity"></a>
### Slack and deliberate mediocrity
([overview](#overview))

The basic idea of Slack comes from the [eponymous LW post](https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack). Quotes I liked, or that made sense to me:

```markdown
Definition: Slack. The absence of binding constraints on behavior.
Poor is the person without Slack. Lack of Slack compounds and traps.

Slack means margin for error. You can relax.

Slack allows pursuing opportunities. You can explore. You can trade.

Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.

Slack permits planning for the long term. You can invest.

Slack enables doing things for your own amusement. You can play games. You can have fun.

Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. 
You can have a code.

Slack presents things as they are without concern for how things look or what others think. You can 
be honest. ...

Slack in project management is the time a task can be delayed without causing a delay to either 
subsequent tasks or project completion time. The amount of time before a constraint binds.

A slacker is one who has a lazy work ethic or otherwise does not exert maximum effort. They slack off. 
They refuse to be bound by what others view as hard constraints.
```

On things being Out To Get You:

```markdown
Many things in this world are Out to Get You. Often they are Out to Get You for a lot, usually 
but not always your time, attention and money.

If you Get Got for compact amounts too often, it will add up and the constraints will bind.

If you Get Got even once for a non-compact amount, the cost expands until the you have no Slack 
left. The constraints bind you.

You might spend every spare minute and/or dollar on politics, advocacy or charity. You might think 
of every dollar as a fraction of a third-world life saved. Racing to find a cure for your daughter’s
cancer, you already work around the clock. You could have an all-consuming job or be a soldier 
marching off to war. It could be a quest for revenge, for glory, for love. Or you might spend every
spare minute mindlessly checking Facebook or obsessed with your fantasy football league.

You cannot relax. Your life is not your own.

It might even be the right choice! Especially for brief periods. When about to be run over by a truck
or evicted from your house, Slack is a luxury you cannot afford. Extraordinary times call for 
extraordinary effort.

Most times are ordinary. Make an ordinary effort.
```

Can you afford to lose Slack?

```markdown
No, you can’t. This is the most famous attack on Slack. Few words make me angrier.

The person who says “You Can Afford It” is saying to ignore constraints that do not bind you. If you 
do, all constraints soon bind you.

Those who do not value Slack soon lose it. Slack matters. Fight to keep yours!

Ask not whether you can afford it. Ask if it is Worth It.

Unless you can’t afford it. Affordability is invaluable negative selection. Never positive selection.

The You Can Afford It tax on Slack quickly approaches 100% if unchecked.

If those with extra resources are asked to share the whole surplus, all are poor or hide their wealth.
Wealth is a burden and makes you a target. Those visibly flush rush to spend their bounty.

Where those with free time are given extra work, all are busy or look busy. Those with copious free time
seek out relatively painless time sinks they can point to.

When looking happy means you deal with everything unpleasant, no one looks happy for long.
```

Venkat Rao's Maya Millennial is an example of someone who lacks Slack:

```markdown
Constraints bind her every action. Her job in life is putting up a front of the person she wants to 
show people that she wants to be. If her constraints noticeably failed to bind the illusion would fail.

Every action is being watched. If no one is around to watch her, the job falls to her. She must post 
all to Facebook, to Snapchat, to Instagram. Each action and choice signals who she is and her loyalty
to the system. Not doing that this time could mean missing her one chance to make it big.

Maya never has free time. There is signaling to do! At a minimum, she must spend such time on alert and
on her phone lest she miss something.

Maya never has spare cash. All must be spent to advance and fit her profile.

Maya lacks free speech, free association, free taste and free thought. All must serve.

Maya is in a world where she must signal she has no Slack. Slack means insufficient dedication and 
loyalty. Slack cannot be trusted. Slack now means slack later, which means failure. Future failure means 
no opportunity.
```

G Gordon Worley III comments on how Slack relates to distributed systems:

```markdown
If you work with distributed systems, by which I mean any system that must pass information between
multiple, tightly integrated subsystems, there is a well understood concept of maximum sustainable load
and we know that number to be roughly 60% of maximum possible load for all systems.

I don't have a link handy to show you the math, but the basic idea is that the probability that one 
subsystem will have to wait on another increases exponentially with the total load on the system and 
the load level that maximizes throughput (total amount of work done by the system over some period of
time) comes in just above 60%. If you do less work you are wasting capacity (in terms of throughput);
if you do more work you will gum up the works and waste time waiting even if all the subsystems are
always busy.

We normally deal with this in engineering contexts, but as is so often the case this property will hold 
for basically anything that looks sufficiently like a distributed system. Thus the "operate at 60% 
capacity" rule of thumb will maximize throughput in lots of scenarios: assembly lines, service-oriented
architecture software, coordinated work within any organization, an individual's work (since it is 
normally made up of many tasks that information must be passed between with the topology being spread
out over time rather than space), and perhaps most surprisingly an individual's mind-body.

"Slack" is a decent way of putting this, but we can be pretty precise and say you need ~40% slack to
optimize throughput: more and you tip into being "lazy", less and you become "overworked".
```

Some things that give Slack also take it away, per commenter elizabeth:

```markdown
The obvious example is cell phones. Especially at first they gave slack by letting you leave the house 
while you were waiting for an important phone call, but eventually ate it by creating an expectation 
that you'd always be available.
```

ialdabaoth contends that "this is because social structures strive to keep slack homeostatic at your level. As soon as you have more slack than you need to service your superiors in the social hierarchy, they will take that slack for themselves" but I'm not convinced of this argument. 

It's possible to have too much Slack -- I relate to this, as someone who's always subconsciously optimized for Slack myself. Per JacekLach:

```markdown
I often find myself with free time, and 'waste it away'. I don't really do anything on most weekends.
Having more constraints as guidance for behaviour in free time could likely remediate that; but I seem 
to be very good at talking myself out of any recurrent commitments, saying that they would reduce my 
freedom/flexibility/slack.

At the same time, it seems to me that I'm happiest, most 'alive', most in the 'flow', in situations with
exactly the kind of binding constraints this post talks of avoiding. The constraints focus you on the 
present, on the very moment, on being. For me this is clearest in sailing regattas - a clear purpose that
acts as a binding constraint (to go as fast as possible while staying safe - a safety margin does not for 
slack make, since you are not willing to ignore crossing it), consuming all your attention (at least during
the time you're responsible for the ship, and often more).

I suppose one can stretch the metaphor and say that having no slack on too many dimensions is likely to 
squash you; but having slack everywhere leaves you floating around aimlessly. Keeping most constraints 
slack and choosing only a couple aligned ones to bind against is possibly a way to find purpose.
```

Frustrated-demiurge talks about the same concept in her post [Affordance widths](https://frustrateddemiurge.tumblr.com/post/144927712238/affordance-widths). I *think* Venkat Rao talks about this same thing too in [this post](https://www.ribbonfarm.com/2019/03/07/mediocratopia-3/), except he comes at it from a different angle. 

```markdown
There is a paradox at the heart of mediocrity studies: excellence is not actually exceptional. If 
you see an excellent behavior or thing, it’s likely to be a middling instance at its level. The 
perception of exceptionalism is an illusion caused by inappropriate comparisons: you think it is a
99 percentile example of Level 3 performance, but it’s really a median example of Level 4 performance.

Changing levels of performance is self-disruption. The moment you hit, say, the 60% performance 
point on the current S-curve of learning, you start looking for ways to level up. This is the basic 
point in Daniel F. Chambliss’ classic paper, The Mundanity of Excellence. People who rise through 
the levels of a competitive sport do so by making discrete qualitative changes to level up before 
they hit diminishing returns from the current level. This process of leveling up, has less to do 
with striving for excellence in the sense of exceptional performance, and more to do with repeatedly
growing past limits. The visibly excellent are never at a local optimum.

In Age of Speed, skier Vince Poscente claims he won primarily by practicing his skills at a level 
above the one he was competing at. So during actual competition, he could win with less than 100% 
effort.

Making winning a habit is about making sure you’re always operating at a level where you have slack;
where you are in fact mediocre. If you’re being pushed towards excellence, it’s time to find a new 
level.
```

"Leveling up before hitting diminishing returns reminds me of another great essay I recently read, Eugene Wei's [Invisible Asymptotes](https://www.eugenewei.com/blog/2018/5/21/invisible-asymptotes) post from his blog [Remains of the Day](https://www.eugenewei.com/). It's pretty long, but full of great stuff. The relevant part is this:

```markdown
Every successful business goes through the famous S-curve, and most companies, and their investors,
spend a lot of time looking for that inflection point towards hockey-stick growth. But just as 
important, and perhaps less well studied, is that unhappy point later in the S-curve, when you hit
a shoulder and experience a flattening of growth. ...

For so many startups and even larger tech incumbents, the point at which they hit the shoulder in
the S-curve is a mystery, and I suspect the failure to see it occurs much earlier. The good thing is
that identifying the enemy sooner allows you to address it. We focus so much on product-market fit,
but once companies have achieved some semblance of it, most should spend much more time on the 
problem of product-market unfit.

For me, in strategic planning, the question in building my forecast was to flush out what I call the
invisible asymptote: a ceiling that our growth curve would bump its head against if we continued down
our current path. It's an important concept to understand for many people in a company, whether a CEO, 
a product person, or, as I was back then, a planner in finance. ...

An obvious problem for many companies, however, is that they are creating new types of businesses and 
services that don't lend themselves to easily identifying such invisible asymptotes. Many are not like
Amazon where there are readily tracked metrics like the size of the global book market with which to
peg their TAM (total addressable market).

Some of the limits to their growth are easier to spot than others. For messaging and some more general
social networking apps, for example, in many cases network effects are geographical. Since these apps 
build on top of real-world social graphs, and many of those are geographically clustered, there are 
winner-take-all dynamics such that in many countries one messaging app dominates, like Kakao in Korea 
or Line in Taiwan. There can be geo-political considerations, too, that help ensure that that WeChat 
will dominate in China to the exclusion of all competitors, for example.

For others, though, it takes a bit more product insight, and some might say intuition, to see the 
ceiling before you bump into it. For both employees and investors, understanding product-market unfit 
follows very closely on identifying product-market fit as an existential challenge.
```

<a name="#moloch"></a>
### Moloch
([overview](#overview))

Like LW commenter Quinn, I had difficulty summarizing Scott Alexander's celebrated essay *Moloch*, even though it was one of the most impactful pieces I've ever read. So I appreciate that he's conveniently summarized it SparkNotes-style under Stuart Armstrong's [essay](https://www.lesswrong.com/posts/otES8gdmFszCvZiRy/moloch-optimisation-and-vs-or-information-and-sacrificial):

```markdown
Intro - no real content.

Moloch as coordination failure: everyone makes a sacrifice to optimize for a zero-sum 
competition,ends up with the same relative status, but worse absolute status.

10 examples: 
- Prisoner's Dilemma, 
- dollar auctions, 
- fish-farming story (tragedy of the commons), 
- Malthusian trap, 
- ruthless/exploitative Capitalist markets, 
- the two-income trap, 
- agriculture, 
- arms races, 
- cancer, 
- political race to the bottom (lowering taxes to attract business)

4 partial examples: 
- inefficient education, 
- inefficient science, 
- government corruption (corporate welfare), 
- Congress (representatives voting against good of nation for good of constituency)

Existing systems are created by incentive structures, not agents, e.g. Las Vegas caused by 
a known bias in human reward circuitry, not optimization for human values.

But sometimes we move uphill anyway. Possible explanations:

- Excess resources / we are in the dream time and can afford non-competitive behavior.
- Physical limitations to what can be sacrificed
- Economic competition actually producing positive utility for consumers (but this is fragile)
- Coordination, e.g. via governments, guilds, friendships, etc.

Technology/ingenuity creates new opportunities to fall into such traps: 
- Technology overcomes physical limitations, consumes excess resources. 
- Automation further decouples economic activity from human values. 
- Technology can improve coordination, but can also exacerbate existing conflicts by giving all 
sides more power.

AGI opens up whole new worlds of traps: 
- Yudkowsky's paperclipper, 
- Hanson's subsistence-level ems, 
- Bostrom's Disneyland with no children.

6 & 7. Gnon - basically the god of the conservative scarcity mindset. Nick Land advocates 
compliance; Nyan wants to capture Gnon and build a walled garden. Scott warns that Moloch is 
far more terrifying than Gnon and will kill both of them anyway.

8 & 9. So we have to kill this Moloch guy, by lifting a better God to Heaven (Elua).
```

Wei Dai points out that this is captured by the standard academic notion of [positional good](http://en.wikipedia.org/wiki/Positional_good). Robert Frank, in [an article on higher ed](https://net.educause.edu/ir/library/pdf/ffp0001s.pdf), calls Moloch a "positional arms race":

```markdown
Participants in virtually all winner-take-all markets face strong incentives to invest in performance
enhancement, thereby to increase their chances of coming out ahead. As in the classic military arms 
race, however, many such investments prove mutually offsetting in the end. When each nation spends more
on bombs, the balance of power is no different than if none had spent more. Yet that fact alone provides
no escape for individual participants. Countries may find it burdensome to spend a lot on bombs, but the 
alternative—to be less well-armed than their rivals—is even worse.

In light of the growing importance of rank in the education marketplace, universities face increasing 
pressure to bid for the various resources that facilitate the quest for high rank. These pressures have 
spawned a positional arms race that already has proved extremely costly, and promises to become more so.
```

<a name="#anthropic-principle"></a>
### Anthropic principle
([overview](#overview))

<a name="#critiques-of-the-anthropic-principle"></a>
### Critiques of the anthropic principle
([overview](#overview))

Funny if uncharitable quote by Cosma Shalizi from the introduction to his notebook [Astrophysics and Cosmology](http://bactra.org/notebooks/astrophysics.html):

```markdown
There are interesting issues of statistical mechanics involved, since there are long-range 
interactions: gravity falls off only slowly with distance r-2, after all), and, unlike 
electromagnetism, there are no positive and negative charges which could lead to screening off.
This leads to some weird effects, like spontaneous clumping, and possibilities like negative 
specific heats.

A personal hatred: the anthropic principle. To illustrate: I once happened --- no joke --- to 
find a twenty dollar bill lying in the street in front of my house. This required an extraordinarily
fine adjustment of a huge range of circumstances. Among these, of course, were the incidents of
American history such that we use paper money, denominated in dollars, that the twenty is a common
but large denomination, and that Andrew Jackson's portait be on it. This last involves our political
history through and indeed since Jackson's time. That political history is incomprehensible without 
the influence of the Enlightenment, and of the ideological struggles of 17th century England (no
Lockean possessive individualism, no Jacksonian democracy). Those struggles were intimately tied to
England's political and military history in the 17th century, which is only comprehensible in light of
(among much else) the Norman Invasion, which in turn was only possible given the condition of Anglo-
Saxon England in 1066, but there would have been no Anglo-Saxon England had there not first been a 
Roman Britain. There would have been no Roman Britain had Britain not already been partly integrated 
into the broader trading network, which was largely on account, then, of its metals. 

So, reasoning anthropically, I can conclude, from my stray twenty, that it was necessary that there be
tin in southern Britain.
```

Cosma also led me to this paper by Lee Smolin, [Scientific alternatives to the anthropic principle](https://arxiv.org/abs/hep-th/0407213), whose abstract is just as blunt:

```markdown
It is explained in detail why the Anthropic Principle (AP) cannot yield any falsifiable predictions,
and therefore cannot be a part of science. Cases which have been claimed as successful predictions 
from the AP are shown to be not that. Either they are uncontroversial applications of selection 
principles in one universe (as in Dicke's argument), or the predictions made do not actually logically 
depend on any assumption about life or intelligence, but instead depend only on arguments from
observed facts (as in the case of arguments by Hoyle and Weinberg). The Principle of Mediocrity is
also examined and shown to be unreliable, as arguments for factually true conclusions can easily be
modified to lead to false conclusions by reasonable changes in the specification of the ensemble in 
which we are assumed to be typical. 

We show however that it is still possible to make falsifiable predictions from theories of 
multiverses, if the ensemble predicted has certain properties specified here. An example of such a 
falsifiable multiverse theory is cosmological natural selection. It is reviewed here and it is argued
that the theory remains unfalsified. But it is very vulnerable to falsification by current observations,
which shows that it is a scientific theory.
```

Cosma thinks Smolin is right on the money:

```markdown
Take, for instance, the example he gives in section 5.1.3. Fred Hoyle once reasoned that carbon is 
necessary for life, that carbon must have been formed by stellar nucleosynthesis, and that this reaction
could only have proceeded if carbon nuclei had certain properties, which experimentalists then proceeded
to show they did have. Smolin fairly schematizes this as follows. 

(1) X is necessary for life (or intelligence, etc.). 
(2) X is, as it happens, true. 
(3) If X is true, and the laws of physics are Y, then Z must also be true. 
(4) Therefore Z.

We see clearly that the prediction of Z in no way depends on step 1. The argument has the same force if 
step 1 is removed. To see this ask what we would do were Z found not to be true. Our only option would be
to question either Y or the deduction from the presently known laws of physics to Z. We might conclude 
that the deduction was wrong, for example if we made a mistake in a calculation. If no such option worked,
we might have to conclude that the laws of physics might have to be modified. But we would never question 1,
because, while a true fact, it plays no role in the logic of the argument leading to the prediction for Z.
```

There are actually seven subsections that Smolin devotes to 'varieties of the anthropic principle'; feel free to check them out.

<a name="#statistics"></a>
## Statistics
([overview](#overview))

<a name="#general-stats"></a>
### General stats
([overview](#overview))

Piet Hein:

```markdown
The road to wisdom? --- Well, it's plain
and simple to express:

 Err
 and err
 and err again
 but less
 and less
 and less.
```

Peter Medawar:

```markdown
Most scientists receive no tuition in scientific method, but those who have been instructed perform 
no better as scientists than those who have not. Of what other branch of learning can it be said that 
it gives its proficients no advantage; that it need not be taught or, if taught, need not be learned?
```

Cosma Shalizi:

```markdown
It’s much easier to get rid of wrong notions than it is to find correct ones, if the latter is 
possible at all.
```

Cosma Shalizi again, this time on an interesting argument for how “good statistics” should be defined (cf. [Properties versus Principles debate](http://bactra.org/notebooks/properties-vs-principles-for-statistics.html)):

```markdown
In his book The Theory of Literary Criticism, John Ellis argues that it's a mistake to try to define 
many categories in terms of criteria which are applicable to the objects of the categories in themselves; 
they are rather defined (in large part) by their relations to us and to our purposes. The really persuasive 
(to me) example is "weed": a weed is simply an obnoxious plant. Plants may be obnoxious because they are 
fast-growing, hardy, perennial, etc., etc., but none of these properties, or any Boolean combination 
thereof, *defines* weeds; their relation to our purposes in gardening does. It's perfectly sensible to say 
"kudzu is a weed, and one of the reasons why is that it grows so fast", but fast growth doesn't define weeds. 
The quest for criteria or defining principles of weed-hood is (if I may put it this way) fruitless.

I wonder if one can't say something similar about good statistics? What makes something a good method of 
statistical inference is that it gives us a reliable, low-error way of drawing conclusions from data. The 
reasons why a given procedure is reliable, and the ways we find them, are many and various. In the case of 
the Neyman-Pearson lemma, we directly minimize error probabilities; but sometimes we maximize likelihood, 
sometimes we use conditioning to update prior probability distributions, etc. None of these --- particularly 
the last --- *defines* a reliable way of learning from data.
```

<a name="#the-role-of-statistics-in-science"></a>
### The role of statistics in science
([overview](#overview))

This quote from Cosma's [book review](http://bactra.org/reviews/error/) of Deborah Mayo's *Error and the Growth of Experimental Knowledge* contains the most inventive and *original* description of Popperian falsification I’ve ever seen:

```markdown
In our own time, Medawar's friend Karl Popper achieved (fully deserved) eminence by tenacious 
insistence on the importance of this point, becoming a sort of Lenin of the philosophy of science. 
Instead of conferring patents of epistemic nobility, lawdoms and theoryhoods, on certain hypotheses, 
Popper hauled them all before an Anglo-Austrian Tribunal of Revolutionary Empirical Justice.

The procedure of the court was as follows: the accused was blindfolded, and the magistrates then formed 
a firing squad, shooting at it with every piece of possibly-refuting observational evidence they could 
find. Conjectures who refused to present themselves might lead harmless lives as metaphysics without 
scientific aspirations; conjectures detected peaking out from under the blindfold, so as to dodge the 
Tribunal's attempts at refutation, were declared pseudo-scientific and exiled from the Open Society of 
Science. Our best scientific theories, those Stakhanovites of knowledge, consisted of those conjectures 
which had survived harsh and repeated sessions before the Tribunal, demonstrated their loyalty to the 
Open Society by appearing before it again and again and offering the largest target to refutation that 
they could, and so retained their place in the revolutionary vanguard until they succumbed, or were 
displaced by another conjecture with even greater zeal for the Great Purge.

As Popper famously said, better our hypotheses die for our errors than ourselves... It's an answer with 
nice, clean lines, and makes lots of sense to the scientist-at-the-bench, like Medawar.

Alas, the Revolution runs into trouble on several fronts, for instance statistics.
```

Say more words? 

```markdown
Suppose I tell you that a certain slot machine will pay out money 99% of the time. Being credulous, unnaturally patient, and abundantly supplied with coins, you play it 10,000 times and find that it pays out only twice. This is sufficient for you to tell me to get stuffed, if not to sue, and one would think that it would be enough for the Tribunal to shoot my poor conjecture dead, but actually it escapes unharmed. The problem for Uncle Karl is that getting two successes in ten thousand trials is possible given my assertion, and the Tribunal is only authorized to eliminate conjectures in actual contradiction to the facts, as "no mammals lay eggs" is contradicted by the platypus. Popper realized this, and worried about it, eventually saying that we just have to make "risky decisions" about when to reject statistical hypotheses.

But the challenges facing the Tribunal in the execution of its duty mount: another "risky decision" is required, about what ammunition the firing squad can legitimately use, i.e., about what evidence will be accepted when we see whether or not a hypothesis stands up. (The number of times my students have apparently refuted physical laws gives me great sympathy for the European naturalists who refused to accept reports of the platypus's peculiarities for decades.) 

Then there is the problem of conjectural conspiracy: an isolated hypothesis almost never leads to anything we can test observationally; it is only in combination with "auxiliary" hypotheses, sometimes very many of them indeed, that is gives us actionable predictions. But then if a prediction proves false, all we learn is that at least one of our hypotheses is wrong, not which ones are the saboteurs. So far as deductive rectitude is concerned, we are free to frame whichever auxiliaries we like least, and save our favorite hypothesis from execution at the hands of the Tribunal.

The Tribunal even, for all its appearance of salutary rigor, lets far too many suspects go: every conjecture which is compatible with the evidence. These last two problems, respectively those of Quine-Duhem and of methodological underdetermination, are so severe that they form the core of the (intellectually respectable) argument for the counter-revolutionary deviation of scientific relativism. (The argument throttles itself neatly, but that's a subject for another essay.) 

Yet in ordinary life, never mind science, we evade these problems --- those of testing statistical hypotheses, of selecting evidence, of Quine-Duhem, of methodological underdetermination --- every time we change a light-bulb, so something has clearly gone very wrong here (as, in revolutions, things are wont to do).
```

Deborah Mayo on Popper being too soft with conjectures:

```markdown
Although Popper's work is full of exhortations to put hypotheses through the wringer, to make them "suffer 
in our stead in the struggle for the survival of the fittest," the tests Popper sets out are white-glove 
affairs of logical analysis. If anomalies are approached with white gloves, it is little wonder that they 
seem to tell us only that there is an error somewhere and that they are silent about its source. We have 
to become shrewd inquisitors of errors, interact with them, simulate them (with models and computers), 
amplify them: we have to learn to make them talk.
```

Where does Mayo come into the picture?

```markdown
Fortunately, scientists have not only devoted much effort to making errors talk, they have even developed
a theory of inquisition, in the form of mathematical statistics, especially the theory of statistical
inference worked out by Jerzy Neyman and Egon Pearson in the 1930s. Mayo's mission is largely to show how
this very standard mathematical statistics justifies a very large class of scientific inferences, those 
concerned with "experimental knowledge," and to suggest that the rest of our business can be justified on
similar grounds. Statistics becomes a kind of applied methodology, as well as the "continuation of 
experiment by other means."

Mayo's key notion is that of a severe test of a hypothesis, one with "an overwhelmingly good chance of
revealing the presence of a specific error, if it exists --- but not otherwise" (p. 7). More formally
(when we can be this formal), the severity of a passing result is the probability that, if the hypothesis
is false, our test would have given results which match the hypothesis less well than the ones we actually
got do, taking the hypothesis, the evidence used in the test, and the way of calculating fit between
hypothesis and evidence to be fixed. If a severe test does not turn up the error it looks for, it's good
grounds for thinking that the error is absent. By putting our hypotheses through a battery of severe 
tests, screening them for the members of our "error repertoire," our "canonical models of error," we can 
come to have considerable confidence that they are not mistaken in those respects. Instead of a method for
infallibly or even reliably finding truths, we have a host of methods for reliably finding errors: which
turns out to be good enough.

Experimental inquiry, for Mayo, consist of breaking down the question at hand into a series of small bits, 
each of which is relatively easily subjected to severe tests for error, or (depending on how you look at
it) is itself a severe probe for a certain error. In doing this we construct a "hierarchy of models" (an 
idea of Patrick Suppes's, here greatly elaborated). In particular, we need data models, models of how the
data are collected and massaged. "Error" here, as throughout Mayo's work, must be understood in a rather
catholic sense: any deviation from the conditions we assumed in our reasoning about what the experimental
outcomes should be. If we guess that a certain effect (the bending of spoons, let us say) is due to a
certain cause (e.g., the psychic powers of Mr. Uri Geller), it is not enough that spoons bend reliably in
his presence: we must also rule out other mechanisms which would produce the same effect (Mr. Geller's 
bending the spoons with his hands while we're not looking, his substituting pre-bent spoons for unbent 
ones ditto, etc., through material for several lawsuits for libel). But this solves the Quine-Duhem problem.

In fact, it gets better. Recall that methodological underdetermination (which goes by the apt name of MUD 
in Error) is the worry that no amount or quality of evidence will suffice to pick out one theory as the best,
because there are always indefinitely many others which are in equal accord with that evidence, or, to use 
older language, equally well save the phenomena. But saving the phenomena is not the same as being subjected
to a severe test: and, says Mayo, the point is severe testing. While I'm mostly persuaded by this argument,
I'm less sanguine than Mayo is about our ability to always find experimental tests which will let us
discriminate between two hypotheses. I'm fully persuaded that this kind of testing really does underwrite 
our knowledge of phenomena, of (in Nancy Cartwright's phrase) "nature's capacities and their measurement," 
and Mayo herself insists on the importance of experimental knowledge in just this sense (e.g., the remarks 
on "asking the wrong question," pp. 188--9). I'm less persuaded that we can usually or even often make 
justified inferences from this "formal" sort of experimental knowledge, knowledge of the distribution of 
experimental outcomes, to "substantive" statements about objects, processes and the like (e.g., from the
experimental success of quantum mechanics to wave-functions). As an unreconstructed (undeconstructed?) 
scientific realist, I make such inferences, and would like them to be justified, but find myself left hanging.

Distributions of experimental outcomes, then, are the key objects for Mayo's tests, especially the standard 
Neyman-Pearson statistical tests. The kind of probabilities Mayo, and Neyman and Pearson, use are
probabilities of various things happening: meaning that the probability of a certain result, p(A), is the 
proportion of times A occurs in many repetitions of the experiment, its frequency. This is a very familiar 
sense of probability; it's the one we invoke when we say that a fair coin has a 50% probability of coming up
heads, that the chance of getting three sixes with fair (six-sided!) dice is 1 in 216, that a certain 
laboratory procedure will make an indicator chemical change from red to blue 95% of the time when a toxin 
is present. Or, more to the present point: "the hypothesis is significant at the five percent level" means 
"the hypothesis passed the test, and the probability of its doing so, if it were false, is no more than five
percent," which means "if the hypothesis is false, and we repeated this experiment many times, we would
expect to get results inside our passing range no more than five percent of the time."
```

So Mayo is a frequentist. 

<a name="#statistical-literacy"></a>
### Statistical literacy
([overview](#overview))

Carl Shulman's [Research advice](https://docs.google.com/document/d/1_yuuheVqp1quDfkuRcpoW_HO7jPaI7QnRjF1zl_VovU/edit) has the following bit on statistical literacy which I feel really strongly about and wholeheartedly endorse:

```markdown
Use basic arithmetic and statistics frequently and briefly as part of thinking rather than as occasional or 
separate exercises. … Try to convert qualitative claims into quantitative Fermi estimates whenever possible.

In my experience people almost never do enough Fermis with look-ups from Wiki//government sources. And 
especially they have too high a barrier to doing it and won't do it in casual conversation or exploration.

Sometimes it's because talking is not about information and other things are shinier. Some people are afraid 
they'll be wrong, and don't trust their ability to do it (and don't test that). Sometimes it’s because of a 
lack of affordance/habit/knowing about the bigger and standard resources or the basic toolkits of Fermis: 
prices, populations, sales. Sometimes it’s issues with arithmetic and basic statistics fluency/aversion. 
Sometimes because it gives an unwelcome answer.

Worked examples with numbers and realistic figures erode plausible deniability and attractive lies, and force 
explicit claims, use of evidence, and argument.
```

<a name="#math"></a>
## Math
([overview](#overview))

<a name="#math-opinions"></a>
## Math opinions
([overview](#overview))

<a name="#Defining-combinatorics"></a>
### Defining combinatorics
([overview](#overview))

I took an introduction to combinatorics course with Igor Pak back in the day, and found him a fascinating character. Later Googling made me realize he was a pretty big-shot figure -- this gets lost in the Taos and Manolescus of the university, but he's still pretty influential in his subfield -- which made his pontifications somehow endearing. Here's his webpage [What is combinatorics?](https://igorpak.wordpress.com/2013/05/14/what-is-combinatorics/), where he first considers and dismisses popular definitions, and settles on the one given by Gian-Carlo Rota.

Igor rejects the usual "discrete stuff" approach:

```markdown
Few themes emerge.  First, that combinatorics is some kind of discrete universe which 
deals with discrete “configurations”, their existence and counting.  Where to begin?  
This is “sort of” correct, but largely useless.  Should we count logic, rectifiable 
knots and finite fields in, and things like Borsuk conjecture and algebraic combinatorics
out?  This is sort of like defining an elephant as a “large animal with a big trunk and
big ears”.  This “descriptive” definition may work for Webster’s dictionary, but if you 
have never seen an elephant, you really don’t know how big should be the ears, and have
a completely wrong idea about what is a trunk.  And if you have seen an elephant, this 
definition asks you to reject a baby elephant whose trunk and ears are smaller.  
Not good.
```

Igor rejects the "it's defined by its tools and methods, or lack of thereof" approach:

```markdown
Second theme: combinatorics is defined by its tools and methods, or lack of thereof.  
This is more of a wishful thinking than a working definition.  It is true that
practitioners in different parts of combinatorics place a great value on developing new
extensions and variations of the available tools, as well as ingenuous ad hoc arguments. 
But a general attitude, it seems, is basically “when it comes to problem solving, one 
can use whatever works”.  For example, our recent paper proves unimodality results for
the classical Gaussian coefficients and their generalizations via technical results for
Kronecker coefficients, a tool never been used for that before.  Does that make our 
paper “less combinatorial” somehow?  In fact, some experts openly advocate that the more 
advanced the tools are, the better, while others think that “term ‘combinatorial methods’,
has an oxymoronic character”.
```

Igor rejects the "it's ineffable" approach:

```markdown
Third theme: combinatorics is “special” and cannot be defined.  Ugh…  This reminds me of 
an old (1866), but sill politically potent Russian verse (multiple English translations)
by Tyutchev.  I can certainly understand the unwillingness to define combinatorics, but 
saying it is not possible is just not true.
```

Igor rejects the piecemeal approach:

```markdown
Either going over a long list of topics, or giving detailed and technical rules why 
something is and something isn’t combinatorics.  But this bound to raise controversy, 
like who decides?  For example, take PCM’s “few constraints” rule.  Really?  Somebody
thinks block designs, distance-regular graphs or coding theory have too few constraints? 
I don’t see it that way.  In general, this is an encyclopedia style approach.  It can 
work on Wikipedia which is constantly updated and the controversies are avoided by
constant search for a compromise (see also my old post), but it’s not a definition.
```

"Combinatorics is similarly formed by the (historical) battles, and can only be defined as such":

```markdown
After some reading and thinking, I concluded that Gian-Carlo Rota’s 44 y.o. explanation in 
“Discrete thoughts” is exactly right.  Let me illustrate it with my own (lame) metaphor.

Imagine you need to define Russia (not Tyutchev-style).  You can say it’s the largest country
by land mass, but that’s a description, not a definition.  The worst thing you can do is try
to define it as a “country in the North” or via its lengthy borders.  You see, Russia is huge, 
spead out and disconnected.  It lies to the North of China but has a disconnected common 
border, it has a 4253 mile border with Kazakhstan (longer than the US-Canada border excluding
Alaska), surrounding the country from three sides, it lies North-West of Japan, East of 
Latvia, South-West of Lithuania (look it up!), etc.  It even borders North Korea, not that 
this tiny border is much in use.  Basically, Russian borders are complicated and are a result
of numerous wars and population shifts; they have changed many times and might change again.

Now, Rota argues that Combinatorics is similarly formed by the battles, and can only be 
defined as such.  It is a large interconnected field concentrated (but not coinciding!) around
basic discrete tools and problems, but with tentacles reaching deep into “foreign territory”.
Its current shape is a result of numerous “wars” – the borderline problems are tested on which
tools are more successful, and whoever “wins”, gets to absorb a new subfield.  For example, in
its “war” with topology, combinatorics “won” graph theory and “lost” knot theory (despite a 
strong combinatorial influence).  In other areas, such as computer science and discrete 
probability, Rota argues there a lot of cooperation, a mutually beneficial “joint governance” 
(all lame metaphors are mine).  But as a consequence, if one is to define Combinatorics (or 
Russia), the historical-cultural approach would go best.  Not all that different from Sheldon’s
approach to define Physics “from the beginning”.

In conclusion, let’s acknowledge that Combinatorics can indeed be defined in the same (lengthy
historical) manner as a large diverse country, but such definition would be neither short nor
enlightening, more like a short survey.  As Danny Kleitman writes, in practice this lack of a
clear and meaningful definition of the subject “never bothered him”, and we agree.  I think 
it’s time to stop worrying about that.  But if someone makes blank general statements painting
all of combinatorics in a certain way, this is just indefensible.
```

There's also a huge collection of quotes by Igor [here](http://www.math.ucla.edu/~pak/hidden/papers/Quotes/Combinatorics-quotes.htm).

<a name="#two-cultures"></a>
### Two cultures
([overview](#overview))

There is already [an nLab note on this](https://ncatlab.org/davidcorfield/show/Two+Cultures) by the mathematical philosopher David Corfield summarizing most of what's been talked about online. I'll pick quotes I liked for my own worldview-training. 

In the [original essay](http://www.dpmms.cam.ac.uk/~wtg10/2cultures.pdf) (17 pages), Tim Gowers was drawing attention/pointing out a parallel to the following asymmetry in understanding due to lack of communication between science and the humanities as exemplified by the following quote by CP Snow in his "Two Cultures" Rede lecture in 1959: 

```markdown
A good many times I have been present at gatherings of people who, by the standards
of the traditional culture, are thought highly educated and who have with considerable
gusto been expressing their incredulity at the illiteracy of scientists. Once or twice I
have been provoked and have asked the company how many of them could describe
the Second Law of Thermodynamics. The response was cold: it was also negative.
Yet I was asking something which is about the scientific equivalent of: 
*Have you read a work of Shakespeare’s?*
```

Gowers claims that "a similar sociological phenomenon can be observed within pure mathematics":

```markdown
Loosely speaking, I mean the distinction between mathematicians who regard their central
aim as being to solve problems, and those who are more concerned with building and
understanding theories. ...

When I say that mathematicians can be classified into theory-builders and problem-solvers, 
I am talking about their priorities, rather than making the ridiculous claim that they are
exclusively devoted to only one sort of mathematical activity.

As with most categorizations, it involves a certain oversimplification, but not so much as 
to make it useless.
```

The oversimplification part shouldn't be a point of contention if you've internalized principal component analysis, or Box's "all models are wrong but some are useful" dictum. It is *understood* that the classification oversimplifies; the point is whether it's an illuminating classification.

Gowers notes that he isn't the first to point this out. Here's Gian-Carlo Rota in *Indiscrete Thoughts*:

```markdown
Mathematicians can be subdivided into two types: problem solvers and theorizers. Most 
mathematicians are a mixture of the two although it is easy to find extreme examples of both
types.

To the problem solver, the supreme achievement in mathematics is the solution to a problem
that has been given up as hopeless. It matters little that the solution may be clumsy; all
that counts is that it should be the first and that the proof be correct. Once the problem 
solver finds the solution, he will permanently lose interest in it, and will listen to new
simplified proofs with an air of condescension suffused with boredom.

The problem solver is a conservative at heart. For him, mathematics consists of a sequence
of challenges to be met, an obstacle course of problems. The mathematical concepts required
to state mathematical problems are tacitly assumed to be eternal and immutable.

Mathematical exposition is regarded as an inferior undertaking. New theories are viewed with
deep suspicion, as intruders who must prove their worth by posing challenging problems before
they can gain attention. The problem solver resents generalizations, especially those that 
may succeed in trivializing the solution of one of his problems.

The problem solver is the role model for budding young mathematicians. When we describe to the
public the conquests of mathematics, our shining heroes are the problem solvers.

To the theorizer, the supreme achievement of mathematics is a theory that sheds sudden light on
some incomprehensible phenomenon. Success in mathematics does not lie in solving problems but 
in their trivialization. The moment of glory comes with the discovery of a new theory that does
not solve any of the old problems but renders them irrelevant.

The theorizer is a revolutionary at heart. Mathematical concepts received from the past are
regarded as imperfect instances of more general ones yet to be discovered. Mathematical exposition
is considered a more difficult undertaking than mathematical research.

To the theorizer, the only mathematics that will survive are the definitions. Great definitions 
are what mathematicians contribute to the world. Theorems are tolerated as a necessary evil since
they play a supporting role — or rather, as the theorizer will reluctantly admit, an essential 
role — in the understanding of definitions.

Theorizers often have trouble being recognized by the community of mathematicians. Their 
consolation is the certainty, which may or may not be borne out by history, that their theories
will survive long after the problems of the day have been forgotten.

If I were a space engineer looking for a mathematician to help me send a rocket into space, I 
would choose a problem solver. But if I were looking for a mathematician to give a good education 
to my child, I would unhesitatingly prefer a theorizer.
```

Drossbucket quotes Poincare's [Intuition and logic in mathematics](http://www-history.mcs.st-andrews.ac.uk/Extras/Poincare_Intuition.html):

```markdown
It is impossible to study the works of the great mathematicians, or even those of the lesser,
without noticing and distinguishing two opposite tendencies, or rather two entirely different 
kinds of minds. 

The one sort are above all preoccupied with logic; to read their works, one is tempted to believe
they have advanced only step by step, after the manner of a Vauban who pushes on his trenches
against the place besieged, leaving nothing to chance.

The other sort are guided by intuition and at the first stroke make quick but sometimes 
precarious conquests, like bold cavalrymen of the advance guard.
```

Atiyah casts it in terms of dominant visual vs sequential thinking in [What is geometry?](http://www.jstor.org/discover/10.2307/3616542?sid=21105166613741&uid=4&uid=3738032&uid=2):

```markdown
Broadly speaking I want to suggest that geometry is that part of mathematics in which 
visual thought is dominant whereas algebra is that part in which sequential thought is
dominant. This dichotomy is perhaps better conveyed by the words “insight” versus “rigour”
and both play an essential role in real mathematical problems.
```

Mark Kac's "magician" quote:

```markdown
There are two kinds of geniuses: the ‘ordinary’ and the ‘magicians.’ an ordinary genius is a 
fellow whom you and I would be just as good as, if we were only many times better. There is 
no mystery as to how his mind works. Once we understand what they’ve done, we feel certain 
that we, too, could have done it. It is different with the magicians... Feynman is a magician
of the highest caliber.
```

Steven Weinberg's *Dreams of a Final Theory*:

```markdown
Theoretical physicists in their most successful work tend to play one of two roles: they are 
either sages or magicians... It is possible to teach general relativity today by following 
pretty much the same line of reasoning that Einstein used when he finally wrote up his work in 1915. 

Then there are magician-physicists, who do not seem to be reasoning at all but who jump over 
all intermediate steps to a new insight about nature. The authors of physics textbook are usually
compelled to redo the work of the magicians so they seem like sages; otherwise no reader would 
understand the physics.
```

Before we move on: why dichotomy? Why not trichotomy, or "quadrochotomy" [like Barry Mazur's tribes](#what-every-mathematician-should-know)? Commenter alkjash on drossbucket's post [Two types of mathematician](https://www.lesswrong.com/posts/5QnvHZpy4pGgCo3Pp/two-types-of-mathematician) says it's because a dichotomy corresponds to doing PCA:

```markdown
All over the place, we speak in terms of dichotomies and not trichotomies or more. The reason
is basically that each dichotomy corresponds to doing PCA and projecting space onto a single 
axis, and a one-dimensional line has two directions. This suggests that much of the interesting
conversation about any given topic (i.e. axis) can be picked up by having exactly two people 
talk about it. Two people will always differ slightly on the axis. Adding any additional people
to a conversation has rapidly diminishing returns: you may have more total disagreement, but 
rarely more total *dimensionality* in the disagreement.

Applying the Solitaire Principle, for all the same reasons it's useful to have conversations
between two people, it's most useful to draw dichotomies between two pieces of the mind instead
of more. This is why we have Internal Double Crux instead of Triple or more. A conversation/internal
conflict is always about something and should have a purpose, and that purpose projects the entire
conversation onto the one relevant dimension, so it's really only necessary to divide into two sides
along this axis. Thus we get S1/S2, Elephant/Rider, Episodic/Diachronic, etc.
```

Why project onto a single axis in the first place? Qiaochu:

```markdown
There are many reasons it's so tempting to project onto a single axis but maybe the foundational
reason is the dichotomy between approaching and avoiding, or if you prefer, between positive and 
negative reward in reinforcement learning terms. This blows up into good vs. evil, friend vs. enemy,
and so forth.
```

So what's the asymmetry?

```markdown
Subjects that appeal to theory-builders are, at the moment,
much more fashionable than the ones that appeal to problem-solvers.
Moreover, mathematicians in the theory-building areas often regard what they are doing 
as the central core (Atiyah uses this exact phrase) of mathematics, with subjects such 
as combinatorics thought of as peripheral and not particularly relevant to the main aims 
of mathematics. 

One can almost imagine a gathering of highly educated mathematicians 
expressing their incredulity at the ignorance of combinatorialists, most of whom could 
say nothing intelligent about quantum groups, mirror symmetry, Calabi-Yau manifolds, the 
Yang-Mills equation, solitons or even cohomology. If a combinatorialist were to interrupt 
such a gathering and ask roughly how many subsets of {1, 2, . . . , n} can be found such that
the symmetric difference of any two of them has size at least n/3, the response might very 
well be a little frosty. (This problem is very easy if and only if one knows the appropriate
technique, which is to choose sets randomly and show that the chances of any given pair of them
having a symmetric difference of size less than n/3 are exponentially small. So the answer
is e^cn for some constant c > 0.) ...

My main purpose here is to defend some of the less fashionable subjects against
criticisms commonly made of them. I shall devote most of my attention to combinatorics,
since this is the area I know best.
```

A litmus test to see which camp you belong to:

```markdown
(i) The point of solving problems is to understand mathematics better.
(ii) The point of understanding mathematics is to become better able to solve problems.

Most mathematicians would say that there is truth in both (i) and (ii). Not all problems
are equally interesting, and one way of distinguishing the more interesting ones is to
demonstrate that they improve our understanding of mathematics as a whole. Equally,
if somebody spends many years struggling to understand a difficult area of mathematics,
but does not actually do anything with this understanding, then why should anybody else
care? 

However, many, and perhaps most, mathematicians will not agree equally strongly
with the two statements.
```

A prototypical theory-builder would be Michael Atiyah (RIP). From a 1984 interview in the Mathematical Intelligencer:

```markdown
MINIO: How do you select a problem to study?

ATIYAH: I think that presupposes an answer. I don’t think that’s the way I work
at all. Some people may sit back and say, “I want to solve this problem” and they
sit down and say, “How do I solve this problem?” I don’t. I just move around in
the mathematical waters, thinking about things, being curious, interested, talking to
people, stirring up ideas; things emerge and I follow them up. Or I see something
which connects up with something else I know about, and I try to put them together
and things develop. I have practically never started off with any idea of what I’m
going to be doing or where it’s going to go. I’m interested in mathematics; I talk, I
learn, I discuss and then interesting questions simply emerge. I have never started off
with a particular goal, except the goal of understanding mathematics.
```

(Tangentially, this sounds like what John Baez does, as someone who's followed him for years.)

A prototypical problem-solver would be Erdos. See also [Golovanov on Grigori Perelman](#grigori-perelman).

Different branches of math, Gowers claims, "obviously" need different aptitudes (*orientations* is the more correct word here):

```markdown
In some, such as algebraic number theory, or the cluster of subjects
now known simply as Geometry, it seems (to an outsider at least - I have no authority for
what I am saying) to be important for many reasons to build up a considerable expertise
and knowledge of the work of other mathematicians are doing, as progress is often the
result of clever combinations of a wide range of existing results. Moreover, if one selects
a problem, works on it in isolation for a few years and finally solves it, there is a danger,
unless the problem is very famous, that it will no longer be regarded as all that significant.

At the other end of the spectrum is, for example, graph theory, where the basic object,
a graph, can be immediately comprehended. One will not get anywhere in graph theory
by sitting in an armchair and trying to *understand graphs* better. Neither is it particularly
necessary to read much of the literature before tackling a problem: it is of course helpful
to be aware of some of the most important techniques, but the interesting problems tend
to be open precisely because the established techniques cannot easily be applied.
```

Gowers continues with his defense of the problem-solving orientation/subculture:

```markdown
Why should problem-solving subjects be less highly regarded than theoretical ones?
To answer this question we must consider a more fundamental one: what makes one piece
of mathematics more interesting than another? 

Atiyah makes the point (see for example [A2]) that so much
mathematics is produced that it is not possible for all of it to be remembered. The processes
of abstraction and generalization are therefore very important as a means of making sense
of the huge mass of raw data (that is, proofs of individual theorems) and enabling at least
some of it to be passed on. The results that will last are the ones that can be organized
coherently and explained economically to future generations of mathematicians. Of course,
some results will be remembered because they solve very famous problems, but even these,
if they do not fit into an organizing framework, are unlikely to be studied in detail by more
than a handful of mathematicians.

Thus, it is useful to think not so much about the intrinsic interest of a mathematical
result as about how effectively that result can be communicated to other mathematicians,
both present and future. Combinatorics appears to many to consist of a large number
of isolated problems and results, and therefore to be at a disadvantage in this respect.
Each result individually may well require enormous ingenuity, but ingenious people exist,
especially in Hungary, and future generations of combinatorialists will not have the time
or inclination to read and admire more than a tiny fraction of their output.

It is certainly rare in combinatorics for
somebody to find a very general statement which suddenly places a large number of existing 
results in their proper context. It is also true that many of the results proved by
combinatorialists are somewhat isolated and will be completely forgotten (but this does
not distinguish combinatorics from any other branch of mathematics). However, it is not
true that there is no structure at all to the subject. The reason it appears to many mathe-
maticians as though combinatorics is just a miscellaneous collection of individual problems
and results is that the organizing principles are less explicit.

If the processes of abstraction and generalization, which are so important in mathematics,
are of only limited use in combinatorics, then how can the subject be transmitted
to future generations? One way of thinking about this question is to ask what the 
requirements of tomorrow’s combinatorialists are likely to be. As I have said already, their
priority is likely to be solving problems, so their interest in one of today’s results will
be closely related to whether, by understanding it, they will improve their own problem-
solving ability. And this brings us straight to the heart of the matter. The important ideas
of combinatorics do not usually appear in the form of precisely stated theorems, but more
often as *general principles of wide applicability*.
```

Gowers points out that these principles are so powerful that problems can "suddenly switch from being impossible to being almost trivia". 

An example of such a general principle comes from Erdos, which "opened the floodgates to probabilistic arguments in combinatoric":

```markdown

if one is trying to maximize the size of some structure under certain constraints, and
if the constraints seem to force the extremal examples to be spread about in a uniform
sort of way, then choosing an example randomly is likely to give a good answer.
```

This "general principles" phenomenon happens in in nonlinear PDEs too. Terry Tao says in his answer to the MO question [Why can’t there be a general theory of nonlinear PDE?](http://mathoverflow.net/questions/15292/why-cant-there-be-a-general-theory-of-nonlinear-pde):

```markdown
PDE does not have a general theory, but it does have a general set of principles and methods
(e.g. continuity arguments, energy arguments, variational principles, etc.).

Sergiu Klainerman's "PDE as a unified subject" discusses this topic fairly exhaustively.

Any given system of PDE tends to have a combination of ingredients interacting with each
other, such as dispersion, dissipation, ellipticity, nonlinearity, transport, surface tension,
incompressibility, etc. Each one of these phenomena has a very different character. Often the
main goal in analysing such a PDE is to see which of the phenomena "dominates", as this tends
to determine the qualitative behaviour (e.g. blowup versus regularity, stability versus 
instability, integrability versus chaos, etc.) But the sheer number of ways one could combine
all these different phenomena together seems to preclude any simple theory to describe it all.
This is in contrast with the more rigid structures one sees in the more algebraic sides of 
mathematics, where there is so much symmetry in place that the range of possible behaviour is
much more limited. (The theory of completely integrable systems is perhaps the one place where
something analogous occurs in PDE, but the completely integrable systems are a very, very small 
subset of the set of all possible PDE.)
```

Sergiu Klainerman's [PDE AS A UNIFIED SUBJECT](https://web.math.princeton.edu/~seri/homepage/papers/telaviv.pdf) has some great quotes itself expanding on what Terry said above, but Markdown screws with the quotes, so I'll just leave the link there.

Related to the "general principles" phenomenon is the *"become the grand meta-theorem"* comment on drossbucket's LW post [Two types of mathematician](https://www.lesswrong.com/posts/5QnvHZpy4pGgCo3Pp/two-types-of-mathematician) by alkjash that I keep (1) misattributing to Qiaochu (2) losing, irritatingly, so I'm pinning it down here:

```markdown
My take on the "Two Cultures" model of problem-solvers and theory-builders: theory-building
fields of mathematics like algebraic topology (say) are those where the goal is to articulate 
grand meta-theorems that are bigger than any particular application. This was the work of a
Grothendieck.

Meanwhile, concrete problem-solving fields of mathematics like combinatorics are those where
the goal is to become the grand meta-theorem that contains more understanding than any 
particular theorem you can prove. This was the style of an Erdos. The inarticulate grand meta-
theorems lived in his cognitive strategies so that the theorems he actually proved are 
individually only faint impressions thereof. (I think I heard the "become grand meta-theorem" 
phrasing originally from Alon & Spencer.)

Of course reality is self-similar, so it's not surprising that there's currently a big divide
in combinatorics between what I would call the "algebraic/enumerative" style of Richard Stanley
containing the Flajolet and Sedgewick stuff, characterized by fancy algebra/explicit formulae/
crystalline structures and the "analytic/extremal" style of Erdos, characterized by asymptotic
formulae and less legibility. It's surprisingly rare to see a combinatorialist bridge this gap.
```

Qiaochu responds by casting the difference in terms of legibility:

```markdown
Yeah, there's something less legible about combinatorics compared to most other fields of 
mathematics. People like Erdos know lots of important principles and meta-principles for 
solving combinatorial problems but it's a tremendous chore to state those principles explicitly
in terms of theorems and nobody really does it (the closest thing I've seen is Flajolet and 
Sedgewick - by the way, amazing book, highly recommended). A concrete example here is the 
exponential formula, which is orders of magnitude more complicated to state precisely than
it is to understand and use.
```

Qiaochu, re: hammers and nails dichotomy:

```markdown
Theory-building hammers are legible: they're big theorems, or maybe big messes of definitions 
and then theorems (the term of art for this is "machinery"). 

Problem-solving hammers are illegible: they're a bunch of tacit knowledge sitting inside 
some mathematician's head.
```

Sarah Constantin with a 2x2:

```markdown
Ok, here’s a 2x2 that captures a lot of the variation in OP:

abstract/concrete x intuitive/methodical.

Intuitive vs. Methodical is what Atiyah, Klein, and Poincare are talking about. 
Abstract vs Concrete is what Gowers, Rota, and Dyson are talking about.

Abstract and intuitive is like Grothendieck.

Concrete and intuitive is like geometry or combinatorics.

Concrete and methodical is like analysis.

Abstract and methodical — I don’t know what goes in this space.
```

More examples by Terry Tao:

```markdown
There is a famous distinction in prime number theory between the number theorists who 
like to multiply primes, and the number theorists who like to add primes. As the primes 
are very heavily multiplicatively structured, the mathematics of multiplying primes is 
very algebraic in nature, in particular involving field extensions, Galois representations,
etc. But the primes are very additively unstructured, and so for adding primes we see the
tools of analysis used instead (circle method, sieve theory, etc.).

“Counting primes in arithmetic progressions” can be easily converted (via the multiplicative 
Fourier transform) to “counting primes twisted by characters” - an eminently multiplicative
question, and one which is handled by primarily algebraic means. “Counting arithmetic
progressions in primes” however seems to be a stubbornly additive question and thus not 
really amenable to algebraic attacks. (On the other hand, “counting geometric progressions 
in primes” is very easy. :-) ).
```

Terry's comment on "structure" as part of structure/pseudorandom dichotomy and "structure" as seen by theory-builders, where he introduces the notion of "noisy" additive cohomologies and group theories:

```markdown
It does seem that categorification and similar theoretical frameworks are currently better 
for manipulating the type of exact mathematical properties (identities, exact symmetries,
etc.) that show up in structured objects than the fuzzier type of properties (correlation, 
approximation, independence, etc.) that show up in pseudorandom objects, but this may well 
be just a reflection of the state of the art than of some fundamental restriction. 

For instance, in my work on the Gowers uniformity norms, there are hints of some sort of
“noisy additive cohomology” beginning to emerge – for instance, one may have some sort of 
function which is “approximately linear” in the sense that its second “derivative” is “mostly
negligible”, and one wants to show that it in fact differs from a genuinely linear function
by some “small” error; this strongly feels like a cohomological question, but we do not yet
have the abstract theoretical machinery to place it in the classical cohomology framework
(except perhaps in the ergodic theory limit of these problems, where there does seem to be
a reasonable interpretation of these informal concepts). 

Similarly, when considering inverse theorems in additive combinatorics, a lot of what we do 
has the feel of “noisy group theory”, and we can already develop noisy analogues of some 
primitive group theory concepts (e.g. quotient groups, group extensions, the homomorphism 
theorems, etc.), but we are nowhere near the level of sophistication (and categorification,
etc.) with noisy algebra that exact algebra enjoys right now. But perhaps that will change in
the future.
```

Interesting remark by Dave Tweed on applied vs pure math as popular uprising vs army tank division:

```markdown
Disclaimer: I’m claiming “pure” mathematics is any activity where one has decreed that the entities
in the problem “work this way” and then proceeds, even if the problem is pursued partly for some other
reason. To me, “applied” really means one is constantly rejecting/modifying parts of the mathematics
“on physical grounds”.

Part of the justification for theory building is that it’s the equivalent of building an army tank
division rather than using a popular uprising: by optimally structuring things you can accomplish
exactly the same things but much, much easier. A good example of this is the theory of generating
functions and precise asymptotic estimation of coefficients. People like Philippe Flajolet are 
doing incredibly intricate and – to my mind – deep work putting together theories that can 
pulverise some of the combinatorial problems that occur in algorithm analysis. This is theory to
solve problems, rather than give pedantically correct names to everything. ...

In contrast, one area I’d like to work in seems to scream out for an organising theory, namely 
the mathematics/algorithmics of scene reconstruction given some subset of camera parameters and
scene parameters/structures. Part of the reason it’s an area “I’d like to work in” rather than 
“an area I do work in” is that there seems to be little structuring theory with sudden appearance 
of different matrix decompositions and different classifications of the degenerate conditions,
etc, at seemingly completely unrelated ways in the different algorithms, so it seems like 
currently to get to the cutting (maybe bleeding :-) ) edge you’ve got to study each algorithm 
from scratch. I’ve seen some work that claims to provide a theory for such things, but they all 
look like “getting the names right” theories rather than “army tank division theories”. 

...

What I was trying to get at by hyperbole in the original post was that you can have theories 
that “explain” things and “get the names right” and they’re good things to have developed. There
seemed to me a sort-of subtext in your post that most theories were of this sort (which is
probably a mis-reading based partly on the “aesthetic” point), so I was pointing out that there 
are other theories which are more about systematizing powerful tools. (Indeed, it’s really a
spectrum rather than a binary choice.) I’m not putting down explanatory theories but rather
pointing out there are also some really rather splendid “tank division” theories around. (Indeed,
arguably differential/integral calculus and linear algebra are more commonly used to “get 
answers” rather than “explain”.)

When I talk about “getting the names right” theories I just mean the cases where pains are taken
in definitions yet it’s not clear that this buys you anything more than precise definitions. This
is in contrast to, say, the more rigorous development of calculus in the nineteenth century which
actually clarifies the errors in some “paradoxes/contradictions” that were “constructible” before.
```

Another difference between the cultures is given in [this comment by John Baez](https://golem.ph.utexas.edu/category/2007/05/the_two_cultures_of_mathematic_1.html#c009757) responding to Terry Tao's claim that

```markdown
Secondly, one thing about the “problem solving” culture as opposed to the “theory building” is
that we tend to make progress by working upward from special cases, rather than downward from more
general formulations.
```

with this:

```markdown
It’s all very complicated.

Theory-builders may *act* as if general theories come full-blown into their heads, just waiting
to be applied to special cases. But often the most exciting part is finding those general theories 
in the first place. For this, one has to dream up patterns that would make sense of obscure clues.
This often progresses from special to general.

I think of my style as ‘finding paths of least resistance’, or ‘following the tao of mathematics’.
I’m more of a hiker than a rock-climber. I prefer to know lots of fun trails, and occasionally bump
into a new one while I’m strolling about, than spend my day trying to climb up a sheer cliff. 
Instead of trying to do anything hard, I prefer to keep lots of facts, patterns and questions in
mind, and make obvious guesses and deductions when I notice patterns turning up.

Given this rather lazy approach to mathematics, the only way to discover anything new is by knowing
a little about lots of things, and knowing some good patterns that haven’t been fully exploited. 
‘Categorification’ is my favorite pattern, because it’s extremely broad, and nowhere near being 
tapped out. But, it’s just one of many.

We can imagine two extremes. At one extreme we have someone who tries very hard to open a specific
door, perhaps melting the lock with an acetylene torch if necessary, or even blowing the door open 
with dynamite. At the other extreme, we have someone who carries a huge ring of keys and tries them 
all on any door they happen to walk by.
```

This reminds me of Julie Moronuki aka Monoid Mary aka Argumatronic's great essay [The Unreasonable Effectiveness of Metaphor](https://argumatronic.com/posts/2018-09-02-effective-metaphor.html), which I keep returning to but don't know where to categorize here.

<a name="#How-mathematical-structures-are-defined"></a>
### How mathematical structures are defined
([overview](#overview))

From Terry Tao's [post on Buzz](http://www.google.com/buzz/114134834346472219368/Mu6ZqEcaZkR/When-defining-the-concept-of-a-mathematical-space) via David Corfield's [notebook entry](https://ncatlab.org/davidcorfield/show/Two+Cultures) (the original link has succumbed to linkrot):

```markdown
When defining the concept of a mathematical space or structure (e.g. a group, a vector space, a Hilbert space, etc.), one needs to list a certain number of axioms or conditions that one wants the space to satisfy. Broadly speaking, one can divide these conditions into three classes:

1. **closed conditions**. These are conditions that generally involve an = or a ≥ sign or the universal quantifier, and thus codify such things as algebraic structure, non-negativity, non-strict monotonicity, semi-definiteness, etc. As the name suggests, such conditions tend to be closed with respect to limits and morphisms.

2. **open conditions**. These are conditions that generally involve a ≠ or a > sign or the existential quantifier, and thus codify such things as non-degeneracy, finiteness, injectivity, surjectivity, invertibility, positivity, strict monotonicity, definiteness, genericity, etc. These conditions tend to be stable with respect to perturbations.

3. **hybrid conditions**. These are conditions that involve too many quantifiers and relations of both types to be either open or closed. Conditions that codify topological, smooth, or metric structure (e.g. continuity, compactness, completeness, connectedness, regularity) tend to be of this type (this is the notorious “epsilon-delta” business), as are conditions that involve subobjects (e.g. the property of a group being simple, or a representation being irreducible). These conditions tend to have fewer closure and stability properties than the first two (e.g. they may only be closed or stable in sufficiently strong topologies). (But there are sometimes some deep and powerful rigidity theorems that give more closure and stability here than one might naively expect.)

Ideally, one wants to have one’s concept of a mathematical structure be both closed under limits, and also stable with respect to perturbations, but it is rare that one can do both at once. Instead, one often has to have two classes for a single concept: a larger class of “weak” spaces that only have the closed conditions (and so are closed under limits) but could possibly be degenerate or singular in a number of ways, and a smaller class of “strong” spaces inside that have the open and hybrid conditions also. A typical example: the class of Hilbert spaces is contained inside the larger class of pre-Hilbert spaces. Another example: the class of smooth functions is contained inside the larger class of distributions.

As a general rule, algebra tends to favour closed and hybrid conditions, whereas analysis tends to favour open and hybrid conditions. Thus, in the more algebraic part of mathematics, one usually includes degenerate elements in a class (e.g. the empty set is a set; a line is a curve; a square or line segment is a rectangle; the zero morphism is a morphism; etc.), while in the more analytic parts of mathematics, one often excludes them (Hilbert spaces are strictly positive-definite; topologies are usually Hausdorff (or at least T_0); traces are usually faithful; etc.)
```

<a name="#why-tricki-failed"></a>
### Why Tricki failed
([overview](#overview)

A couple quotes from Tim Gowers’ post [Is the Tricki dead?](https://gowers.wordpress.com/2010/09/24/is-the-tricki-dead/) saved for future personal reference, answering some questions I’ve had in mind for awhile (and many I hadn’t).

What was the Tricki for?

```markdown
The hope was that after the site had reached some level of completeness, it would be possible to
take a mathematics research problem (or subproblem) and search efficiently for known techniques 
that were likely to be relevant.

It would be doing something a little different from Wikipedia mathematics articles, which concentrate
more on what I like to think of as “things with names”. For instance, if you suspect that discrete 
Fourier analysis is likely to be a useful tool for your problem, then you can type “discrete Fourier 
analysis” into Google and find many links, including to a Wikipedia article that contains many of the
basic facts.

But what if it doesn’t occur to you that discrete Fourier analysis is what you need (even though it in
fact is)? The idea was that, using easily identifiable features of your problem, you would be able to
follow links to ever more specific Tricki pages until you would reach a page that explained *when* 
discrete Fourier analysis was useful and *how* it was used. In general, the whole site would be about how
to do mathematics rather than about lists of facts.
```

One of Gowers’ strongest motivations:

```markdown
the thought that one day the [automated theorem proving] programmers would be able to do all the low-level 
stuff and the Tricki would be able to tell them how to do the higher-level stuff.
```

He notes that the ‘activation energy’ required to write a Tricki article is prohibitively high:

```markdown
I still believe in this general concept, but for a wiki-type site to be successful it must reach the stage
where people think it is worth contributing. Since writing a good Tricki article takes a bit of work, the 
motivation to do it has to be particularly high. And it seems that it is not high enough for the site to 
have taken off.
```

One-time contributor Emmanuel Kowalski agrees:

```markdown
I still like the idea of the Tricki a lot. For me, the difficulty in writing more articles is that 
I feel that a Tricki article should be well-written, in a way which is different from either a blog
post, or a MathOverflow question or answer. And this means it involves more time — I typically write
blog posts in one sitting, and would dash off an answer to a MO question as I would a comment to a 
blog post, but when I wrote the only Tricki article I have done, it was more like writing a short 
survey paper. Unfortunately, time is difficult to find for such contributions, at least for me…
```

This is related to the following comment by Joseph Malkevitch:

```markdown
Unfortunately, for those contributors to Tricki who are academic mathematicians (employees of mathematics
departments) there is little career incentive to writing or developing materials of this kind.

Perhaps there might be a group of people who would volunteer to be “editors” for certain pieces of Tricki 
and when these individuals saw a post to MO, journal article, or web article, with content related to the
area they “supervise,” they could contact the author of that item to modify if for posting on Tricki. The
fact that there is less work in adapting something nice that has been done already compared with creating
something new from scratch may encourage more contributions.
```

The part about lack of career incentive reminds me of Chris Olah’s [Research Debt](https://distill.pub/2017/research-debt/) essay, under ‘Where are the Distillers?’:

```markdown
Like the theoretician, the experimentalist or the research engineer, the research distiller is an integral 
role for a healthy research community. Right now, almost no one is filling it.

Why do researchers not work on distillation? One possibility is perverse incentives, like wanting your 
work to look difficult. Those certainly exist, but we don’t think they’re the main factor.

Another possibility is that they don’t enjoy research distillation. Again, we don’t think that’s what’s
going on.

Lots of people want to work on research distillation. Unfortunately, it’s very difficult to do so, 
because we don’t support them.

An aspiring research distiller lacks many things that are easy to take for granted: a career path, places
to learn, examples and role models. Underlying this is a deeper issue: their work isn’t seen as a real 
research contribution. We need to fix this.
```

And then MO came along:

```markdown
During that time, a new factor has come into play: Mathoverflow. For the Tricki to be successful, it 
had to do something that Wikipedia doesn’t do. And now, to be successful, it would also have to do 
something that Mathoverflow doesn’t do. This is a serious point: I used to think that one of the main
functions of the Tricki would be to make it much easier for people to find out what was known about 
a problem, but Mathoverflow seems to me to be a better way of doing that. …

Is there, at least in principle, still a niche for the Tricki, or is it squeezed out by Wikipedia on 
the static side and Mathoverflow on the dynamic side?
```

What niche might Tricki still occupy that hasn’t been taken by MO?

```markdown
I’m thinking of questions along the lines of “This problem that has just emerged is of a kind I haven’t
seen before but it feels as though people ought to have thought about similar things; what should I do
now?” Obviously Mathoverflow would be useful for some such problems, but sometimes they are a bit too 
vague, sometimes one might have a subproblem that one does not wish to make public, and sometimes they 
may be sufficiently easy that one would prefer just to look up the answer rather than bother other 
people with the question.

[Also] if the Tricki became more complete, then one could browse it more systematically than either 
Wikipedia or Mathoverflow. One could for example decide to read up on all the standard techniques in
some subarea.
```

Some comments by Greg Graviton:

```markdown
First of all, I think the quality of the articles on the Tricki is excellent and well worth preserving
in form or another, for instance the discussion of homology as “linear combinations of level lines
(lines/surfaces where a function is constant)”. Most importantly, you can’t find this stuff elsewhere
except in informal discussions with other mathematicians. There ought to be a place to make that public.

Concerning the format, I think the current technology of the Tricki fails to deliver your original 
“promise” of a powerful search function, that somehow helps you from nothing to a trick that applies.
This is a hard problem, it’s not sure whether it can be solved with computers, and bolting the Tricki 
on top of a Drupal installation apparently does not really solve anything. (On the technology note, note
that MathOverflow was programmed from scratch as well, so a fresh start is quite unavoidable, not to 
mention that even small issues in usability (e.g. sluggish to load) matter a lot.)
```

I think the Tricki failed for the same reason Arbital failed: [Arbital postmortem](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem). Arbital was much more ambitious in both goal and execution attempt, as you can see from reading the linked postmortem.

Continuing with Greg:

```markdown
Then, there is the “bandwidth” problem of human communication. The fastest way to convey a
mathematical concept or trick is to present it to your colleague on the blackboard. It takes 
very little effort on your part because a lot of information per time is being exchanged: mistakes,
mnemonics, steps are weighted by difficulty and importance, questions can be asked, etc. etc. 
In contrast, the amount of information that a written article can transfer is very small and it 
takes a lot of effort and skill to maximize its utility.
```

Anytime mathematical communication is brought up I’m always reminded of Bill Thurston’s essay [On proof and progress in mathematics](https://arxiv.org/abs/math/9404236), in particular this quote:

```markdown
Much of the difficulty has to do with the language and culture of mathematics, which is divided 
into subfields. Basic concepts used every day within one subfield are often foreign to another 
subfield. Mathematicians give up on trying to understand the basic concepts even from neighboring 
subfields, unless they were clued in as graduate students.

In contrast, communication works very well within the subfields of mathematics. Within a subfield,
people develop a body of common knowledge and known techniques. By informal contact, people learn 
to understand and copy each other’s ways of thinking, so that ideas can be explained clearly and easily.

Mathematical knowledge can be transmitted amazingly fast within a subfield. When a significant theorem
is proved, it often (but not always) happens that the solution can be communicated in a matter of 
minutes from one person to another within the subfield. The same proof would be communicated and
generally understood in an hour talk to members of the subfield. It would be the subject of a 15- or
20-page paper, which could be read and understood in a few hours or perhaps days by members of the 
subfield.

Why is there such a big expansion from the informal discussion to the talk to the paper? One-on-one,
people use wide channels of communication that go far beyond formal mathematical language. They use 
gestures, they draw pictures and diagrams, they make sound effects and use body language. Communication
is more likely to be two-way, so that people can concentrate on what needs the most attention. With 
these channels of communication, they are in a much better position to convey what’s going on, not
just in their logical and linguistic facilities, but in their other mental facilities as well.

In talks, people are more inhibited and more formal. Mathematical audiences are often not very good at
asking the questions that are on most people’s minds, and speakers often have an unrealistic preset 
outline that inhibits them from addressing questions even when they are asked. In papers, people are
still more formal. Writers translate their ideas into symbols and logic, and readers try to translate 
back.

People familiar with ways of doing things in a subfield recognize various patterns of statements or 
formulas as idioms or circumlocution for certain concepts or mental images. But to people not already 
familiar with what’s going on the same patterns are not very illuminating; they are often even 
misleading. The language is not alive except to those who use it.
```

Back to Greg:

```markdown
Related is the question of rewards. It’s rewarding to publish a paper (career) or book (fame), and 
it’s rewarding to talk to other people (see MathOverflow). There is no reward associated with a tricki
article, also because the appreciation by a reader happens at a different time than when the article 
is written.

However, I think there is room for the Tricki as a “MathOverflow for intuition questions”. The idea is
that a Tricki article could be initiated as a discussion between people and then transformed into a
proper article by volunteers. By the way, this is how the community wiki feature of StackOverflow/
MathOverflow was originally supposed to work. I think it’s still possible to do that, albeit with a 
software that encourages the corresponding behavior better than MathOverlfow currently does.

Again, note that building such software and community is a very hardproblem! A simple wiki software
probably won’t cut it, simply because it doesn’t encourage the right behavior. To get an impression of
its hardness, remember that Wikipedia’s success could not have been planned, it was just the fittest to
survive, and that MathOverflow didn’t get the community wiki thing right either. This is not intended to
discourage, I’d just like to point out that a successful Tricki probably needs more cleverness than is 
apparent at first.
```

Xamuel notes that “you almost have to already know what trick you need before you can search for it”:

```markdown
Take for example the problem “Prove every vector space has a basis”. If you don’t already know how
to do it, you aren’t likely to know to search for keywords like “make something maximal”, “extend
something one thing at a time”, much less “Zorn’s Lemma”.

If I were designing the Tricki, I would set it up like this. There would be articles for individual
problems, and articles about techniques. The former would link to the latter. I would then fill it up
with as many “stock” problems (mostly from undergrad math) as I could. Essentially it would be like 
[Example Problems](http://exampleproblems.com/), except, instead of just giving the solutions, it would link to the pages for the 
techniques used in the solution. As the number of problems grew, this would be much more efficient
than ultimately writing the same techniques over and over and over (which is what you’d do if you took
an Example Problems site to its logical conclusion).

So for example it would have a page “Prove every vector space has a basis” which would in turn link to
“How to use Zorn’s Lemma” as well as maybe “Partially ordering sets by inclusion”, and any other 
technique pages that might be relevant.
```

Terry Tao notes that MO takes advantage of [Metcalfe’s law](http://en.wikipedia.org/wiki/Metcalfe's_law), whereas the Tricki doesn’t:

```markdown
If N people ask questions and N people read [MO] to answer questions, there are O(N^2) opportunities 
for a question to be answered.

In contrast, to write a Tricki article, one of N people must come up with the idea and then write at 
least a stub article, and so there are only about O(N) opportunities for such an article to be created.
```

Gowers notes that MO can help Tricki:

```markdown
One of the difficulties in writing a Tricki article is that it can be hard to think of examples. 
For example (!) I know that the trick of making something easier by generalizing it is frequently
useful. But if I try to think of a good example my mind goes a bit blank, which is part of the reason
I have not written such an article.

However, the job has just got easier: if I post a big-list question on MO asking for good examples of
this technique, I can be pretty sure of getting some excellent ones. Then I can base an article on
people’s answers and include a link to the MO discussion (partly to credit people with their help, and
partly because the discussion will be different and will therefore offer more than the Tricki article
on its own).
```

Vipul Naik contends that what the Tricki needed was at least a handful of super-passionate contributors, probably in the vein of [r/slatestarcodex - Most of What You Read on the Internet is Written by Insane People](https://www.reddit.com/r/slatestarcodex/comments/9rvroo/most_of_what_you_read_on_the_internet_is_written/):

```markdown
I think there’s a general perception here that what needs to be done is get a large number of 
contributors. But I think that at this stage, if the Tricki needs to grow, it needs just 2-5 very 
passionate contributors, who agitate day and night about continually improving the organisation, 
adding new articles, etc. Large numbers of diffused contributions could take over at a later stage,
but they won’t do the trick at this stage.

For instance, the psychology wiki here: http://psychology.wikia.com/wiki... was written almost 
completely by one person at least for the first few years — now there are contributions from large
numbers of people.

The story is a little different with something like Math Overflow, because it is not primarily an 
information corpus as a place to ask and answer questions. But even there, it is the dedicated few 
who monitor the site who made sure that it took off. The need for that “dedicated few” in the case 
of the Tricki would be substantially more.
```

Commenter probable trickipedian mentions non-academic mathematicians as potential contributors, due to their specialized domain knowledge:

```markdown
One likely source of Tricki contributors – not for deep mathematics, but useful applied math “tricks”
– would be people who use mathematics for our research, but are not necessarily academic mathematicians.
For instance, I have (over the years) built a collection of definite integrals not found in standard 
“tables of integrals”; because they occur in (my) somewhat obscure research area, they are generally 
unknown outside it.

I (obviously) cannot publish such work in the journals of my discipline, and the techniques are too 
classical and elementary to constitute modern mathematics research, but it would be nice if I could 
post them on a well-trafficked public venue and someone else found it useful. I could supply (weak) 
proofs or justifications for them and have verified them to the best of my abilities, though the proofs
would certainly not meet the standards of rigor required by pure mathematicians. If a mathematician
could ‘vet’ them, I would be pleased to get a login and contribute.
```

Phi. Isett argues that the Tricki could be enhanced by integrating it with online, publicly-written math books:

```markdown
For example, usually a textbook will use a well-known and common trick at some stage in a proof, but 
will rarely have the opportunity to give much insight into the trick itself, its origins, its limits,
alternative methods, etc. All too often, the first time you see a trick, it is also much more complicated
than the most basic example of its use, which makes it harder to digest. Imagine you learned the Fourier
transform before you ever diagonalized a matrix… You might not know why it has certain magical properties
when it comes to translations and differentiation — in any event, it’s useful to understand that 
diagonalizing commuting operators is a truly general trick.

Often the trick appears as an ingenious technical detail in a proof. For example: the proof of Sard’s 
theorem uses this trick you might call “decomposing the total change into small changes”. ...You may 
recognize this trick from a proof of the Fundamental Theorem of Calculus, but maybe you didn’t see it
coming. Or you might find it enlightening to see the same trick in other contexts and in a much more 
general light. If you can link to the Tricki, then the individual techniques can be explored separately
and generally without disturbing the flow of the exposition.
```

<a name="#solving-famous-open-problems"></a>
### Solving famous open problems
([overview](#overview))

(See also Scott Aaronson on [making progress on big problems](#general-philosophy) in philosophy.)

From Terry Tao's post [Be sceptical of your own work](https://terrytao.wordpress.com/career-advice/be-sceptical-of-your-own-work/):

```markdown
Actual solutions to a major problem tend to be arrived at by a process more like the following (often 
involving several mathematicians over a period of years or decades, with many of the intermediate steps 
described here being significant publishable papers in their own right):

1. Isolate a toy model case x of major problem X.
2. Solve model case x using method A.
3. Try using method A to solve the full problem X.
4. This does not succeed, but method A can be extended to handle a few more model cases of X, such as 
x’ and x”.
5. Eventually, it is realised that method A relies crucially on a property P being true; this property is 
known for x, x’, and x”, thus explaining the current progress so far.
6. Conjecture that P is true for all instances of problem X.
7. Discover a family of counterexamples y, y’, y”, … to this conjecture. This shows that either method A 
has to be adapted to avoid reliance on P, or that a new method is needed.
8. Take the simplest counterexample y in this family, and try to prove X for this special case. Meanwhile, 
try to see whether method A can work in the absence of P.
9. Discover several counterexamples in which method A fails, in which the cause of failure can be 
definitively traced back to P. Abandon efforts to modify method A.
10. Realise that special case y is related to (or at least analogous to) a problem z in another field 
of mathematics. Look up the literature on z, and ask experts in that field for the latest perspectives on 
that problem.
11. Learn that z has been successfully attacked in that field by use of method B. Attempt to adapt method 
B to solve y.
12. After much effort, an adapted method B’ is developed to solve y.
13. Repeat the above steps 1-12 with A replaced by B’ (the outcome will of course probably be a little different 
from the sample storyline presented above). Continue doing this for a few years, until all model special cases 
can be solved by one method or another.
14. Eventually, one possesses an array of methods that can give partial results on X, each of having their 
strengths and weaknesses. Considerable intuition is gained as to the circumstances in which a given method 
is likely to yield something non-trivial or not.
15. Begin combining the methods together, simplifying the execution of these methods, locating new model 
problems, and/or finding a unified and clarifying framework in which many previous methods, insights, 
results, etc. become special cases.
16. Eventually, one realises that there is a family of methods A^* (of which A was the first to be discovered) 
which, roughly speaking, can handle all cases in which property P^* (a modern generalisation of property P) 
occurs. There is also a rather different family of methods B^* which can handle all cases in which Q^* occurs.
17. From all the prior work on this problem, all known model examples are known to obey either P^* or Q^*. 
Formulate Conjecture C: all cases of problem X obey either P^* or Q*^.
18. Verify that Conjecture C in fact implies the problem. This is a major reduction!
19. Repeat steps 1-18, but with problem X replaced by Conjecture C. (Again, the storyline may be different from 
that presented above.) This procedure itself may iterate a few times.
20. Finally, the problem has been boiled down to its most purified essence: a key conjecture K which (morally, 
at least) provides the decisive input into the known methods A^*, B^*, etc. which will settle conjecture C 
and hence problem X.
21. A breakthrough: a new method Z is introduced to solve an important special case of K.
22. The endgame: method Z is rapidly developed and extended, using the full power of all the intuition, experience, 
and past results, to fully settle K, then C, and then at last X.
23. The technology developed to solve major problem X is adapted to solve other related problems in the 
field. But now a natural successor question X’ to X arises, which lies just outside of the reach of the 
newly developed tools… and we go back to Step 1.
```

Greg Kuperberg stresses Terry’s point regarding “personal fallibility in the face of open problems”:

```markdown
If you think that you have solved an open problem, you should keep trying to understand it better for a 
while, because there is a good chance that you will find a mistake. This is still good advice even if it’s 
a run-of-the-mill open problem, not an incredibly hard open problem like Fermat’s Last Theorem.

Here is the sort of intuitive fallacy that can come up in a geometry argument: “If I lengthen all three 
sides of a triangle, its area will go up.” It sounds believable, and it even works in a few examples — but 
it’s not always true for obtuse triangles. Of course if you view this example in isolation, it seems 
boneheaded. But when working on an open problem, most people have to make intuitive leaps of this sort. If 
you think that you have a new result, that’s a great time to go back and debug your intuition at each step.

Terry also makes the more specific point that you should know the difference between climbing the mountain 
and marching around it. Restating a problem several times, or reducing it to another problem, is actually 
very good practice. But solving an open problem usually takes more than that. A few problems can be solved 
that way. Also, a few people have at times brilliantly transformed a broad set of problems by generalizing 
and restating them — this was Grothendieck’s work philosophy. But a more common outcome is that if you
restate a question too many times, eventually you will make it “easier” by restating it erroneously.

An example problem that can be solved almost entirely with restatement: Counting problems where the answer 
is the Catalan numbers, for example the number of lists of 2n balanced parentheses. However, this is only
medium-hard as counting problems go, and the restatements have to be somewhat clever to work.

An example where restatements are not known to work by themselves: Counting alternating-sign matrices, or 
ASMs. There are a dozen or so interesting-looking encodings of alternating-sign matrices. You can spend ages
rummaging through them in search of a simple counting principle to count ASMs. In fact, restatements of the
ASM problem have been important, for instance the description of ASMs in terms of square ice. But even the
important restatements do not solve this problem; rather they only give you access to substantive methods 
such as the Yang-Baxter equation.
```

(Tangent: I half-remember a discrete math HW assignment, a counting problem, regarding the number of lists of balanced parentheses. Puzzled for hours over that one until I thought of viewing it from (yet) another angle, after which it became much more tractable, if not exactly trivial. Point of view, as Alan Kay will tell you, is worth 80 IQ points; in this case the restatement I found was probably worth more like 10, but that was enough.)

Qiaochu asks:

```markdown
I’m quite curious – can you give an example where that process actually occurred, preferably multiple 
times? I’m not really aware of any resources for finding out the full story behind how a conjecture – 
especially not a particularly well-known, but still important, one – was proven, so I (and I hope others 
as well) would find such an example quite valuable.

Really the only good example I know of is the writeup of the Polymath project!
```

Terry’s response:

```markdown
Dear Qiaochu,

Actually most solutions to major problems (e.g. Poincare conjecture, Fermat’s last theorem, etc.) have 
this sort of history – a painstaking series of explorations, conjectures, setbacks, steady accumulation
of intuition and insights, realisation of the naivete of earlier approaches, etc. It is indeed difficult
though to find a coherent narrative for any given one of these problems (except perhaps at conference 
talks, or an advanced survey article); usually the story is spread throughout a dozen papers, and one has
to have a fair amount of familiarity with the problem to really appreciate the unfolding of progress.

For some problems, though, there are some good surveys that focus on exactly this sort of narrative, e.g.
Goldston-Pintz-Yildirim’s “The path to recent progress on small gaps between primes”. Milnor’s Clay article 
on the Poincare conjecture covers the topological phase of the attacks on that problem fairly well, and 
the important discovery of the geometrisation conjecture, though it is rather light on the Ricci flow approach 
which is of course at the heart of Perelman’s solution (but it is fair to say that this geometric approach 
would only have been seriously attempted after it became abundantly clear that the topological methods had 
fallen quite short of what was needed to settle the conjecture). Morgan’s BAMS survey article on the subject 
covers the latter quite thoroughly, though the focus is more on the technical details than on the development
of ideas. Finally, I discuss the narrative surrounding Szemeredi’s theorem in my article “What is good 
mathematics?” (linked to on the sidebar of this blog).
```

<a name="#mathematical-maturity"></a>
### Mathematical maturity
([overview](#overview))

From Redditor [man_after_midnight](https://www.reddit.com/user/man_after_midnight), answering Mathematicians of Reddit: [Is there some point where you see a big picture and everything "clicks"?](https://www.reddit.com/r/math/comments/1mtian/mathematicians_of_reddit_is_there_some_point/):

```markdown
The way it was described to me when I was in high school was in terms of 'levels'.

Sometimes, in your mathematics career, you find that your slow progress, and careful accumulation of tools 
and ideas, has suddenly allowed you to do a bunch of new things that you couldn't possibly do before. Even 
though you were learning things that were useless by themselves, when they've all become second nature, a 
whole new world of possibility appears. You have "leveled up", if you will. Something clicks, but now there 
are new challenges, and now, things you were barely able to think about before suddenly become critically 
important.

It's usually obvious when you're talking to somebody a level above you, because they see lots of things 
instantly when those things take considerable work for you to figure out. These are good people to learn 
from, because they remember what it's like to struggle in the place where you're struggling, but the things 
they do still make sense from your perspective (you just couldn't do them yourself).

Talking to somebody two or levels above you is a different story. They're barely speaking the same language, 
and it's almost impossible to imagine that you could ever know what they know. You can still learn from them, 
if you don't get discouraged, but the things they want to teach you seem really philosophical, and you don't 
think they'll help you—but for some reason, they do.

Somebody three levels above is actually speaking a different language. They probably seem less impressive to 
you than the person two levels above, because most of what they're thinking about is completely invisible to 
you. From where you are, it is not possible to imagine what they think about, or why. You might think you can, 
but this is only because they know how to tell entertaining stories. Any one of these stories probably contains 
enough wisdom to get you halfway to your next level if you put in enough time thinking about it.

What follows is my rough opinion on how this looks in a typical path towards a Ph.D. in math. Obviously this 
is rather subjective, and makes math look too linear, but I think it's a useful thought experiment.

Consider the change that a person undergoes in first mastering elementary algebra. Let's say that that's one 
level. This student is now comfortable with algebraic manipulation and the idea of variables.

The next level may come somewhere during a first calculus course. The student now understands the concept of 
the infinitely small, of slope at a point, and can reason about areas, physical motion, and optimization.

Many stop here, believing that they have finally learned math. Those who do not stop, might proceed through 
multivariable calculus and perhaps a basic linear algebra course with the tools they currently possess. Their 
next level comes when they find themselves suffering through an abstract algebra course, and have to once 
again reshape their whole thought process just to squeak by with a C.

Once this student masters all of that, the rest of the undergraduate curriculum at their university might be a 
breeze. But not so with graduate school. They gain a level their first year. They gain another their third year. 
And they are horrified to discover that they are expected to gain a third level before they graduate. This 
level is the hardest of them all, because it is the first one that consists in mastering material that has been 
created largely by the student.

I don't know how many levels there are after that. At least three.

So, the bad news is, you never do see the whole picture (though you see the old picture shrink down to a tiny 
point), and you can't really explain what you do see. But the good news is that the world of mathematics is so 
rich and exciting and wonderful that even your wildest dreams about it cannot possibly compare. It is not like 
seeing the Matrix—it is like seeing the Matrix within the Matrix within the Matrix within the Matrix within the
Matrix.
```

John Baez shared it on Google+ and [added](https://plus.google.com/+johncbaez999/posts/36g9jx54nni):

```markdown
As he points out, this talk of 'levels' is too linear. You can be much better at algebraic geometry than 
your friend, but way behind them in probability theory. Or even within a field like algebraic geometry, 
you might be able to understand sheaf cohomology better than your friend, yet still way behind in some 
classical topic like elliptic curves.

To have worthwhile conversations with someone who is not evenly matched with you in some subject, it's 
often good for one of you to play 'student' while the other plays 'teacher'. Playing teacher is an ego 
boost, and it helps organize your thoughts - but playing student is a great way to amass knowledge and 
practice humility... and a good student can help the teacher think about things in new ways.

Taking turns between who is teacher and who is student helps keep things from becoming unbalanced. And it's 
especially fun when some subject can only be understood with the combined knowledge of both players.

I have a feeling good mathematicians spend a lot of time playing these games - we often hear of famous 
teams like Atiyah, Bott and Singer, or even bigger ones like the French collective called 'Bourbaki'. For 
about a decade, I played teacher/student games with James Dolan, and it was really productive. I should 
probably find a new partner to learn the new kinds of math I'm working on now. Trying to learn things by 
yourself is a huge disadvantage if you want to quickly rise to higher 'levels'.
```

I also found Baez’s comment on how this relates to his experience ‘preaching’ category theory to ‘the masses’ interesting:

```markdown
To me category theory seems like a great branch of math for seeing how people climb up levels. Perhaps 
this because I've studied it a bunch and know a lot of people who think about it. But perhaps it's also 
because category theory is extremely conceptual, so lots of things of things seem obvious when you 
understand them, with no need for calculation to prove them - just a verbal argument will do - while the 
same verbal arguments seem cryptic and baffling before you understand them.

The point is that one needs to really understand the concepts used in these verbal arguments. One puts 
in a lot of sweat to understand them, but after one has, one can often use them with an ease that seems 
magical to people at lower levels. This is probably why lots of people get annoyed with category 
theorists, calling them 'too abstract'.

But it's also good to remember that category theory is just one portion of math, so people at a high 
level in category theory can still be embarrassingly clueless and awkward in other branches of math.
```

And to Toby Bartels’ suggestion that these ‘levels’ consist mostly of increasing abstraction:

```markdown
There are importantly different levels of competence in doing mathematics other than levels of 
abstraction: for example, levels of skill in devising proofs, levels of skill in inventing research 
program, and levels of understanding the 'big picture' of mathematics: what the big problems are and 
why they're interesting.

I often run into freshly-minted category theory postdocs who initially impress me with their ability 
to throw around (infinity,1)-categories, stacks, topoi and the like... but then amuse me because they 
don't know enough math to apply these concepts in powerful ways, or even understand where they fit into 
the grand scheme of things. They just haven't had time to learn the logic, algebra, topology, geometry, 
analysis, physics, and so on that these abstract concepts were designed to help us with. So they may 
seem superficially to be at a 'high level', but in a lot of ways they're not. (I don't think 'level' is 
quite the right word for what I mean, since it's too linear.)
```

Gary Ray R on how it pertains to engineering:

```markdown
I think the idea of levels also applies to Mechanical Engineering. I remember really having difficulty 
with Thermodynamics. I spent hours and hours on the homework, and never though I got any better, I was 
so happy with a B-. At the end of the course the instructor asked if I wanted to be his TA and grader 
for the next year. I was flummoxed, me, I just barely got this stuff. He said, you can do it. Well the 
next year I graded Thermo and that was one of the very hardest things in my engineering studies. But I 
jumped a level and did not really realize it.

Later when I went back for grad school in Materials Engineering, the Thermo seemed so easy, I was surprised 
at what I was so hard now seemed so clear. I could explain almost anything with Gibbs Free Energy, and 
the math was not that hard. But the concept took a while to sink in.
```

Michael Nielsen has [a great essay](http://cognitivemedium.com/srs-mathematics) talking about how he used spaced repetition via Anki flashcards to iteratively deepen his understanding of ("see through") a math concept. See also [Nielsen on augmenting long-term memory](#augmenting-long-term-memory). 

First -- on understanding in math being not black and white but layered / spectrum:

```markdown
What does it mean to understand a piece of mathematics? Naively, we perhaps think of this 
in relatively black and white terms: initially you don’t understand a piece of mathematics,
then you go through a brief grey period where you’re learning it, and with some luck and 
hard work you emerge out the other side “understanding” the mathematics.

In reality, mathematical understanding is much more nuanced. My experience is that it’s
nearly always possible to deepen one’s understanding of any piece of mathematics. This is 
even true – perhaps especially true – of what appear to be very simple mathematical ideas.

I first really appreciated this after reading an essay by the mathematician Andrey Kolmogorov.
You might suppose a great mathematician such as Kolmogorov would be writing about some very 
complicated piece of mathematics, but his subject was the humble equals sign: what made it a
good piece of notation, and what its deficiencies were. Kolmogorov discussed this in loving 
detail, and made many beautiful points along the way, e.g., that the invention of the equals
sign helped make possible notions such as equations (and algebraic manipulations of equations).

Prior to reading the essay I thought I understood the equals sign. Indeed, I would have been
offended by the suggestion that I did not. But the essay showed convincingly that I could 
understand the equals sign much more deeply.

This experience suggested three broader points. First, it’s possible to understand other 
pieces of mathematics far more deeply than I assumed. Second, mathematical understanding 
is an open-ended process; it’s nearly always possible to go deeper. Third, even great
mathematicians – perhaps, especially, great mathematicians – thought it worth their time
to engage in such deepening.
```

Nielsen being Nielsen, he gets excited about how to make iterative deepening of mathematical understanding actionable via heuristics. He's collected "many such heuristics over the years"; his essay talks about Anki. 

Does this really depend on Anki though? Not really:

```markdown
There’s very little in the above process that explicitly depended on me using Anki’s spaced-
repetition flashcards. Rather, what I’ve described is a general process for pulling apart 
the proof of a theorem and making much more sense of it, essentially by atomizing the 
elements. There’s no direct connection to Anki at all – you could carry out the process 
using paper and pencil.
```

Here's the "Ankification" of the proof of the following theorem: "a complex normal matrix is always diagonalizable by a unitary matrix".

```markdown
*Phase I: understanding the proof*: This involves multiple passes over the proof. Initially,
it starts out with what I think of as grazing, picking out single elements of the proof and 
converting to Anki cards. ... 

I work hard to restate ideas in multiple ways. Indeed, I worked hard to simplify both questions
and answers – the just given question-and-answer pair started out somewhat more complicated.
Part of this was some minor complexity in the question, which I gradually trimmed down. The 
answer I’ve stated above, though, is much better than in earlier versions. Earlier versions
mentioned M explicitly (unnecessary), had more blocks in the matrices, used ⋯⋯ rather than 
⋅⋅, and so on. You want to aim for the minimal answer, displaying the core idea as sharply as 
possible. 

I can’t emphasize enough the value of finding multiple different ways of thinking about the
“same” mathematical ideas. Here’s a couple more related restatements:

*Q: What’s a geometric interpretation of the diagonal entries in the matrix MM†?

A: The lengths squared of the respective rows.

Q: What’s a geometric interpretation of the diagonal entries in the matrix M†M?

A: The lengths squared of the respective columns.

Q: What do the diagonal elements of the normalcy condition MM†=M†M mean geometrically?

A: The corresponding row and column lengths are the same.*

What you’re trying to do at this stage is learn your way around the proof. Every piece
should become a comfortable part of your mental furniture, ideally something you start to
really feel. That means understanding every idea in multiple ways, and finding as many 
connections between different ideas as possible.

People inexperienced at mathematics sometimes memorize proofs as linear lists of statements.
A more useful way is to think of proofs is as interconnected networks of simple observations.
Things are rarely true for just one reason; finding multiple explanations for things gives 
you an improved understanding. This is in some sense “inefficient”, but it’s also a way of
deepening understanding and improving intuition. You’re building out the network of the proof,
making more connections between nodes.

One way of doing this is to explore minor variations. ...

(By the way, it’s questions like these that make me think it helps to be fairly 
mathematically experienced in carrying this Ankification process out. For someone who has
done a lot of linear algebra these are very natural observations to make, and questions to
ask. But I’m not sure they would be so natural for everyone. The ability to ask the “right”
questions – insight-generating questions – is a limiting part of this whole process, and
requires some experience.)

I’ve been describing the grazing process, aiming to thoroughly familiarize yourself with
every element of the proof. This is useful, but is also a rather undirected process, with 
no clear end point, and not necessarily helping you understand the broader to structure of
the proof. I also impose on myself a set of aspirational goals, all variations on the idea 
of distilling the entire proof to one question and (simple) answer. The aim is to fill in 
the answers to questions having forms like:

*Q: In one sentence, what is the core reason a (complex) normal matrix is diagonalizable?

And:

Q: What is a simple visual representation of the proof that (complex) normal matrices are
diagonalizable?*

I think of these question templates as boundary conditions or forcing functions. They’re 
things to aim for, and I try to write questions that will help me move toward answers. That 
starts with grazing, but over time moves to more structural questions about the proof, and
about how elements fit together.

In general, it’s helpful to make both questions and answers as atomic as possible; it seems
to help build clarity. That atomicity doesn’t mean the questions and answers can’t involve 
quite sophisticated concepts, but they ideally express a single idea.

In practice, as I understand the proof better and better the aspirational goal cards change
their nature somewhat.

What you really want is to feel every element (and the connections between them) in your 
bones. Some substantial part of that feeling comes by actually constructing the cards. That’s
a feeling you can’t get merely by reading an essay, it can only be experienced by going
through the deep Ankification process yourself. Nonetheless, I find that process, as described
up to now, is also not quite enough. You can improve upon it by asking further questions
elaborating on different parts of the answer, with the intent of helping you understand the
answer better.

Another helpful trick is to have multiple ways of writing these top-level questions. Much of 
my thinking is non-verbal (especially in subjects I’m knowledgeable about), but I still find
it useful to force a verbal question-and-answer...

*Phase II: variations, pushing the boundaries*: Let’s get back to details of how the
Ankification process works. One way of deepening your understanding further is to find ways
of pushing the boundaries of the proof and of the theorem. I find it helpful to consider many
different ways of changing the assumptions of the theorem, and to ask how it breaks down (or
generalizes). Another good strategy is to ask if the conditions can be weakened. ...

This second phase really is open-ended: we can keep putting in variations essentially ad 
infinitum. The questions are no longer directly about the proof, but rather are about poking
it in various ways, and seeing what happens. The further I go, and the more I connect to other
results, the better.
```

"Exhaust" cards:

```markdown
As described, this deep Ankification process can feel rather wasteful. Inevitably, over 
time my understanding of the proof changes. When that happens it’s often useful to rewrite
(and sometimes discard or replace) cards to reflect my improved understanding. And some of
the cards written along the way have the flavor of exhaust, bad cards that seem to be
necessary to get to good cards. I wish I had a good way of characterizing these, but I 
haven’t gone through this often enough to have more than fuzzy ideas about it.
```

How it feels like to "be inside a piece of mathematics":

```markdown
Typically, my mathematical work begins with paper-and-pen and messing about, often in
a rather ad hoc way. But over time if I really get into something my thinking starts to
change. I gradually internalize the mathematical objects I’m dealing with. It becomes 
easier and easier to conduct (most of) my work in my head. I will go on long walks, and
simply think intensively about the objects of concern. Those are no longer symbolic or 
verbal or visual in the conventional way, though they have some secondary aspects of 
this nature. Rather, the sense is somehow of working directly with the objects of 
concern, without any direct symbolic or verbal or visual referents. Furthermore, as my 
understanding of the objects change – as I learn more about their nature, and correct my
own misconceptions – my sense of what I can do with the objects changes as well. It’s as
though they sprout new affordances, in the language of user interface design, and I get 
much practice in learning to fluidly apply those affordances in multiple ways.

This is a very difficult experience to describe in a way that I’m confident others will
understand, but it really is central to my experience of mathematics – at least, of 
mathematics that I understand well. I must admit I’ve shared it with some trepidation; 
it seems to be rather unusual for someone to describe their inner mathematical experiences
in these terms (or, more broadly, in the terms used in this essay).
```

Related is Einstein's musings in his letter to Hadamard describing his thought processes:

```markdown
The words or the language, as they are written or spoken, do not seem to play any role 
in my mechanism of thought. The psychical entities which seem to serve as elements in 
thought are certain signs and more or less clear images which can be “voluntarily” 
reproduced and combined… The above-mentioned elements are, in my case, of visual and some
of muscular type. Conventional words or other signs have to be sought for laboriously 
only in a secondary stage, when the mentioned associative play is sufficiently established
and can be reproduced at will.
```

Nielsen thinks this is all chunking:

```markdown
People who intensively study a subject gradually start to build mental libraries of “chunks” 
– large-scale patterns that they recognize and use to reason. This is why some grandmaster 
chess players can remember thousands of games move for move. They’re not remembering the 
individual moves – they’re remembering the ideas those games express, in terms of larger 
patterns. And they’ve studied chess so much that those ideas and patterns are deeply
meaningful, much as the phrases in a lover’s letter may be meaningful. It’s why top 
basketball players have extraordinary recall of games. Experts begin to think, perhaps only
semi-consciously, using such chunks. The conventional representations – words or symbols in
mathematics, or moves on a chessboard – are still there, but they are somehow secondary.
```

This segues into Terry Tao's [three stages of mathematical education](https://terrytao.wordpress.com/career-advice/theres-more-to-mathematics-than-rigour-and-proofs/) article:

```markdown
One can roughly divide mathematical education into three stages:

1. The “pre-rigorous” stage, in which mathematics is taught in an informal, intuitive 
manner, based on examples, fuzzy notions, and hand-waving. (For instance, calculus is 
usually first introduced in terms of slopes, areas, rates of change, and so forth.) 
The emphasis is more on computation than on theory. This stage generally lasts until 
the early undergraduate years.

2. The “rigorous” stage, in which one is now taught that in order to do maths “properly”,
one needs to work and think in a much more precise and formal manner (e.g. re-doing calculus
by using epsilons and deltas all over the place). The emphasis is now primarily on theory;
and one is expected to be able to comfortably manipulate abstract mathematical objects
without focusing too much on what such objects actually “mean”. This stage usually occupies 
the later undergraduate and early graduate years.

3. The “post-rigorous” stage, in which one has grown comfortable with all the rigorous 
foundations of one’s chosen field, and is now ready to revisit and refine one’s pre-
rigorous intuition on the subject, but this time with the intuition solidly buttressed by 
rigorous theory. (For instance, in this stage one would be able to quickly and accurately 
perform computations in vector calculus by using analogies with scalar calculus, or informal
and semi-rigorous use of infinitesimals, big-O notation, and so forth, and be able to 
convert all such calculations into a rigorous argument whenever required.) The emphasis is 
now on applications, intuition, and the “big picture”. This stage usually occupies the late
graduate years and beyond.

The transition from the first stage to the second is well known to be rather traumatic, with
the dreaded “proof-type questions” being the bane of many a maths undergraduate. But the
transition from the second to the third is equally important, and should not be forgotten.

It is of course vitally important that you know how to think rigorously, as this gives you
the discipline to avoid many common errors and purge many misconceptions. Unfortunately, this
has the unintended consequence that “fuzzier” or “intuitive” thinking (such as heuristic
reasoning, judicious extrapolation from examples, or analogies with other contexts such as 
physics) gets deprecated as “non-rigorous”. All too often, one ends up discarding one’s
initial intuition and is only able to process mathematics at a formal level, thus getting 
stalled at the second stage of one’s mathematical education.  (Among other things, this can
impact one’s ability to read mathematical papers; an overly literal mindset can lead to 
“compilation errors” when one encounters even a single typo or ambiguity in such a paper.)

The point of rigour is not to destroy all intuition; instead, it should be used to destroy 
bad intuition while clarifying and elevating good intuition. It is only with a combination
of both rigorous formalism and good intuition that one can tackle complex mathematical
problems; one needs the former to correctly deal with the fine details, and the latter to 
correctly deal with the big picture. Without one or the other, you will spend a lot of time
blundering around in the dark (which can be instructive, but is highly inefficient). So once
you are fully comfortable with rigorous mathematical thinking, you should revisit your
intuitions on the subject and use your new thinking skills to test and refine these intuitions
rather than discard them. One way to do this is to ask yourself dumb questions; another is 
to relearn your field.

The ideal state to reach is when every heuristic argument naturally suggests its rigorous 
counterpart, and vice versa. Then you will be able to tackle maths problems by using both 
halves of your brain at once – i.e., the same way you already tackle problems in “real life”.
```

See [this article](https://terrytao.wordpress.com/advice-on-writing-papers/on-compilation-errors-in-mathematical-reading-and-how-to-resolve-them/) by Terry for more on compilation errors.

The kind of mistakes made by mathematicians of each stage:

```markdown
It is perhaps worth noting that mathematicians at all three of the above stages of 
mathematical development can still make formal mistakes in their mathematical writing.
However, the nature of these mistakes tends to be rather different, depending on what 
stage one is at:

1. Mathematicians at the pre-rigorous stage of development often make formal errors 
because they are unable to understand how the rigorous mathematical formalism actually 
works, and are instead applying formal rules or heuristics blindly.  It can often be 
quite difficult for such mathematicians to appreciate and correct these errors even 
when those errors are explicitly pointed out to them.

2. Mathematicians at the rigorous stage of development can still make formal errors
because they have not yet perfected their formal understanding, or are unable to perform
enough “sanity checks” against intuition or other rules of thumb to catch, say, a sign
error, or a failure to correctly verify a crucial hypothesis in a tool.  However, such 
errors can usually be detected (and often repaired) once they are pointed out to them.

3. Mathematicians at the post-rigorous stage of development are not infallible, and are
still capable of making formal errors in their writing.  But this is often because they
no longer need the formalism in order to perform high-level mathematical reasoning, and 
are actually proceeding largely through intuition, which is then translated (possibly 
incorrectly) into formal mathematical language.

The distinction between the three types of errors can lead to the phenomenon (which can
often be quite puzzling to readers at earlier stages of mathematical development) of a 
mathematical argument by a post-rigorous mathematician which locally contains a number
of typos and other formal errors, but is globally quite sound, with the local errors 
propagating for a while before being cancelled out by other local errors.  (In contrast,
when unchecked by a solid intuition, once an error is introduced in an argument by a pre-
rigorous or rigorous mathematician, it is possible for the error to propagate out of 
control until one is left with complete nonsense at the end of the argument.)
```

Commenter Chris makes the following claim:

```markdown
I’d call pre-rigorous the “cargo cult” stage. You’re not doing mathematics, you’re merely
performing a very close approximation to it using rote learned rules. It was this sort of
mathematics taught in the first year of the undergraduate curriculum at my university which 
caused me to take physics as my major.

Physicists and engineers call the third type of mathematics you propose a “back of the
envelope” calculation. I suspect the less pretentious mathematicians do also. It is the
step you use to flesh out a hypothesis before you apply rigour.
```

I only recorded it as context for Terry's response:

```markdown
Hmm. I think perhaps I would classify the “back of the envelope” calculations as a 
fourth stage, let’s call it the “heuristic” stage, in the following, almost commuting square:

pre-rigorous —> rigorous
| ……………………….|
v ……………………….v
heuristic —> post-rigorous

As I discussed in the post, mathematicians tend to proceed through the upper route, but I
do see the point that physicists and engineers tend to proceed through the lower route. 
Though, as I said, the diagram doesn’t quite commute; there are some significant cultural
differences in doing mathematics that depend on which route one took to achieve the post-
rigorous stage.

The distinction between heuristic and post-rigorous is that in the latter, one uses
intuition and rigour in an integrated fashion; one knows how to justify one’s intuition
and convert it to rigorous arguments, and conversely one knows how to take rigorous 
arguments and extract an intuitive explanation. For instance, one could convert arguments
involving infinitesimals into rigorous epsilon-delta arguments whenever required, and vice
versa. At the heuristic level, one could argue accurately with infinitesimals, but might 
not be able to convert them into a rigorous argument.

Just as mathematicians sometimes get stuck at the rigorous stage, unable to fully develop
their intuition, I would imagine that the converse problem can happen to people educated 
using the physicist/engineer approach, who then miss out on the stereoscopic view that one
gets from using both rigour and intuition simultaneously.
```

Going back to Nielsen's chunking above, there's also Bill Thurston's legendary MO question [Thinking and explaining](https://mathoverflow.net/questions/38639/thinking-and-explaining), whose favorite answers I'm quoting wholesale below.

Here's the [legendary answer by Anonymous Quoran](https://www.quora.com/What-is-it-like-to-understand-advanced-mathematics-Does-it-feel-analogous-to-having-mastery-of-another-language-like-in-programming-or-linguistics/answers/873950?amp&share=1&srid=p6KQ) I always have to much around in the Google search results to get at for some reason:

```markdown
**You can answer many seemingly difficult questions quickly**. But you are not very impressed
by what can look like magic, because you know the trick. The trick is that your brain can
quickly decide if a question is answerable by one of a few powerful general purpose "machines"
(e.g., continuity arguments, the correspondences between geometric and algebraic objects,
linear algebra, ways to reduce the infinite to the finite through various forms of
compactness) combined with specific facts you have learned about your area. The number of
fundamental ideas and techniques that people use to solve problems is, perhaps surprisingly,
pretty small -- see http://www.tricki.org/tricki/map for a partial list, maintained by Timothy
Gowers.

**You are often confident that something is true long before you have an airtight proof for it
(this happens especially often in geometry)**. The main reason is that you have a large 
catalogue of connections between concepts, and you can quickly intuit that if X were to be 
false, that would create tensions with other things you know to be true, so you are inclined 
to believe X is probably true to maintain the harmony of the conceptual space. It's not so much 
that you can imagine the situation perfectly, but you can quickly imagine many other things that
are logically connected to it.

**You are comfortable with feeling like you have no deep understanding of the problem you are 
studying**. Indeed, when you do have a deep understanding, you have solved the problem and it 
is time to do something else. This makes the total time you spend in life reveling in your mastery
of something quite brief. One of the main skills of research scientists of any type is knowing how
to work comfortably and productively in a state of confusion. More on this in the next few bullets.

**Your intuitive thinking about a problem is productive and usefully structured, wasting little 
time on being aimlessly puzzled**. For example, when answering a question about a high-dimensional
space (e.g., whether a certain kind of rotation of a five-dimensional object has a "fixed point"
which does not move during the rotation), you do not spend much time straining to visualize those 
things that do not have obvious analogues in two and three dimensions. (Violating this principle
is a huge source of frustration for beginning maths students who don't know that they shouldn't be
straining to visualize things for which they don't seem to have the visualizing machinery.) Instead...

**When trying to understand a new thing, you automatically focus on very simple examples that 
are easy to think about, and then you leverage intuition about the examples into more impressive
insights**. For example, you might imagine two- and three-dimensional rotations that are analogous
to the one you really care about, and think about whether they clearly do or don't have the 
desired property. Then you think about what was important to the examples and try to distill those
ideas into symbols. Often, you see that the key idea in the symbolic manipulations doesn't depend
on anything about two or three dimensions, and you know how to answer your hard question. 

**As you get more mathematically advanced, the examples you consider easy are actually complex 
insights built up from many easier examples; the "simple case" you think about now took you two
years to become comfortable with**. But at any given stage, you do not strain to obtain a magical
illumination about something intractable; you work to reduce it to the things that feel friendly.

To me, **the biggest misconception that non-mathematicians have about how mathematicians work is that 
there is some mysterious mental faculty that is used to crack a research problem all at once**. It's 
true that sometimes you can solve a problem by pattern-matching, where you see the standard tool 
that will work; the first bullet above is about that phenomenon. This is nice, but not fundamentally
more impressive than other confluences of memory and intuition that occur in normal life, as when
you remember a trick to use for hanging a picture frame or notice that you once saw a painting of 
the street you're now looking at.

In any case, by the time a problem gets to be a research problem, it's almost guaranteed that
simple pattern matching won't finish it. So in one's professional work, the process is piecemeal:
you think a few moves ahead, trying out possible attacks from your arsenal on simple examples 
relating to the problem, trying to establish partial results, or looking to make analogies with 
other ideas you understand. This is the same way that you solve difficult problems in your first
real maths courses in university and in competitions. What happens as you get more advanced is 
simply that the arsenal grows larger, the thinking gets somewhat faster due to practice, and you
have more examples to try. Sometimes, during this process, a sudden insight comes, but it would 
not be possible without the painstaking groundwork [ http://terrytao.wordpress.com/ca... ].

Indeed, most of the bullet points here summarize feelings familiar to many serious students of 
mathematics who are in the middle of their undergraduate careers; as you learn more mathematics,
these experiences apply to "bigger" things but have the same fundamental flavor.

**You go up in abstraction, "higher and higher"**. The main object of study yesterday becomes 
just an example or a tiny part of what you are considering today. For example, in calculus
classes you think about functions or curves. In functional analysis or algebraic geometry, you
think of spaces whose points are functions or curves -- that is, you "zoom out" so that every
function is just a point in a space, surrounded by many other "nearby" functions. Using this 
kind of zooming out technique, you can say very complex things in short sentences -- things 
that, if unpacked and said at the zoomed-in level, would take up pages. Abstracting and 
compressing in this way makes it possible to consider extremely complicated issues with one's 
limited memory and processing power.

**The particularly "abstract" or "technical" parts of many other subjects seem quite accessible
because they boil down to maths you already know. You generally feel confident about your ability
to learn most quantitative ideas and techniques**. A theoretical physicist friend likes to say, 
only partly in jest, that there should be books titled "______ for Mathematicians", where _____ 
is something generally believed to be difficult (quantum chemistry, general relativity, securities
pricing, formal epistemology). Those books would be short and pithy, because many key concepts in 
those subjects are ones that mathematicians are well equipped to understand. Often, those parts 
can be explained more briefly and elegantly than they usually are if the explanation can assume a
knowledge of maths and a facility with abstraction. 

Learning the domain-specific elements of a different field can still be hard -- for instance,
physical intuition and economic intuition seem to rely on tricks of the brain that are not 
learned through mathematical training alone. But the quantitative and logical techniques you 
sharpen as a mathematician allow you to take many shortcuts that make learning other fields
easier, as long as you are willing to be humble and modify those mathematical habits that are 
not useful in the new field.

**You move easily among multiple seemingly very different ways of representing a problem**. For
example, most problems and concepts have more algebraic representations (closer in spirit to an 
algorithm) and more geometric ones (closer in spirit to a picture). You go back and forth between
them naturally, using whichever one is more helpful at the moment. 

Indeed, some of the most powerful ideas in mathematics (e.g., duality, Galois theory, algebraic
geometry) provide "dictionaries" for moving between "worlds" in ways that, ex ante, are very 
surprising. For example, Galois theory allows us to use our understanding of symmetries of shapes
(e.g., rigid motions of an octagon) to understand why you can solve any fourth-degree polynomial
equation in closed form, but not any fifth-degree polynomial equation. Once you know these threads
between different parts of the universe, you can use them like wormholes to extricate yourself 
from a place where you would otherwise be stuck. The next two bullets expand on this.

**Spoiled by the power of your best tools, you tend to shy away from messy calculations or long,
case-by-case arguments unless they are absolutely unavoidable**. Mathematicians develop a powerful
attachment to elegance and depth, which are in tension with, if not directly opposed to, mechanical
calculation. Mathematicians will often spend days figuring out why a result follows easily from some 
very deep and general pattern that is already well-understood, rather than from a string of 
calculations. Indeed, you tend to choose problems motivated by how likely it is that there will be 
some "clean" insight in them, as opposed to a detailed but ultimately unenlightening proof by
exhaustively enumerating a bunch of possibilities. (Nevertheless, detailed calculation of an example 
is often a crucial part of beginning to see what is really going on in a problem; and, depending on
the field, some calculation often plays an essential role even in the best proof of a result.)

In A Mathematician's Apology [http://www.math.ualberta.ca/~mss..., the most poetic book I know on what it is "like" to be a mathematician], G.H. Hardy wrote:

"In both [these example] theorems (and in the theorems, of course, I include the proofs) there is a
very high degree of unexpectedness, combined with inevitability and  economy. The arguments take so
odd and surprising a form; the weapons used seem so childishly simple when compared with  the far-
reaching results; but there is no escape from the conclusions. There are no complications of detail—
one line of attack is enough in each case; and this is true too of the proofs of many much more 
difficult theorems, the full appreciation of which demands quite a high degree of technical
proficiency. We do not want many ‘variations’ in the proof of a mathematical theorem: ‘enumeration 
of cases’, indeed, is one of the duller forms of mathematical argument. A mathematical proof should
resemble a simple and clear-cut constellation, not a scattered cluster in the Milky Way."

[...]

"[A solution to a difficult chess problem] is quite genuine mathematics, and has its merits; but it
is just that ‘proof by enumeration of cases’ (and of cases which do not, at bottom, differ at all
profoundly) which a real mathematician tends to despise."

**You develop a strong aesthetic preference for powerful and general ideas that connect hundreds of
difficult questions, as opposed to resolutions of particular puzzles**. Mathematicians don't really
care about "the answer" to any particular question; even the most sought-after theorems, like
Fermat's Last Theorem, are only tantalizing because their difficulty tells us that we have to 
develop very good tools and understand very new things to have a shot at proving them. It is
what we get in the process, and not the answer per se, that is the valuable thing. The 
accomplishment a mathematician seeks is finding a new dictionary or wormhole between different 
parts of the conceptual universe. As a result, many mathematicians do not focus on deriving the 
practical or computational implications of their studies (which can be a drawback of the hyper-
abstract approach!); instead, they simply want to find the most powerful and general connections.
Timothy Gowers has some interesting comments on this issue, and disagreements within the 
mathematical community about it [ http://www.dpmms.cam.ac.uk/~wtg1... ].

**Understanding something abstract or proving that something is true becomes a task a lot like
building something**. You think: "First I will lay this foundation, then I will build this framework
using these familiar pieces, but leave the walls to fill in later, then I will test the beams..." 
All these steps have mathematical analogues, and structuring things in a modular way allows you to 
spend several days thinking about something you do not understand without feeling lost or frustrated.
(I should say, "without feeling unbearably lost and frustrated"; some amount of these feelings is 
inevitable, but the key is to reduce them to a tolearable degree.)

Andrew Wiles, who proved Fermat's Last Theorem, used an "exploring" metaphor:

"Perhaps I can best describe my experience of doing mathematics in terms of a journey through a
dark unexplored mansion. You enter the first room of the mansion and it's completely dark. You 
stumble around bumping into the furniture, but gradually you learn where each piece of furniture
is. Finally, after six months or so, you find the light switch, you turn it on, and suddenly it's
all illuminated. You can see exactly where you were. Then you move into the next room and spend 
another six months in the dark. So each of these breakthroughs, while sometimes they're momentary,
sometimes over a period of a day or two, they are the culmination of—and couldn't exist without—
the many months of stumbling around in the dark that proceed them." [ http://www.pbs.org/wgbh/nova/phy... ]

**In listening to a seminar or while reading a paper, you don't get stuck as much as you used to**
in youth because you are good at modularizing a conceptual space, taking certain calculations or
arguments you don't understand as "black boxes", and considering their implications anyway. You
can sometimes make statements you know are true and have good intuition for, without understanding
all the details. You can often detect where the delicate or interesting part of something is based
on only a very high-level explanation. (I first saw these phenomena highlighted by Ravi Vakil, who
offers insightful advice on being a mathematics student: http://math.stanford.edu/~vakil/... .)

**You are good at generating your own definitions and your own questions in thinking about some new
kind of abstraction**. One of the things one learns fairly late in a typical mathematical education 
(often only at the stage of starting to do research) is how to make good, useful definitions.
Something I've reliably heard from people who know parts of mathematics well but never went on to be
professional mathematicians (i.e., write articles about new mathematics for a living) is that they
were good at proving difficult propositions that were stated in a textbook exercise, but would be
lost if presented with a mathematical structure and asked to find and prove some interesting facts 
about it. Concretely, the ability to do this amounts to being good at making definitions and, using
the newly defined concepts, formulating precise results that other mathematicians find intriguing or
enlightening. 

This kind of challenge is like being given a world and asked to find events in it that come together 
to form a good detective story. You have to figure out who the characters should be (the concepts 
and objects you define) and what the interesting mystery might be. To do these things, you use 
analogies with other detective stories (mathematical theories) that you know and a taste for what
is surprising or deep. How this process works is perhaps the most difficult aspect of mathematical
work to describe precisely but also the thing that I would guess is the strongest thing that 
mathematicians have in common.

**You are easily annoyed by imprecision in talking about the quantitative or logical**. This is
mostly because you are trained to quickly think about counterexamples that make an imprecise claim
seem obviously false.

**On the other hand, you are very comfortable with intentional imprecision or "hand-waving" in areas
you know, because you know how to fill in the details**. Terence Tao is very eloquent about this here 
[ http://terrytao.wordpress.com/ca... ]: 

"[After learning to think rigorously, comes the] 'post-rigorous' stage, in which one has grown 
comfortable with all the rigorous foundations of one’s chosen field, and is now ready to revisit
and refine one’s pre-rigorous intuition on the subject, but this time with the intuition solidly
buttressed by rigorous theory. (For instance, in this stage one would be able to quickly and 
accurately perform computations in vector calculus by using analogies with scalar calculus, or
informal and semi-rigorous use of infinitesimals, big-O notation, and so forth, and be able to
convert all such calculations into a rigorous argument whenever required.) The emphasis is now 
on applications, intuition, and the 'big picture'. This stage usually occupies the late graduate
years and beyond."

In particular, an idea that took hours to understand correctly the first time ("for any arbitrarily
small epsilon I can find a small delta so that this statement is true") becomes such a basic element
of your later thinking that you don't give it conscious thought.

Before wrapping up, it is worth mentioning that mathematicians are not immune to the limitations 
faced by most others. They are not typically intellectual superheroes. For instance, they often 
become resistant to new ideas and uncomfortable with ways of thinking (even about mathematics) 
that are not their own. They can be defensive about intellectual turf, dismissive of others, or
petty in their disputes. Above, I have tried to summarize how the mathematical way of thinking 
feels and works at its best, without focusing on personality flaws of mathematicians or on the 
politics of various mathematical fields. These issues are worthy of their own long answers!

**You are humble about your knowledge because you are aware of how weak maths is, and you are comfortable
with the fact that you can say nothing intelligent about most problems**. There are only very few
mathematical questions to which we have reasonably insightful answers. There are even fewer questions,
obviously, to which any given mathematician can give a good answer. After two or three years of a 
standard university curriculum, a good maths undergraduate can effortlessly write down hundreds of
mathematical questions to which the very best mathematicians could not venture even a tentative answer.
(The theoretical computer scientist Richard Lipton lists some examples of potentially "deep" ignorance 
here: http://rjlipton.wordpress.com/20...) This makes it more comfortable to be stumped by most problems;
a sense that you know roughly what questions are tractable and which are currently far beyond our abilities
is humbling, but also frees you from being very intimidated, because you do know you are familiar with the
most powerful apparatus we have for dealing with these kinds of problems.
```

Ravi Vakil's [advice page](http://math.stanford.edu/~vakil/potentialstudents.html) for potential PhD students has some great quotes. The most memorable is this one:

```markdown
Here's a phenomenon I was surprised to find: you'll go to talks, and hear various words, whose 
definitions you're not so sure about. At some point you'll be able to make a sentence using those
words; you won't know what the words mean, but you'll know the sentence is correct. You'll also 
be able to ask a question using those words. You still won't know what the words mean, but you'll
know the question is interesting, and you'll want to know the answer. Then later on, you'll learn 
what the words mean more precisely, and your sense of how they fit together will make that learning
much easier. 

The reason for this phenomenon is that mathematics is so rich and infinite that it is impossible to 
learn it systematically, and if you wait to master one topic before moving on to the next, you'll 
never get anywhere. Instead, you'll have tendrils of knowledge extending far from your comfort zone. 
Then you can later backfill from these tendrils, and extend your comfort zone; this is much easier 
to do than learning "forwards". 

(Caution: this backfilling is necessary. There can be a temptation to learn lots of fancy words and
to use them in fancy sentences without being able to say precisely what you mean. You should feel 
free to do that, but you should always feel a pang of guilt when you do.)
```

<a name="#good-mathematics"></a>
### Good mathematics
([overview](#overview))

From Terry Tao's paper [What is good mathematics?](https://arxiv.org/pdf/math/0702396.pdf):

```markdown
There are many different types of mathematics which could be designated “good”. For instance, “good 
mathematics” could refer (in no particular order) to

(i) Good mathematical problem-solving (e.g. a major breakthrough on an important mathematical problem);
(ii) Good mathematical technique (e.g. a masterful use of existing methods, or the development of 
new tools);
(iii) Good mathematical theory (e.g. a conceptual framework or choice of notation which systematically 
unifies and generalises an existing body of results);
(iv) Good mathematical insight (e.g. a major conceptual simplification, or the realisation of a unifying
principle, heuristic, analogy, or theme);
(v) Good mathematical discovery (e.g. the revelation of an unexpected and intriguing new mathematical 
phenomenon, connection, or counterexample);
(vi) Good mathematical application (e.g. to important problems in physics, engineering, computer science,
statistics, etc., or from one field of mathematics to another);
(vii) Good mathematical exposition (e.g. a detailed and informative survey on a timely mathematical topic,
or a clear and well-motivated argument);
(viii) Good mathematical pedagogy (e.g. a lecture or writing style which enables others to learn and do
mathematics more effectively, or contributions to mathematical education);
(ix) Good mathematical vision (e.g. a long-range and fruitful program or set of conjectures);
(x) Good mathematical taste (e.g. a research goal which is inherently interesting and impacts important
topics, themes, or questions);
(xi) Good mathematical public relations (e.g. an effective showcasing of a mathematical achievement to 
non-mathematicians, or from one field of mathematics to another);
(xii) Good meta-mathematics (e.g. advances in the foundations, philosophy, history, scholarship, or
practice of mathematics);
(xiii) Rigorous mathematics (with all details correctly and carefully given in full);
(xiv) Beautiful mathematics (e.g. the amazing identities of Ramanujan; results which are easy (and pretty) 
to state but not to prove);
(xv) Elegant mathematics (e.g. Paul Erd˝os’ concept of “proofs from the Book”; achieving a difficult 
result with a minimum of effort);
(xvi) Creative mathematics (e.g. a radically new and original technique, viewpoint, or species of result);
(xvii) Useful mathematics (e.g. a lemma or method which will be used repeatedly in future work on the subject);
(xviii) Strong mathematics (e.g. a sharp result that matches the known counterexamples, or a result 
which deduces an unexpectedly strong conclusion from a seemingly weak hypothesis);
(xix) Deep mathematics (e.g. a result which is manifestly non-trivial, for instance by
capturing a subtle phenomenon beyond the reach of more elementary tools);
(xx) Intuitive mathematics (e.g. an argument which is natural and easily visualisable);
(xxi) Definitive mathematics (e.g. a classification of all objects of a certain type; the final word on
a mathematical topic); etc.

As the above list demonstrates, the concept of mathematical quality is a highdimensional one, and lacks an
obvious canonical total ordering. I believe this is because mathematics is itself complex and high-dimensional,
and evolves in unexpected and adaptive ways; each of the above qualities represents a different way in which we
as a community improve our understanding and usage of the subject. There does not appear to be universal
agreement as to the relative importance or weight of each of the above qualities. This is partly due to tactical
considerations: a field of mathematics at a given stage of development may be more receptive to one approach to 
mathematics than another. It is also partly due to cultural considerations: any given field or school of
mathematics tends to attract like-minded mathematicians who prefer similar approaches to a subject. It also 
reflects the diversity of mathematical ability; different mathematicians tend to excel in different mathematical 
styles, and are thus well suited for different types of mathematical challenges.
```

This multifaceted nature of "good math" is healthy for math as a whole, since desirable traits can be detrimental to a field if pursued at the expense of others, per the following (made up) scenarios:

```markdown
• A field which becomes increasingly ornate and baroque, in which individual
results are generalised and refined for their own sake, but the subject as a
whole drifts aimlessly without any definite direction or sense of progress;
• A field which becomes filled with many astounding conjectures, but with no
hope of rigorous progress on any of them;
• A field which now consists primarily of using ad hoc methods to solve a collection
of unrelated problems, which have no unifying theme, connections, or purpose;
• A field which has become overly dry and theoretical, continually recasting and
unifying previous results in increasingly technical formal frameworks, but not
generating any exciting new breakthroughs as a consequence; or
• A field which reveres classical results, and continually presents shorter, simpler,
and more elegant proofs of these results, but which does not generate any truly
original and new results beyond the classical literature.
```

<a name="#what-every-mathematician-should-know"></a>
### What every mathematician should know
([overview](#overview))

Timothy Chow is well worth reading. Here's his answer to the MO question [How has "what every mathematician should know" changed?](https://mathoverflow.net/questions/19356/how-has-what-every-mathematician-should-know-changed?rq=1):

```markdown
I believe that one shift is that "what every mathematician should know" is nowadays much less a 
specific body of mathematical facts and much more a facility with navigating the ocean of mathematical
knowledge.

For example, I might not need to have advanced computer programming skills, but I do need to have some
sense of what kinds of computations are feasible and when it is appropriate for me to do a computation.

I might not need to hold in my head everything that is known about a certain topic, even if that topic 
is close to my area of specialization, but I definitely need to have the ability to search the literature,
assess what is in a certain paper that my search turns up, and know when I should ask an expert and how 
to formulate a targeted question to ask.

Similarly, I might not need detailed knowledge of fields (seemingly) distant from my own, but I do need 
to be able to discern when those distant fields might provide relevant tools for my own work.

So far I have been focusing on what a mathematician needs to know in order to be an effective researcher.
However, the phrase "what every mathematician should know" carries overtones of what one should know if 
one wants to earn a reputation for being an educated, knowledgeable, respectable, and attractive 
representative of the profession. In my opinion this is quite a different question. For this, you need to
be fluent in the language of the hot topics du jour, and au courantwith flashy announcements of big
breakthroughs in all areas of mathematics. While there's some correlation between this kind of knowledge
and the knowledge I discussed above, I find it questionable whether, literally speaking, every 
mathematician should have it.
```

Here he echoes Barry Mazur, whose [writing on the subject](http://www.math.harvard.edu/~mazur/preprints/math_ed_2.pdf) is so beautiful and fulfilling I can't help but reproduce large parts of it wholesale here. 

Lingua franca, unifiers, ubiquitous:

```markdown
Certain fields of mathematics, at certain times, play the role of lingua franca in the sense that
mathematics from vastly different fields get formulated in the vocabulary, the terminology, or even 
more strikingly in the conceptual framework of that specific field. Weierstrass’s theory of functions,
Cantor’s Set Theory and Group Theory have played (and continue to play) such a role; the vocabulary 
of Category Theory has permeated disparate disciplines. These are some of the grand forces in mathematics
that shape our way of communicating to one another. Other fields formulate powerful viewpoints, templates,
that cross over to distant disciplines–let’s call these fields unifiers. Algebraic Topology has done 
service as a unifier, as has large aspects of Algebraic Geometry. Other fields are so ubiquitous that 
they cast light on all other disciplines of mathematics: measure theory, probability and statistics
come to mind; perhaps aspects of combinatorics.

In any epoch there will be the lingua franca, the unifiers and the ubiquitous of that epoch. A young
(or an old) mathematician, of no matter what specialty, would do well to be acquainted—at least a tiny
bit acquainted—with the mathematical goings-on in these fields. So which fields are of that sort these days?
```

Critical mass of learned info causing phase shift in POV of entire subject:

```markdown
Before offering a concrete list of good “fields of acquaintance” I want to convey an idea of a friend
of mine, who is a student of European History. He tells me that at one point in his career studying
European History, he experienced an abrupt phase shift. Once you’ve achieved—says my friend—a certain 
critical mass of historical information, all of a sudden your view of the entire subject changes. First,
your power of simply retaining information increases multifold; but more importantly, your way of thinking 
about the subject bears no relation to the way you approached things initially. My friend accounted for 
this surprising moment as a consequence of accumulation, perhaps to overload, of somehow-connected specifics
that forced him to involuntarily re-configure—in a more meaningful way— his modes of organization, and 
contemplation, of the entirety of this corpus of knowledge.

Well, it would be interesting if we could put our finger on the critical moments, the phase shifts—if they 
exist—in our mathematical education. If they do exist they may depend less on our having devoured any 
specific collection of mathematical ideas, and more on our having exposed ourselves to some un-specifiable
critical mass. With this in mind, I’ll end, below, with a list that nowadays, in my opinion, stands for 
“fields of acquaintance” any critical mass of them being a good choice for a good mathematical education.
```

A broad mathematical education should help a person achieve a predilection for the four (most classical, significantly overlapping) aspects of mathematical thought — Geometry, Algebra, Computation, and The mathematical intuition(s) derived from Physics — which aren’t the same as the traditional subject classifications, but rather should be thought of as highly developed ‘intuitions’:

```markdown
I say that these are “overlapping,’ but in reality, what makes mathematics one subject is that nothing 
that we do is entirely contained in one of these categories: they seem to stand for distinct intuitions,
and have given rise to distinct realms of thought, and yet they are inseparably welded together. These four
categories have been fused together so substantially in recent times that it may even be misleading to keep
bringing them up. For there are subjects of great importance such as the theory of modular (or automorphic) 
forms that defy this categorization completely, since they stretch their substance, their tools, and the 
intuition they rely on over all four of these items.

Nevertheless, Geometry, Algebra, Computation, and Mathematical Physics represent a recognizable, if wobbly, 
partition of mathematical sensibilities. Let us consider them as motivating intuitions rather than fixed 
repositories of knowledge; i.e., as fundamental types of highly developed senses that some mathematicians
enjoy: there are people with strong geometric intuitions; there are those with strong algebraic intuitions;
there are those who are very sensitive to various aspects of computation and estimation and then there are 
the lucky people who also bring into the mix some basic physical intuition. I also think that we all understand
what it means for some mathematician to have any one of these gifts—whether or not we ourselves possess it.

So, a broad mathematical education should, perhaps, aim to help a person achieve (at least somewhat) a 
predilection for each of these ways of thinking. I would like to view each of these intuitions as “core” 
rather than any particular conglomeration of subject matter as core.

In response to an earlier draft, David Mumford asked me where I would put the traditional, and grand, subject 
of Analysis in this classification, Analysis being a subject dealing with intuitions as fundamental as time, 
change, and continuity; my feeling is that the subject is so many-dimensional that it derives its inspiration
and intuitions from every, or any, direction: when you say that someone is a strong analyst, you might mean 
that this person has a keen sense of a priori estimates in PDE, which I would tag as computation and estimation;
or you might be talking of a good complex analyst dealing with dynamical systems which I would tag as geometry,
etc. I would probably want to spread Analysis over all four categories.
```

The four intuitions, “today’s list” per Mazur:

```markdown
• Geometry-in the broad sense: This can be experienced in so many different ways that any one person’s 
“critical mass” will be disjoint from another person’s. Our sense of geometry might go from knot theory
to differential geometry (and the related analysis; e.g., the spectrum of the Laplacian) to classification
of n-dimensional manifolds to symplectic geometry to dynamical systems to sphere-packing to fixed point
theorems to K-theory to systems of elliptic PDE’s in the large to the Index Theorem to Bott periodicity
to the homotopy groups of spheres. . . and here we would be moving into the more algebraic realms of 
algebraic topology, which nowadays is also commingling with algebraic geometry.

• Algebra-in the broad sense. This includes the ubiquitous notion of groups (say; finite, finitely presented,
Lie, algebraic, arithmetic, and adelic) and their linear and projective (finite- and infinite-dimensional)
representations. It includes elementary aspects of any of the subjects with “algebraic” as an adjective in
their name: algebraic topology, algebraic geometry, and algebraic number theory. It includes the entire 
basic vocabulary of very general languages such as category theory and more specific languages of use in 
traditional functional analysis such as the theory of Hilbert and Banach spaces.

• Computation–in the broad sense. This stretches from machine computation, algorithms, numerical analysis, 
estimation, and statistics, to combinatorics, analysis and probability; i.e., ways of dealing with data,
practically and/or theoretically.

• Mathematics related to Physics. Newtonian Mechanics, Optics, Maxwell’s Equations, Relativity, some Quantum
Mechanics and some physics related to field theories and string theory; and—of course—the mathematics that 
connects to this.
```

<a name="#Solving-math-problems-terribly"></a>
### Solving math problems terribly
([overview](#overview))

From Alon Amit's wonderful essay [Solving math problems terribly](https://qr.ae/TUnpLe):

```markdown
When you solve a math problem, especially a hard one, there’s a profound sense of
accomplishment accompanied by a need to share your masterpiece with the world. In doing 
so, most people’s instinct is to present the solution in its most pristine, elegant,
nicely worked-out form. This works well to impress, but not so well to teach.

After all, your path to discovery almost certainly didn’t land on that clean, elegant 
solution right away. You stumbled, fumbled and groped in the dark for a while, you tried
various things that failed, you hit on the right path only thanks to some instinct or 
intuition or methodology that makes sense to you but is hard to describe, and so on. All
of that stuff is, lamentably, missing from the vast majority of research papers, “solutions
to exercises” appendices, and even most worked-out solutions on places like AoPS, where 
ostensibly the goal is to help people become better problem solvers.

On several occasions on Quora, I resisted the urge to present a short, glamorous solution
to a problem, and instead revealed in painful detail the false turns, stupid blunders and 
failed attempts I went through as I was solving it...

I like that. I think it’s helpful, I think it’s not done often enough, and I think
presenting the most elegant solution can be downright harmful. It makes readers feel it’s
magic, and they would have had no chance of discovering it themselves. It can be 
intimidating and makes math look like voodoo. It’s not. Many things that look like strokes 
of genius are actually the result of very methodical and organized efforts.

So, in the future, I intend to keep doing that as much as I can. And when I do, I don’t 
want to spend the first few paragraphs of every such answer explaining my rationale. ...

Please keep in mind:

1. This approach will make many of my answers *long*. Don’t be intimidated by that. I could
have made them 90% shorter and 100% more mysterious. If you’re genuinely interested in 
learning how to solve math problems, I hope that my revealing the meandering path is more
helpful even if it makes for longer answers.

2. My experience and my failures are still my own, and yours may differ. I hope you learn 
something from my experience, but I can’t guarantee that it works in all cases.

3. It’s impossible to reduce a problem solving journey to absolute first principles. Every 
problem we successfully solve relies on some amount of prior knowledge, experience and 
preparation. I’ll try to highlight those explicitly whenever I can, but please be aware that
you do need to be prepared. If I solve a problem using complex analysis and you happen to 
not know anything about complex analysis, it’s unlikely that you’ll learn a whole lot from 
reading my answer.
```

<a name="#product-management-of-math"></a>
### Product management of math
([overview](#overview))

I've never seen this perspective articulated before or since, which makes it all the more valuable. This is from Alon Amit's [The Product Management of Mathematics](https://affinemess.quora.com/The-Product-Management-of-Mathematics). 

Exposition:

```markdown
Ask someone “what is a real number?”, and you will likely get something involving Cauchy
sequences, or Dedekind cuts, or decimal expansions.

Ask them “what is a complex number?”, and you may be informed about pairs of real numbers
with an i thrown in.

Ask them “what’s the difference between Riemann and Lebesgue integration?”, and in all 
likelihood you’ll get a dollop of Riemann sums, vertical vs horizontal partitioning, and
some measure theory.

All of these answers are correct. All of them, and many like them, are wrong. Not *wrong* as
in *incorrect*, but wrong as in this is *not the right answer*. It’s likely unhelpful, or at 
least not the most helpful it could be.

Why?

Ask someone “what is the difference between Facebook and Twitter?” Imagine they start telling
you about Scala, PHP, Thrift, Hadoop, Memcached and the like. If they know what they’re
talking about, they may provide you with a very accurate picture of the differences between
the tech stacks and software architectures of the two services. Their answer may be fully 
correct.

But this isn’t what you wanted to know, is it? The answer is correct, but useless.

What you want to know is what is the difference between the *products*. What people who *use* 
them experience differently. You want to know about friending vs following, sharing vs
broadcasting, personal social circles vs celebrity fan bases, and so on. You don’t care about
specific features, and you certainly don’t care about the underlying technology. You care 
about the experience, what it feels like, what you can do and how you are likely to use them.

You see, the math examples I gave are like answering the FB/Twitter question with the tech 
stack. It’s telling you about the inner workings, the construction, the engineering of those 
mathematical concepts instead of telling you what they *do*, which is what a Math Product Manager
would tell you.

A Math Product Manager would explain the difference between Riemann and Lebesgue integration
without ever mentioning Riemann sums and partitioning the range instead of partitioning the 
domain. Those things belong to the Math Engineers. The Riemann Integral and Lebesgue Integral 
are *products*. They have some commonalities (like Facebook and Twitter) and certain profound, 
crucial differences, and it is those differences you should care about, and those are 
differences in what they *do*, not how they are *built*.

Both integrals are machines, black boxes, devices that crunch functions and produce numbers. 
They are both linear, monotonic and have other nice features. The Lebesgue integral, however,
can handle many functions which make the Riemann integral crash. The Lebesgue integral, in fact,
has a much better-written spec: it’s possible, with reasonable effort, to describe just what it
can and cannot do. The Riemann integral is a classical “engineering-driven” product: it was 
built, rather than defined. It works great when it works, but describing exactly what it’s *good*
for is next to impossible.

This is the difference between engineering-oriented and product-oriented *exposition* of mathematical
ideas. It’s a difference in how we choose to teach, to explain, to clarify. 
```

Research:

```markdown
But the difference between Math PMs and Math Engineers goes well beyond math education. It exists 
right at the very endeavor of doing mathematics.

Product Managers *define* products. Engineers *build* them. Math research can sometimes be seen in 
an analogous way.

When faced with a challenge, many mathematicians start building scaffolding and structures with 
ingredients they have, piecing them together and hoping to achieve what they set to achieve. But
other times, they try to step back and define what it is they hope to build.

This can take the form of a *program*: a plan for a mathematical reality we don’t yet understand. We
don’t even know if it’s possible, but the far-sighted among us can envision. Robert Langlands,
famously, created such a program.

Other times, this literally takes the form of a “product spec”. Following the success of proving 
the Weil Conjectures for curves over finite fields, André Weil created an ambitious program for
expanding them into a profound, far-reaching edifice, but he also suggested how this could be done:
by creating a cohomology theory for contexts where there was no apparent way to have one. Weil 
wrote a spec for this cohomology theory, but he didn’t know how to build it. It took decades to 
build, but this is just how étale cohomology came to be: it was an engineering response to a 
product spec. Grothendieck, Deligne and others knew what requirements they needed to satisfy. They
set out to satisfy them. Grothendieck himself created far, far-reaching definitions for mathematical
universes we are still trying to put together.

At a much smaller scale, math research and even math problem solving are sometimes like that. A 
mathematician may say to themselves, if only I had a non-abelian analogue of this… this is a rough
spec. A problem solver may think, wow, if only I could have an energy function with *these* 
properties… hmmm… It’s a top-down instead of a bottom-up approach: instead of using the given data,
you are portraying what you need, the end result, and then you try to build it, sometimes literally
working backwards from the specification.

To be clear, this isn’t a competition or a war between camps. Product Managers and Engineers work 
together, not against each other. Successful companies and products emerge when they work together
well. There are usually more engineers than PMs, and it’s the same in math. In software development,
it is more common to have the discipline to define what you’re after before you start building it. 
Many mathematicians work without a product spec, and that’s fine and reasonable because often they 
don’t know what they will find. It’s research, not product development.

But there are at least two contexts in which the PM perspective is important: when mathematical 
leaders like Langlands, Grothendieck and Weil define a vision for what’s needed; and when we *teach*. 
Because when we teach it’s crucial to explain not just *how* something is built but also *why*. Why 
did we introduce quaternions? Why did we introduce cohomology theory? Why did we introduce measures,
matroids or matrices? What can you *do* with those things? How do you *use* them?

That’s true even if, historically, some of those things were stumbled upon rather than designed, 
constructed haphazardly rather than under the watchful eye of a product designer. The number e was
originally defined as the limit of certain compound interest calculations; this is almost irrelevant
nowadays. That’s not why it’s so important. When we teach it today, we can clarify its utility and
importance as a *product*.

I’ve often quoted Tim Gowers’ tenet that mathematical objects *are* what they *do*. I think this is 
a facet of the same idea: thinking in product terms, rather than engineering terms, can often be 
helpful in understanding math, in teaching math, and in guiding mathematical research. You don’t 
have to be math PM, but it’s good to know that they exist. And I take comfort in knowing that 
sometimes I play a PM role when I teach math, in rough analogy to my own day job.

It’s a good feeling.
```

<a name="#Thinking-and-explaining"></a>
### Thinking and explaining
([overview](#overview))

I always return to the late Bill Thurston's great MO question [Thinking and explaining](https://mathoverflow.net/questions/38639/thinking-and-explaining) every few months or years for the wealth of insightful quotes within. Here I place them within the wider context of my walled garden of ideas/document.

Bill's question:

```markdown
How big a gap is there between how you think about mathematics and what you say to others?
Do you say what you're thinking? Please give either personal examples of how your thoughts
and words differ, or describe how they are connected for you.


I've been fascinated by the phenomenon the question addresses for a long time. We have 
complex minds evolved over many millions of years, with many modules always at work. A 
lot we don't habitually verbalize, and some of it is very challenging to verbalize or to
communicate in any medium. Whether for this or other reasons, I'm under the impression 
that mathematicians often have unspoken thought processes guiding their work which may 
be difficult to explain, or they feel too inhibited to try. One prototypical situation 
is this: there's a mathematical object that's obviously (to you) invariant under a 
certain transformation. For instant, a linear map might conserve volume for an 'obvious'
reason. But you don't have good language to explain your reason---so instead of 
explaining, or perhaps after trying to explain and failing, you fall back on computation. 
You turn the crank and without undue effort, demonstrate that the object is indeed
invariant.

Here's a specific example. Once I mentioned this phenomenon to Andy Gleason; he immediately
responded that when he taught algebra courses, if he was discussing cyclic subgroups of a
group, he had a mental image of group elements breaking into a formation organized into 
circular groups. He said that 'we' never would say anything like that to the students. His
words made a vivid picture in my head, because it fit with how I thought about groups. I 
was reminded of my long struggle as a student, trying to attach meaning to 'group', rather
than just a collection of symbols, words, definitions, theorems and proofs that I read in 
a textbook.
```

The following are MO's responses I liked. 

Here's one by Bill that comes to mind every once in a while:

```markdown
When listening to a lecture, I can't possibly attend to every word: so many words blank 
out my thoughts. My attention repeatedly dives inward to my own thoughts and my own mental
models, asking 'what are they really saying?' or 'where is this going?'. I try to shortcut
through my own understanding, then emerge to see if I'm still with the lecture.
```

And here's another, also by Bill:

```markdown
I've learned that when I go back to look at something, my thinking has usually rounded off
too many corners, so my understanding is much fuzzier. I sometimes find things I have 
written to be very obtuse. I was too wrapped up in my then state of mind to express ideas 
clearly even to myself, reading it much later.

What's important is not the process by which you arrived at an idea, but a story that gives
the idea context and meaning. It's a story you make: a setting of meaning and reason for the 
idea, rather than the history of how you stumbled on the idea.
```

Terry Tao is -- well, he's as close as you'll get to a universal problem-solver this generation:

```markdown
I find there is a world of difference between explaining things to a colleague, and explaining
things to a close collaborator. With the latter, one really can communicate at the intuitive
level, because one already has a reasonable idea of what the other person's mental model of the
problem is. In some ways, I find that throwing out things to a collaborator is closer to the 
mathematical thought process than just thinking about maths on one's own, if that makes any 
sense.

One specific mental image that I can communicate easily with collaborators, but not always to 
more general audiences, is to think of quantifiers in game theoretic terms. Do we need to show
that for every epsilon there exists a delta? Then imagine that you have a bag of deltas in your
hand, but you can wait until your opponent (or some malicious force of nature) produces an 
epsilon to bother you, at which point you can reach into your bag and find the right delta to 
deal with the problem. Somehow, anthropomorphising the "enemy" (as well as one's "allies") can 
focus one's thoughts quite well. This intuition also combines well with probabilistic methods, 
in which case in addition to you and the adversary, there is also a Random player who spits out
mathematical quantities in a way that is neither maximally helpful nor maximally adverse to your 
cause, but just some randomly chosen quantity in between. The trick is then to harness this
randomness to let you evade and confuse your adversary.

Is there a quantity in one's PDE or dynamical system that one can bound, but not otherwise 
estimate very well? Then imagine that it is controlled by an adversary or by Murphy's law, and 
will always push things in the most unfavorable direction for whatever you are trying to
accomplish. Sometimes this will make that term "win" the game, in which case one either gives up
(or starts hunting for negative results), or looks for additional ways to "tame" or "constrain" 
that troublesome term, for instance by exploiting some conservation law structure of the PDE.

For evolutionary PDEs in particular, I find there is a rich zoo of colourful physical analogies
that one can use to get a grip on a problem. I've used the metaphor of an egg yolk frying in a
pool of oil, or a jetski riding ocean waves, to understand the behaviour of a fine-scaled or 
high-frequency component of a wave when under the influence of a lower frequency field, and how
it exchanges mass, energy, or momentum with its environment. In one extreme case, I ended up 
rolling around on the floor with my eyes closed in order to understand the effect of a gauge 
transformation that was based on this type of interaction between different frequencies. 
(Incidentally, that particular gauge transformation won me a Bocher prize, once I understood how 
it worked.) I guess this last example is one that I would have difficulty communicating to even 
my closest collaborators. Needless to say, none of these analogies show up in my published papers,
although I did try to convey some of them in my PDE book eventually.

ADDED LATER: I think one reason why one cannot communicate most of one's internal mathematical
thoughts is that one's internal mathematical model is very much a function of one's mathematical
upbringing. For instance, my background is in harmonic analysis, and so I try to visualise as
much as possible in terms of things like interactions between frequencies, or contests between 
different quantitative bounds. This is probably quite a different perspective from someone 
brought up from, say, an algebraic, geometric, or logical background. I can appreciate these other
perspectives, but still tend to revert to the ones I am most personally comfortable with when I am
thinking about these things on my own.

ADDED (MUCH) LATER: Another mode of thought that I and many others use routinely, but which I 
realised only recently was not as ubiquitious as I believed, is to use an "economic" mindset to 
prove inequalities such as X≤Y or X≤CY for various positive quantities X,Y, interpreting them in the
form "If I can afford Y, can I therefore afford X?" or "If I can afford lots of Y, can I therefore
afford X?" respectively. This frame of reference starts one thinking about what types of quantities
are "cheap" and what are "expensive", and whether the use of various standard inequalities 
constitutes a "good deal" or not. It also helps one understand the role of weights, which make things
more expensive when the weight is large, and cheaper when the weight is small.

ADDED (MUCH, MUCH) LATER: One visualisation technique that I have found very helpful is to
incorporate the ambient symmetries of the problem (a la Klein) as little "wobbles" to the objects 
being visualised. This is most familiarly done in topology ("rubber sheet mathematics"), where every
object considered is a bit "rubbery" and thus deforming all the time by infinitesimal homeomorphisms.
But geometric objects in a scale-invariant problem could be thought of as being viewed through a 
camera with a slightly wobbly zoom lens, so that one's mental image of these objects is always 
varying a little in size. Similarly, if one is in a translation-invariant setting, one's mental
camera should be sliding back and forth just a little to remind you of this, if one is working in a 
Euclidean space then the camera might be jiggling through all the rigid motions, and so forth. A more
advanced example: if the problem is invariant under tensor products, as per the tensor product trick,
then one's low dimensional objects should have a tiny bit of shadowing (or perhaps look like one of 
these 3D images when one doesn't have the polarised glasses, with the slightly separated red and blue
components) that suggest that they are projections of a higher dimensional Cartesian product.

One reason why one wants to do this is that it helps suggest useful normalisations. If one is
viewing a situation with a wobbly zoom lens and there is some length that appears all over one's
analysis, one is reminded that one can spend the scale invariance of the problem to zoom up or 
down as appropriate to normalise this scale to equal 1. Similarly for other ambient symmetries.

This sort of wobbling of symmetries is also available in less geometric settings. When viewing,
say, a graph on n vertices, perhaps the labels 1,…,n on the vertices have a tendency to swap with 
each other every so often, to emphasise the symmetry of relabeling in graph theory. Similarly, 
when dealing with a set {a,b,c,d,…}, perhaps the positions of the elements a,b,c,d in one's
enumeration of the set are volatile and swap places every so often. In analysis, one often only 
cares about the order of magnitude of some very large or very small quantity X, rather than its 
exact value; so one should view this quantity as being a bit squishy in size, growing or shrinking
by a factor of two or so every time one looks at the problem. If there is some probability theory
in one's problem, and some of your objects are random variables rather than deterministic 
variables, then you can imagine that every so often the "game resets", with the random variables
jumping around to different values in their range (and any quantities depending on these variables 
changing accordingly), whereas the deterministic variables stay fixed. Similarly if one has generic
points in a variety, or nonstandard objects in a space (with the point being that if something bad
happens if, say, your generic point is trapped in a subvariety, you can "reset the game" in which 
the generic point is now outside the subvariety; similarly one can "reset" an unbounded nonstandard 
number to be larger than any given standard number, etc.).
```

Tim Gowers notes that this also happens "at the very bottom" in simple arithmetic:

```markdown
This phenomenon occurs not just in advanced mathematics but also right at the very bottom in 
simple mental arithmetic. If I have to do a moderately complicated calculation such as adding 
two three-digit numbers, there's often a part of my brain that jumps ahead to the answer before
another more cautious part has got there with carefully checked calculations. The first part 
just sort of feels the answer and then says "I told you so" to the second part, except 
occasionally when the first part gets it wrong and the second part says "Now you know why I
bother to be careful" to the first part.

And there are also aspects of how I carry out integer addition and subtraction that I would 
normally be a bit embarrassed to verbalize, such as that if I subtract 48 from 135 then there's 
a preliminary answer, 97, that I know from experience is wrong and has to be corrected by
subtracting 10. (The justification for the preliminary answer is that 13-4=9 and that the answer
must end in a 7.) It's not quite what's going on in my head, but it's almost as though I say, 
"OK I'll subtract 58 instead so as to get the right answer." But if I were teaching this to a 
child then I'd tell a slightly different story, such as borrowing 1, or first subtracting 50 and 
then adding 2.
```

Bill agrees:

```markdown
I agree, beginning math is a very rich and intriguing area. I've discussed arithmetic questions
with many young children, and they are often very creative in strategies to think their way to
an answer. It really requires being on your toes to discern their thought processes, because the
words do not match adult expectations; they often take phrases with logical meanings that I've
suppressed because of convention. To teach math to kids, I think it's paramount to encourage 
them to think, rather than teach conventional "borrowing 1" type stories. Early math teaching 
usually *suppresses* thinking.
```

I also like Deane Yang's response to Gowers above:

```markdown
Actually, I like explaining ways of solving problems that involve making mistakes, intentional 
or not, and then figuring out how to correct them. I like this better than trying to teach
error-free algorithms, because it incorporates the error-checking as a natural part of the process.
I believe we should be teaching more systematic methods for finding and correcting errors. 
Students should learn when guessing, checking, and correcting is faster and easier than a more 
direct algorithm. Integrals that require more than one integration by parts is an obvious example
of this.
```

Alon Amit takes what Deane says above and makes it the guiding ethos of his expository math writing on Quora (see [Solving math problems terribly](#Solving-math-problems-terribly)).

And then there's Vivek Shende's "subconscious mastication", which is analogous to Ramanujan, just slower:

```markdown
I have a worse problem than having unspoken thought processes: some of my best thought processes
are simply beneath the level of consciousness and I don't notice them at all until they're finished.
Even then, I often get only an answer and not an explanation out of them. Surely this happens to 
everyone: the problem solved during sleep, the idea on a walk in the woods, the conviction that a 
conjecture is true on utterly minimal evidence, the argument that pops up full formed in the middle
of a conversation.

My mathematical process is roughly this: consciously, I try a lot of stupid things which essentially
have no chance of working but do have the benefit of exposing me to lots of examples; these examples
pile up and are subconsciously masticated for days, weeks, months -- I'm not old enough 
mathematically to put "years" here yet -- and eventually by some inner and unobservable process I 
just have a feeling about what to do.

Perhaps that's an exaggeration. But I certainly do feel that way sometimes, and to the extent that 
it's true, it means that the whole project of trying to communicate how I thought of something is 
just telling stories, at least if I say anything other than "well, I just knew one day."
```

Cam MacLeman:

```markdown
One of my favorites from undergrad was describing a linear transformation as a commander-
in-chief, who told the generals (a basis) where to go, who in turn tells all the soldiers
(the rest of the vectors) where to go. The chain of command in action in a linear algebra
class.
```

<a name="#math-advice"></a>
### Math advice
([overview](#overview))

<a name="#reading-the-masters-in-math"></a>
### Reading the masters in math
([overview](#overview))

(See also [Reading the masters in philosophy](#reading-the-masters-in-philosophy), or my old post [If Aristotle were a pro skater: or, reading the masters in math and philosophy*](https://mosstuff.quora.com/If-Aristotle-were-a-pro-skater-or-reading-the-masters-in-math-and-philosophy) to see both math and philo in one place.) 

Should you read the masters? In other words, should you prefer primary sources to summaries and commentaries?

On the one hand, the masters are the masters, so surely there’s something to reading them. On the other hand, I’m the kind of person who easily gets lost in walls of text, so when it comes to writing on difficult / ‘slippery’ topics I prefer polished, non-digressive reads. And primary texts, so they seem, are nothing but digressive.

(I suppose it depends why you’re reading. If it’s for enjoyment then secondhand sources certainly won’t cut it. I’m usually looking for insight; enjoyment I relegate to fiction, or exceptionally-written exposition, or something.)

It doesn’t help that Andrew L’s comment in [this MO thread](https://mathoverflow.net/questions/28268/do-you-read-the-masters) is essentially what I think of the masters (to wit, that they’re notoriously hard to read):

```markdown
There's a myth surrounding Abel's dictum that stems from the unreadability of the masters
like Gauss as a measure of their nearly inhuman brilliance. This is a fallacy.

The reason the masters are so difficult to read is because we are catching them with their
pants down in the act of creation: they are groping towards the right notation and
terminology, but aren't quite there yet.

For example, it's pretty clear Riemann in his doctoral lecture was trying to explain the 
need for higher dimensional spaces that went beyond familiar three dimensional space 
("multiply extended quantities") which preserved all the familiar properties of the usual
Euclidean spaces, i.e. Kleinian transformations and calculus in local neighborhoods. The
problem was without either linear algebra or the fundamentals of topology, it was next to 
impossible to express this idea clearly and precisely. He just ends up babbling on about
what's needed. But all the same, Riemann recognized what was needed, even if how to
express it correctly was beyond his ability.
```

There must be *something* to reading the masters, or at least primary sources. What could it be?

Often you can judge how ‘mature’ a field is by seeing how much shorter proofs of old results become when appropriate machinery gets developed and subsumes them as special cases within the larger theoretical framework. (Think Grothendieck, but less extreme.) This is usually good. But then there are cases to the contrary, like Fedja’s experience, recounted in the MO thread [Papers better than books?](https://mathoverflow.net/questions/140954/papers-better-than-books?lq=1&noredirect=1):

```markdown
Very recently I and Misha Sodin had a strong incentive to learn the Ito-Nisio lemma… The 
textbooks we could find fell into 2 categories: those that didn't present the proof at 
all and those presenting it on page 2xx as a combination of theorems 3.x.x, 4.x.x, 5.x.x,
etc.

The original paper is less than 10 pages long, essentially self-contained, and very easy
to read and understand.

The moral is the same as Boris put forth: the books are there to optimize the time you 
need to spend to learn the whole theory. However, for every particular implication A->B 
the approach they usually take is something like E->F->G, G->F, (F and Q)->B; since A->E,
then A->G; once we know G, we have F, so it suffices to prove that A->Q to show that A->B;
we show that Q,R,S,T,U are equivalent, with the trivial implication S->Q left to the 
reader as an exercise; finally, we prove that A->S.

So if all you need is A->B, you may be much better off reading the paper whose only purpose
is to prove exactly that.
```

Andrew L again, same thread as the first:

```markdown
One of my favorite books is Hassler Whitney's Geometric Integration Theory. I have friends 
in differential geometry who tell me it's a dinosaur, that his proof of the de Rham theorem 
is incredibly coarse and tedious. Yes, it is — but it has the advantage of being a DIRECT
proof from the construction of simplexes on the boundary of an embedded manifold. I love the
book because although Whitney's ideas were old fashioned, they were incredibly powerful IDEAS
that allow us to tackle the subject concretely and with an amazing amount of insight. THAT'S
what we get from reading the masters — their insight and depth of understanding that allows 
us to see beyond the machinery into why things are defined as they are.
```

Igor Pak makes a cameo:

```markdown
There is more than one reason to read "masters". One such reason is field-specific and can be 
phrased as "read the latest work right before a scientific revolution" (standard example is 
the large body of work by Cayley, Sylvester, Gordan, etc., in the pre-Hilbert classical 
invariant theory). Often such results are more powerful in very specific cases of interest.

Another practical reason to read "masters" is to avoid embarrassment. Lots of (mostly minor) 
results are not mentioned in later treatises, so a number of people rediscover these results 
because they are either too lazy to read, or simply assume that "masters" couldn't have possibly
be so smart to figure out these results back then... When going through the references in
writing this survey, I read all 80 pages of J.J. Sylvester, A constructive theory of partitions,
arranged in three acts, an interact and an exodion, Amer. J. Math. 5 (1882), 251–330. As a 
result, I discovered that a number of recent results were already proved there, sometimes by 
leaders in the field (let me not name them here - see the survey).
```

And then there is Roy Smith’s rich, long answer from the same thread; I’ll only quote a few scattered paragraphs:

```markdown
These are elementary examples hence from a fairly naive and uneducated person, myself, who 
has not at all plumbed the depth of many original papers. But these few forays have definitely 
convinced me there is a benefit that cannot be gained elsewhere, as these exposures can
transform the understanding of ordinary mortals closer to that of more knowledgeable persons, 
at least in a narrow vein. So while it might be thought that only the strongest mathematicians
can attempt these papers, my advice would be that reading such masters may be even more helpful
to us average students.

Once as grad student in Auslander's algebraic geometry class, I vowed to try out Abel's advice
and read the master Zariski's paper on the concept of a simple point. I was very discouraged 
when several hours passed and I had managed only a few pages. Upon returning to class, Auslander
began to pepper us with questions about regular local rings. I found out how much I had learned
when I answered them all easily until he literally told me to be quiet, since I obviously knew 
the subject cold. (To be honest, I did not know the very next question he posed, but I was off 
the hook.) …

The sense of wonder and awe one gets upon reading people like Riemann or Euler, is also quite
wonderful. Any student who has struggled to compute the sum of the even powers of the reciprocals
of natural numbers 1/n^2k, will be amazed at Euler's facile accomplishment of this for many
values of k. Calculus students estimating π by the usual series to 3 or 4 places will also be 
impressed at his scores of correct digits. …

A remark on the definition of master, versus creator. There are cases where a later master 
reexamines an earlier work and adds to it, and in these cases it seems valuable to read both 
versions. In addition to examples given above of Newton generalizing Euclid and Mumford using
Hilbert, perhaps Mumford's demonstration of the power of Grothendieck's Riemann Roch theorem in 
calculating inavriants of moduli space of curves is relevant.
```

I’ve been conflating ‘masters’ and ‘primary sources’. Usually they’re the same, because the really enduring texts are written by the masters, but sometimes the masters write expository texts too, and it’s still marginally beneficial to read them, per Ilya Grigoriev’s comment in the MO thread [Why do so many textbooks have so much technical detail and so little enlightenment?](https://mathoverflow.net/questions/13089/why-do-so-many-textbooks-have-so-much-technical-detail-and-so-little-enlightenme?rq=1):

```markdown
Providing real enlightenment well is very, very hard, and requires a very intimate relationship with a subject.

Even for well-established subjects, like undergraduate mathematics, where there are a million
mathematicians who know the subject very well, I find that all the really good books are written
by the true titans of the field -- like Milnor, Serre, Kolmogorov, etc. They understand the 
underlying structure and logical order of the subject so well that it can be presented in a way
that it basically motivates itself -- basically, they can explain math the way they discovered 
it, and it's beautiful. Every next theorem you read is obviously important, and if it isn't then
the proof motivates it. …It's interesting how all the best books I know don't have explicit 
paragraphs providing the motivation - they don't need them.
```

<a name="#go-to-seminars"></a>
### Go to seminars
([overview](#overview))

Ravi Vakil's [advice page](http://math.stanford.edu/~vakil/potentialstudents.html) for potential PhD students has loads of advice related to going to seminars:

```markdown
Older graduate students will verify that there is a high correlation between those students who 
are doing the broadest and deepest work and those who are regularly attending seminars. Many
people erroneously conclude that those who are the strongest students therefore go to seminars, 
while in fact the causation goes very much in the opposite direction.

Go to research seminars earlier than you think you should. Do not just go to seminars that you 
think are directly related to what you do (or more precisely, what you currently think you 
currently do). You should certainly go to every single seminar related to algebraic geometry that
you can, and likely drop by other seminars occasionally too. Learning to get information out of
research seminars is an acquired skill, usually acquired much later than the skill of reading 
mathematics. You may think it isn't helpful to go to a seminar where you understand just 5% of
what the speaker says, and may want to wait until you are closer to 100%; but no one is anywhere
near 100% (even the speaker!), so you should go anyway.

Try to follow the thread of the talk, and when you get thrown, try to get back on again. (This
isn't always possible, and admittedly often the fault lies with the speaker.)

At the end of the talk, you should try to answer the questions: What question(s) is the speaker
trying to answer? Why should we care about them? What flavor of results has the speaker proved?
Do I have a small example of the phenonenon under discussion? You can even scribble down these 
questions at the start of the talk, and jot down answers to them during the talk.

Try to extract three words from the talk (no matter how tangentially related to the subject at 
hand) that you want to know the definition of. Then after the talk, ask me what they mean. (In 
general, feel free to touch base with me after every seminar. I might tell you something 
interesting related to the talk.)

See if you can get one lesson from the talk (broadly interpreted). If you manage to get one lesson
from each talk you go to, you'll learn a huge amount over time, although you'll only realize this 
after quite a while. (If you are unable to learn even one thing about mathematics from a talk,
think about what the speaker could have done differently so that you could have learned something.
You can learn a lot about giving good talks by thinking about what makes bad talks bad.)

Try to ask one question at as many seminars as possible, either during the talk, or privately
afterwards. The act of trying to formulating an interesting question (for you, not the speaker!)
is a worthwhile exercise, and can focus the mind.

Your thesis problem may well come out of an idea you have while sitting in a seminar.

Go to seminar dinners when at all possible, even though it is scary, and no one else is going.

Go to colloquia fairly often, so you have a reasonable idea of what is happening in other parts 
of mathematics. It is amazing what can become relevant to your research. You won't believe it 
until it happens to you. And it won't happen to you unless you go to colloquia. Ditto for seminars
in other fields.
```

Vakil expands upon one of the points above in this page: [The "Three Things" Exercise for getting things out of talks](http://math.stanford.edu/~vakil/threethings.html):

```markdown
*The challenge of talks*. It is tricky to get things out of talks, even after a lot of practice. 
It is very easy to go to a talk, and at some point have your eyes glaze over. Talks are like horses:
once you are thrown off, it is hard to get back on. Especially if the horse is stomping on your face.
(That's why it is very bad to come into a talk a few minutes late --- even if it is sometimes
necessary.)

"Three Things" is an exercise to learn how to get things out of talks. It can be useful if you are in
the first few years of going to seminars --- I've intended it as practice for graduate students --- 
but I've also found that I got much more out of talks (especially those out of my comfort zone) when
using it. It is admittedly a little contrived, and when a bunch of us first experimented with it 
(perhaps around 2007?), we stopped doing it after a while because we got tired of it.

*The theory is as follows*. If you can get even three small things out of a talk, it is a successful
talk. And if you can't get even three small things out of a talk, it was not a successful experience.
Note that the things you get out of a talk needn't be the things that your neighbor got out of a talk,
or the things the speaker expected you to get out of the talk.

*Here is how it works*. Take a clean sheet of paper, or an index card. Your goal is to have three things,
and only three things, on this sheet at the end of the talk. The "things" can be of many forms:

- a definition you want to remember (e.g. "a K3 surface is...")
- a theorem you want to remember ("the moduli space of polarized K3 surfaces is smooth")
- a motivating or key example ("a quartic is an example of a K3 surface")
- a motivating problem ("why are all moduli spaces of polarized K3 surfaces the same dimension?")
- a question you want to ask the speaker ("why is that hypothesis in your theorem?")
- a question you want to ask someone else (a definition, motivation, a question about a connection etc.)
- anything else of a similar flavor: something specific that made you think. Something vague ("I liked 
the part where she talked about groups") does not count as a "thing".

*As you watch the talk*, look out for "things" you like. When one comes your way, write it down. Then
later write down a second. Then write down a third. Hopefully a fourth will come your way --- and
then you must look over the previous three, and decide which one must be cut. A dirty secret is that
you may not be able to prevent yourself from remembering the one you cut --- and the ones you kept
and reviewed will be more fixed in your mind.

(If you take notes in a more traditional sense, you can still play the game, by putting a star beside
each "thing". This works a little less well; you will be less focused on looking for "things".)

*After the talk*: if other people are playing, send each other your things by email (or discuss them
in person). It is surprisingly enlightening. And there will likely be some follow-up discussion. It
doesn't take much time (to type or to send one sentence responses to others' things if the spirit 
moves you). If you have questions, then ask them to someone (perhaps the speaker over the semianr
dinner; or perhaps your advisor or your students or your colleagues). Don't let them drop.
```

See also his celebrated [backfilling tendrils of knowledge](#mathematical-maturity) quote.

<a name="#asking-the-right-question"></a>
### Asking the right question
([overview](#overview))

[Jay Daigle](https://jaydaigle.net/) is an assistant professor of math at Occidental College who received his PhD in number theory at Caltech under Matthias Flach. He’s also jadagul of [Maybe-Mathematical Musings](http://jadagul.tumblr.com/), one of the two really consistently high-quality math-content-producers/commentators I’ve had the pleasure of stumbling across on math Tumblr (the other being Rob Nostalgebraist, who also writes [genuinely great fiction](https://archiveofourown.org/users/nostalgebraist/pseuds/nostalgebraist) and is a [brilliant book reviewer](https://www.goodreads.com/author/show/13933106.nostalgebraist)). He’s sufficiently [differently free from me](https://www.ribbonfarm.com/2014/11/05/dont-surround-yourself-with-smarter-people/) in Venkat Rao’s sense, plus just flat-out smarter, that I always get a lot out of his writings in Alan Kay’s “point of view is worth 80 IQ points” sense, so I always enjoy reading his math #effortposts.

Recently I discovered that he’s collected these effortposts into a standalone blog, [Jay's Blog](https://jaydaigle.net/blog/paradigms-and-priors/), so I’ve had a lot of fun going over his essays. Here are some quotes from one of those essays, [Asking the Right Question](https://jaydaigle.net/blog/asking-the-right-question/), which I’m saving here for quick future reference. (You should Jay’s original essay instead of defaulting to my quotes below! He’s a really enjoyable read, I promise.)

First off: why is asking good questions so important? About four reasons, the last one being the main thrust of this section:

```markdown
First and most obviously, it’s easier to get help with things and learn things if you can ask better
questions. 

Second, and maybe more importantly, framing questions well is a lot of what makes you a good 
mathematician.

The most boring way to ask a bad question is just to not include enough information. Sometimes this
is just laziness (“I don’t understand how to do this problem, please help”). And I’ve definitely seen 
questions asked that are thin disguises over “I don’t want to do my homework; can someone do it for me?”

But more often, badly-phrased questions result from deep confusion on the part of the asker. If they 
understood the material well enough to ask their question clearly and correctly, they wouldn’t need to
ask it in the first place.
```

It's important to be able to figure out what question to ask in the first place:

```markdown
A lot of math is less about answering questions than about figuring out exactly what question you should
be asking, and how to make it precise. We tend to sweep this under the rug a bit when teaching, in a way 
that I suspect probably leads to a certain amount of confusion.

When we teach, we often ask a question, and then demonstrate a tool to answer it, without necessarily 
stopping to explain why that question is a good one, or how people settled on asking exactly that question.
And this often leaves our students with the sense that what they’re doing doesn’t really mean anything.

This is a major reason students fall back on figuring out what “type of problem” they’re working on, and
then following “the steps” to get the answer. They see math as a sort of opaque box, and a question asks
them to perform the correct magical ritual to get the answer. Because if the words you’re using—and your
questions—don’t have a meaning, that’s all you can really do.

And that’s how you get questions like “how do I find solutions to f(x) = sin(x)+1/2.” I can tell what the
original question probably was. But because the student doesn’t really understand what a “function” is, 
they ask a question that is, read literally, completely nonsensical.
```

An example of a badly-written but actually great question:

```markdown
Sometimes, you see a question that’s basically “this one thing feels kind of like this other thing, but 
I can’t tell you how. Can you tell me?”

These people are doing good math. They’re noticing a pattern, and trying to put it into words. They’re 
maybe not quite there, and sometimes it’s hard to answer the question clearly. But it shows great
instincts.

And this is how math tends to actually get done! When we teach, we tend to define terms, then state 
theorems about them, and then prove the theorems. But this is exactly backwards from how math is often
actually done. First we understand what’s going on; then we figure out what the rule is and write it 
down; and finally understand what conditions are important and give those conditions names. That is, 
we formulate a proof, then state the theorem, and then define the terms.

These questions are working on step 1. I want to encourage them.
```

<a name="#why-math-is-boring"></a>
### Why math is boring
([overview](#overview))

From John Baez's "very rough draft" [Why Mathematics is Boring](http://math.ucr.edu/home/baez/boring.pdf), where he argues in the abstract that the boring style of math publications is a serious matter to contend with:

```markdown
Storytellers have many strategies for luring in their audience and keeping them interested.
These include standardized narrative structures, vivid characters, breaking down long stories
into episodes, and subtle methods of reminding the readers of facts they may have forgotten. The
typical style of writing mathematics systematically avoids these strategies, since the explicit goal
is “proving a fact” rather than “telling a story”. Readers are left to provide their own narrative
framework, which they do privately, in conversations, or in colloquium talks. As a result, even
expert mathematicians find papers — especially those outside their own field — boring and
difficult to understand. This impedes the development of mathematics.
```

Introduction:

```markdown
In their research papers, mathematicians usually eschew narrative techniques designed to keep readers
interested, since their main goal is not to “entertain” or even explain, but present logical arguments
as efficiently as possible. While this makes a certain sense, it neglects the human dimension
of mathematics. It neglects the fact that a piece of mathematics is almost useless if almost nobody
understands it. But, before anyone can understand a piece of mathematics, they must first become
interested in it. So, for a mathematician who wants to fully develop a piece of mathematics, discovery
and proof are only the first steps on a longer road. The next step is getting people interested.

Unfortunately, mathematicians are not trained in this art. Indeed, their writing is famous for
being “dry”. There are exceptions, and these exceptions are worth studying. But it also makes sense
to look to people whose whole business is getting people interested: story-tellers.
Everyone enjoys a good story. We have been telling and listening to stories for untold millennia.
Stories are one of our basic ways of understanding the world. I believe that when we read a piece of
mathematics, part of us is reading it as a highly refined and sublimated sort of story, with characters
and a plot, conflict and resolution.

If this is true, maybe we should consider some tips for short story writers, and see how they can
be applied — in transmuted form — to the writing of mathematics. These tips may sound a bit
crass to mathematicians, or even readers of “serious” fiction. But they go straight to the heart of
what gets people interested, and what keeps them interested, in a piece of writing.
```

Under "write a catchy first paragraph":

```markdown
We are constantly encountering texts; we don’t bother reading all the way through most of them.
Once texts were rare and precious. Now, in the era of the world-wide web, there is always too much
to read. We must efficiently cull out most of the material vying for our attention. Often we base
our decision on the first sentence or two.

Since most writers of short stories succeed largely on their sheer number of readers, and few
people read stories because they need to, writers of short stories learn the importance of quickly
grabbing the reader’s attention. In a catchy story, each sentence makes the reader want to read the
next. The first few sentences bear the brunt of this responsibility.

Mathematicians operate in a more forgiving environment, with guaranteed permanent employment for many. 
They can succeed with only few people reading their work. Consider two of the 
most famous mathematicians of recent years: Andrew Wiles and Grigori Perelman. How many of us
   have really read Wiles’ proof of Fermat’s last theorem, or Perelman’s sketched proof of the Poincar´e
conjecture? Even among professional mathematicians, most are satisfied to know that a few experts
vouch for these proofs’ validity. So, instead of broadening their readership, mathematicians are
mostly concerned with impressing other experts in their field.
```

See also [why academic writing sucks](#why-academic-writing-sucks). 

So how might we do better? 

From Terry Tao's [Write in your own voice](https://terrytao.wordpress.com/advice-on-writing-papers/write-in-your-own-voice/):

```markdown
When one is not simply quoting the prior text for historical or archival purposes, it is best to 
paraphrase and interpret the previous text rather than to copy that text verbatim.  This is for a 
number of reasons:

- One wants to avoid conveying any impression to readers, referees, or editors of plagiarism, padding,
or intellectual laziness in one’s papers.  (Note that the latter is a danger even if one is copying 
from one’s own work, rather than that of others.)

- The prior work may be dated in view of more recent developments and insights, as mentioned above.

- If you are copying or adapted a piece of text from another author that you do not fully understand
yourself, then it may end up being inappropriate or incongruous for your intended purpose, and may 
convey the impression of superficiality or being ill-informed.  If the text becomes inaccurate due 
to this adaptation, then this can also cause some embarrassment and annoyance for the original author
of that text.

- Excessive use of quotation from famous mathematicians to make one’s own work look more impressive is
the mathematical equivalent of name-dropping, and should be avoided.  Appeal to authority should not be
the primary basis for motivating a paper; a handful of citations to demonstrate the depth of interest 
in the problem being studied is usually sufficient.

- But most importantly of all, for one’s further mathematical development and career, one needs to
develop one’s own consistent mathematical “voice” and style, and to avoid the impression of simply 
imitating the voices of other authors.  There is no need in this subject for the mathematical equivalent
of a parrot, and a text which is a mix of the author’s voice and the voice of others can read very 
strangely.
```

Terry advises strongly *against* imitating an author's style:

```markdown
In some cases, the imitation of a previous author’s style and text is intended as a sign of respect or 
flattery for that author.  This is misguided; an author will in fact often find such mimicry to actually 
be somewhat offensive.  If one wants to truly respect a mathematician, then understand that
mathematician’s methods, results, and exposition, and improve, update, adapt, and advance all three.  
Even the greatest mathematician’s contributions should advance with the field, rather than being 
worshipped and preserved in some supposed state of perfection; the latter is mostly suitable only for 
historical purposes.
```

Contrast [Scott Alexander's comment on Luke's *Rhetoric* post](#nonfiction) (it's a long section, so I'll quote only the relevant bit):

```markdown
Your role models here should be those vampires who hunt down the talented, suck out their souls, and absorb
their powers. Which writers' souls you feast upon depends on your own natural style and your goals. I've
gained most from reading Eliezer, Mencius Moldbug, Aleister Crowley, and G.K. Chesterton; I'm currently making
my way through Chesterton's collected works pretty much with the sole aim of imprinting his writing style into 
my brain.

Stepping from the sublime to the ridiculous, I took a lot from reading Dave Barry when I was a child. He 
has a very observational sense of humor, the sort where instead of going out looking for jokes, he just
writes about a topic and it ends up funny. It's not hard to copy if you're familiar enough with it. And
if you can be funny, people will read you whether you have any other redeeming qualities or not.

Getting imprinted with good writers like this will serve you for your entire life. It will serve you 
whether you're on your fiftieth draft of a thesis paper, or you're rushing a Less Wrong comment in
the three minutes before you have to go to work. It will even serve you in regular old non-written
conversation, because wit and clarity are independent of medium.
```

More general advice by Terry on writing papers (all go to links for further reading):

```markdown
- Use the introduction to “sell” the key points of your paper; the results should be described accurately.
One should also invest some effort in both organising and motivating the paper, and in particular in 
selecting good notation and giving appropriate amounts of detail. But one should not over-optimise the paper.
It also assists readability if you factor the paper into smaller pieces, for instance by making plenty 
of lemmas.

- To reduce the time needed to write and organise a paper, I recommend writing a rapid prototype first.

- For first time authors especially, it is important to try to write professionally, and in one’s own voice.
One should take advantage of the English language, and not just rely purely on mathematical symbols.

- The ratio between results and effort in one’s paper should be at a local maximum.
```

More on [creating lemmas](https://terrytao.wordpress.com/advice-on-writing-papers/create-lemmas/) to factor the paper into smaller pieces to aid readability -- this is similar to [information hiding](http://en.wikipedia.org/wiki/Information_hiding) in software engineering, or [structured programming](http://en.wikipedia.org/wiki/Structured_programming) and [modularity](http://en.wikipedia.org/wiki/Modularity_%28programming%29):

```markdown
A typical argument in modern mathematics is often quite intricate, requiring many different steps, 
ingredients, and notation. The authors of the argument, having been intimately involved in all aspects 
of its construction, often do not realise just how complicated such an argument appears to a reader who 
is encountering it for the first time (I myself have been guilty of this oversight).

Part of the reason for this is that there is plenty of implicit structure in a paper which is crucial 
to understanding it properly, and which is known to the authors, but is not readily apparent to the
readers. Suppose for instance that part of a paper goes like this:

…
In Section 2, facts A, B, and C are derived, and then used to deduce D.
In Section 3, D is used to derive E.
In Section 4, D, and another fact F, are used to derive G.
In Section 5, E, G, and another fact H, are used to derive I.
…

A reader who is going through this paper one section at a time will try to keep A, B, C, and D all in
mind after finishing Section 2, and moving on to later sections.  However, facts A, B, and C are never
used again; they were instrumental to the argument because they allowed one to establish D, but once D 
is established, A, B, and C can be safely forgotten. Note, though that the reader does not know this. 
As a consequence, while reading Sections 3, 4, and 5, the reader has to set aside some of his or her 
mental resources to retain some facts which are of no further use, thus obscuring the structure of the
argument and making it more difficult to follow.

Now one could address this by placing some remarks at the end of Section 2 along the lines of “Facts A,
B, and C will not be used again in the rest of the paper”, or by devoting more thought to organising 
and motivating the paper. These are all worthwhile things to do, but a much more elegant solution is
simply to encapsulate D as a lemma, and to place A, B, C inside the proof of that lemma. 

This conveys several useful structural cues to the reader: firstly, that D is likely to be an important 
fact to use in later parts of the argument, and secondly, that A, B, and C are not needed elsewhere in 
the paper and can be safely forgotten. This additional structure will be useful to all readers, but will
be especially appreciated by those readers who are already expert in how to prove facts such as D, since
they can then glance at the statement of the lemma, readily convince themselves that the lemma is plausible
(possibly by using other tools than A, B, and C), and then quickly move on to the next part of the argument.
One can also add some remarks “for the experts” after the proof of a lemma, discussing possible alternate
proofs, refinements, special cases, connections to other lemmas in the literature, etc.

We have seen how “folding” the argument into lemmas can reduce the complexity of that argument by
“localising” certain facts in the argument. The same method can also be used to localise notation, e.g. some
special-purpose notation used to prove D but is not used elsewhere in the paper. The design philosophy here 
is similar to that of information hiding in software engineering. (Other relevant software engineering 
philosophies for mathematical writing include structured programming and modularity.)

Lemmas also provide a good opportunity to explicitly “recap” all the running hypotheses, assumptions, and
notational conventions that have already been introduced in the argument, which can be invaluable to a
reader who has misunderstood or forgotten about part of this implied context. (Sometimes, such a recap would
be tediously long, in which case a sentence such as “Let the notation and assumptions be as above” may 
suffice. When these situations occur, one might wish to formalise all the running assumptions by judiciously
introducing some good notation.)

In some cases, the conclusion of the lemma may only need a portion of these hypotheses; this might be worth 
stating explicitly within the statement of the lemma, as it can clarify the nature of that lemma, and may 
also make it more useful for future applications.

One should write the statement of a lemma in a way that makes it easy to use, rather than easy to prove. 
Thus, one should try to make the hypotheses of the lemma natural and easy to verify, and the conclusions
of the lemma manifestly useful. Basically, the idea is to push as much of the details of the argument 
into the lemma as one can, to make the rest of the argument as simple as possible. Also, it may end up 
that you (or someone else) will eventually find a simpler proof of that lemma, thus reducing the net
complexity of the paper markedly (cf. the object-oriented approach to software engineering).

Folding the argument into lemmas also makes it easier to write a rapid prototype, as once one finalises 
the statement of the lemma, one can defer the proof of the lemma until later.

In summary, it’s almost always a good idea to have plenty of lemmas (and propositions and corollaries too,
of course) in an argument; it makes the overall structure of the argument more apparent, it makes the 
argument easier to follow, and can also provide some useful tools for future work in the area.
```

More on [avoiding premature optimization](https://terrytao.wordpress.com/advice-on-writing-papers/dont-overoptimise/) -- see also [the same in computer science](#premature-optimization) and [readability in writing](#readability):

```markdown
here is a danger in being too perfectionist, and in trying to make every part of a paper as “optimal” 
as possible. After all the “easy” improvements have been made to a paper, one encounters a law of 
diminishing returns, in which any further improvements either require large amounts of time and effort,
or else require some tradeoffs in other qualities of the paper.

For instance, suppose one has a serviceable lemma that suffices for the task of proving the main
theorems of the paper at hand. One can then try to “optimise” this lemma by making the hypotheses weaker
and the conclusion stronger, but this can come at the cost of lengthening the proof of the lemma, and 
obscuring exactly how the lemma fits in with the rest of the paper. In the reverse direction, one could
also “optimise” the same lemma by replacing it with a weaker (but easier to prove) statement which still
barely suffices to prove the main theorem, but is now unsuitable for use in any later application. Thus
one encounters a tradeoff when one tries to improve the lemma in one direction or another. (In this case,
one resolution to this tradeoff is to have one formulation of the lemma stated and proved, and then add 
a remark about the other formulation, i.e. state the strong version and remark that we only use a special
case, or state the weak version and remark that stronger versions are possible.)

Carefully optimising results and notations in the hope that this will help future researchers in the 
field is a little risky; later authors may introduce new insights or new tools which render these 
painstakingly optimised results obsolete. The only time when this is really profitable is when you already 
know of a subsequent paper (perhaps a sequel to the one you are already writing) which will indeed rely
heavily on these results and notations, or when the current paper is clearly going to be the definitive 
paper in the subject for a long while.

If you haven’t already written a rapid prototype for your paper, then optimising a lemma may in fact be a 
complete waste of time, because you may find later on in the writing process that the lemma will need to
be modified anyway to deal with an unforeseen glitch in the original argument, or to improve the overall 
organisation of the paper.

I have sometimes seen authors try to optimise the length of the paper at the expense of all other
attributes, in the mistaken belief that brevity is equivalent to simplicity. While it can be that shorter
papers are simpler than longer ones, this is generally only true if the shortness of the paper was
achieved naturally rather than artificially. If brevity was attained by removing all examples, remarks,
whitespace, motivation, and discussion, or by striking out “redundant” English phrases and relying purely
on mathematical abbreviations (e.g. \forall instead of “For all”, etc.) and various ungrammatical
contractions, then this is generally a poor tradeoff; somewhat ironically, a paper which has been 
overcompressed may be viewed by readers as being more difficult to read than a longer, gentler, and more 
leisurely treatment of the same material. (See also “Give appropriate amounts of detail.”)

On the other hand, optimising the readability of the paper is always a good thing (except when it is at
the expense of rigour or accuracy), and the effort put into doing so is appreciated by readers.
```

More on [Maximising the results-to-effort ratio](https://terrytao.wordpress.com/advice-on-writing-papers/maximising-the-results-to-effort-ratio/):

```markdown
As a professional courtesy, research papers in mathematics should be at a “local maximum” with respect
to the results-to-effort ratio: any “cheap” consequences, generalisations, variants, illustrative
counterexamples, etc. of one’s main results should be put into the paper if this can be done with only
moderate effort on the author’s part.  If one is too lazy to do this, these consequences might not 
appear in the literature for some time (as they are too close to your own paper to be separately
publishable in their own right), and each reader may have to rederive them by himself or herself, which
is a much less efficient process in the long run.

Conversely, if a huge fraction of the paper is devoted to only a minor extension of the main results,
one may consider removing that section, or replacing it by a sketch or even just a remark; it may be 
that a subsequent paper is able to achieve that result with much less effort anyway.
```

<a name="#general-intelligence"></a>
## General intelligence
([overview](#overview))

<a name="#logi"></a>
### *Levels of Organization in General Intelligence*
([overview](#overview))

Here are some interesting passages from Eliezer Yudkowsky’s paper [Levels of Organization in General Intelligence](https://intelligence.org/files/LOGI.pdf).

On the what and why of intelligence:

```markdown
Intelligence is an evolutionary advantage because it enables us to model, predict, and manipulate reality. 
Evolutionary problems are not limited to stereotypical ancestral contexts such as fleeing lions or chipping 
spears; our intelligence includes the ability to model social realities consisting of other humans, and the 
ability to predict and manipulate the internal reality of the mind.

Philosophers of the mind sometimes define “knowledge” as cognitive patterns that map to external reality 
(Newell 1980), but a surface mapping has no inherent evolutionary utility. Intelligence requires more than 
passive correspondence between internal representations and sensory data, or between sensory data and reality. 
Cognition goes beyond passive denotation; it can predict future sensory data from past experience. Intelligence 
requires correspondences strong enough for the organism to choose between futures by choosing actions on the 
basis of their future results.

Intelligence in the fully human sense requires the ability to manipulate the world by reasoning backward from a 
mental image of the desired outcome to create a mental image of the necessary actions. (In Section 2, these 
ascending tests of ability are formalized as sensory, predictive, decisive, and manipulative bindings between a 
model and a referent.)
```

For more on the evolution of intelligence in LOGI see [here](#evolution).

On AI being “not like” physics:

```markdown
I am admittedly biased against the search for a “single essence” of intelligence; I believe that the search for 
a single essence of intelligence lies at the center of AI’s previous failures. Simplicity is the grail of physics, 
not AI. Physicists win Nobel Prizes when they discover a previously unknown underlying layer and explain its 
behaviors. We already know what the ultimate bottom layer of an Artificial Intelligence looks like; it looks like 
ones and zeroes. Our job is to build something interesting out of those ones and zeroes. The Turing formalism does 
not solve this problem any more than quantum electrodynamics tells us how to build a bicycle; knowing the abstract 
fact that a bicycle is built from atoms doesn’t tell you how to build a bicycle out of atoms—which atoms to use and 
where to put them. Similarly, the abstract knowledge that biological neurons implement human intelligence does not 
explain human intelligence. The classical hype of early neural networks, that they used “the same parallel 
architecture as the human brain,” should, at most, have been a claim of using the same parallel architecture as an 
earthworm’s brain. (And given the complexity of biological neurons, the claim would still have been wrong.)
```

Or as Tooby and Cosmides (1992) put it:

```markdown
The science of understanding living organization is very different from physics or chemistry, where parsimony makes 
sense as a theoretical criterion. The study of organisms is more like reverse engineering, where one may be dealing 
with a large array of very different components whose heterogenous organization is explained by the way in which 
they interact to produce a functional outcome. Evolution, the constructor of living organisms, has no privileged 
tendency to build into designs principles of operation that are simple and general.
```

Here’s the more obvious kind of “physics envy” in AI:

```markdown
The field of Artificial Intelligence suffers from a heavy, lingering dose of genericity and black-box, blank-slate, 
tabula-rasa concepts seeping in from the Standard Social Sciences Model (SSSM) identified by Tooby and Cosmides 
(1992). The general project of liberating AI from the clutches of the SSSM is more work than I wish to undertake in 
this paper, but one problem that must be dealt with immediately is physics envy. The development of physics over the 
last few centuries has been characterized by the discovery of unifying equations which neatly underlie many complex 
phenomena. Most of the past fifty years in AI might be described as the search for a similar unifying principle 
believed to underlie the complex phenomenon of intelligence.

Physics envy in AI is the search for a single, simple underlying process, with the expectation that this one discovery 
will lay bare all the secrets of intelligence. The tendency to treat new approaches to AI as if they were new theories 
of physics may at least partially explain AI’s past history of overpromise and oversimplification. Attributing all 
the vast functionality of human intelligence to some single descriptive facet—that brains are “parallel,” or 
“distributed,” or “stochastic”; that minds use “deduction” or “induction”— results in a failure (an overhyped failure) 
as the project promises that all the functionality of human intelligence will slide out from some simple principle.
```

And here’s the more subtle one:

```markdown
The effects of physics envy can be more subtle; they also appear in the lack of interaction between AI projects. 
Physics envy has given rise to a series of AI projects that could only use one idea, as each new hypothesis for 
the one true essence of intelligence was tested and discarded.

Douglas Lenat’s AM and Eurisko programs (Lenat 1983)—though the results were controversial and may have been mildly 
exaggerated (Ritchie and Hanna 1984)—used nonetheless very intriguing and fundamental design patterns to deliver 
significant and unprecedented results. Despite this, the design patterns of Eurisko, such as self-modifying 
decomposable heuristics, have seen almost no reuse in later AIs. Even Lenat’s subsequent Cyc project (Lenat, Prakash, 
and Shepherd 1985) apparently does not reuse the ideas developed in Eurisko.

From the perspective of a modern-day programmer, accustomed to hoarding design patterns and code libraries, the 
lack of crossfertilization is a surprising anomaly. One would think that self-optimizing heuristics would be useful 
as an external tool, e.g. for parameter tuning, even if the overall cognitive architecture did not allow for the 
internal use of such heuristics.
```

A related point:

```markdown
Leaving out key design elements, without replacement, on the basis of the mistaken belief that they are not 
relevant to general intelligence, is an error that displays a terrifying synergy with “physics envy.” In extreme 
cases—and most historical cases have been extreme—the design ignores everything about the human mind except one 
characteristic (logic, distributed parallelism, fuzziness, etc.), which is held to be “the key to intelligence.” 
(On my more pessimistic days I sometimes wonder if successive fads are the only means by which knowledge of a 
given feature of human intelligence becomes widespread in AI.)
```

On the importance of emphasizing (the right kind of) “supersystem design” in developing general intelligence:

```markdown
I argue strongly for “supersystems,” but I do not believe that “supersystems” are the necessary and sufficient 
Key to AI. General intelligence requires the right supersystem, with the right cognitive subsystems, doing the 
right things in the right way. Humans are not intelligent by virtue of being “supersystems,” but by virtue of 
being a particular supersystem which implements human intelligence. I emphasize supersystem design because I 
believe that the field of AI has been crippled by the wrong kind of simplicity—a simplicity which, as a design 
constraint, rules out workable designs for intelligence; a simplicity which, as a methodology, rules out 
incremental progress toward an understanding of general intelligence; a simplicity which, as a viewpoint, renders 
most of the mind invisible except for whichever single aspect is currently promoted as the Key to AI.
```

On the need to focus on, not design simplicity, but “sufficiently complex explanations” and “usefully deep designs”:

```markdown
If the quest for design simplicity is to be “considered harmful,” what should replace it? I believe that rather 
than simplicity, we should pursue sufficiently complex explanations and usefully deep designs. In ordinary 
programming, there is no reason to assume a priori that the task is enormously large. In AI the rule should be 
that the problem is always harder and deeper than it looks, even after you take this rule into account.

Knowing that the task is large does not enable us to meet the challenge just by making our designs larger or more 
complicated; certain specific complexity is required, and complexity for the sake of complexity is worse than 
useless. Nonetheless, the presumption that we are more likely to underdesign than overdesign implies a different 
attitude towards design, in which victory is never declared, and even after a problem appears to be solved, we go 
on trying to solve it.

If this creed were to be summed up in a single phrase, it would be: “Necessary but not sufficient.” In accordance 
with this creed, it should be emphasized that supersystems thinking is only one part of a larger paradigm, and that 
an open-ended design process is itself “necessary but not sufficient.” These are first steps toward AI, but not 
the only first steps, and certainly not the last steps.
```

On his candidate for “the most debilitating mistake in AI”:

```markdown
If I had to pick one single mistake that has been the most debilitating in AI, it would be implementing a process 
too close to the token level—trying to implement a high-level process without implementing the underlying layers 
of organization. Many proverbial AI pathologies result at least partially from omitting lower levels of 
organization from the design.

Take, for example, that version of the “frame problem”—sometimes also considered a form of the “commonsense 
problem”—in which intelligent reasoning appears to require knowledge of an infinite number of special cases. 
Consider a CPU which adds two 32-bit numbers. The higher level consists of two integers which are added to produce 
a third integer. On a lower level, the computational objects are not regarded as opaque “integers,” but as ordered 
structures of 32 bits. When the CPU performs an arithmetic operation, two structures of 32 bits collide, under 
certain rules which govern the local interactions between bits, and the result is a new structure of 32 bits. Now 
consider the woes of a research team, with no knowledge of the CPU’s underlying implementation, that tries to 
create an arithmetic “expert system” by encoding a vast semantic network containing the “knowledge” that two and 
two make four, twenty-one and sixteen make thirty-seven, and so on. This giant lookup table requires eighteen 
billion billion entries for completion. 

In this hypothetical world where the lower-level process of addition is not 
understood, we can imagine the “common-sense” problem for addition; the launching of distributed Internet projects 
to “encode all the detailed knowledge necessary for addition”; the frame problem for addition; the philosophies of 
formal semantics under which the LISP token thirty-seven is meaningful because it refers to thirty-seven objects in 
the external world; the design principle that the token thirty-seven has no internal complexity and is rather given 
meaning by its network of relations to other tokens; the “number grounding problem”; the hopeful futurists arguing 
that past projects to create Artificial Addition failed because of inadequate computing power; and so on.
```

On the difference between human-written programs and computation in neurons:

```markdown
Another class of problem stems from “porting” across the extremely different programming styles of evolution 
versus human coding. Human-written programs typically involve a long series of chained dependencies that intersect 
at single points of failure— “crystalline” is a good term to describe most human code. Computation in neurons 
has a different character. Over time our pictures of biological neurons have evolved from simple integrators of 
synaptic inputs that fire when a threshold input level is reached, to sophisticated biological processors with 
mixed analog-digital logics, adaptive plasticity, dendritic computing, and functionally relevant dendritic and 
synaptic morphologies (Koch and Segev 2000). What remains true is that, from an algorithmic perspective, neural 
computing uses roughly arithmetical operations14 that proceed along multiple intertwining channels in which 
information is represented redundantly and processed stochastically. Hence, it is easier to “train” neural 
networks—even nonbiological connectionist networks—than to train a piece of human-written code. Flipping a random 
bit inside the state of a running program, or flipping a random bit in an assembly-language instruction, has a 
much greater effect than a similar perturbation of a neural network. For neural networks the fitness landscapes 
are smoother. Why is this? Biological neural networks need to tolerate greater environmental noise (data error) 
and processor noise (computational error), but this is only the beginning of the explanation.

Smooth fitness landscapes are a useful, necessary, and fundamental outcome of evolution. Every evolutionary 
success starts as a mutation—an error—or as a novel genetic combination. A modern organism, powerfully adaptive 
with a large reservoir of genetic complexity, necessarily possesses a very long evolutionary history; that is, 
the genotype has necessarily passed through a very large number of successful mutations and recombinations along 
the road to its current form… “Smooth fitness landscapes” imply, among other things, that a small perturbation in 
the program code (genetic noise), in the input (environmental noise), or in the state of the executing program 
(processor noise), is likely to produce at most a small degradation in output quality. In most human-written code, 
a small perturbation of any kind usually causes a crash. Genomes are built by a cumulative series of point 
mutations and random recombinations. Human-written programs start out as high-level goals which are translated, 
by an extended serial thought process, into code. A perturbation to human-written code perturbs the code’s final 
form, rather than its first cause, and the code’s final form has no history of successful mutation. The thoughts 
that gave rise to the code probably have a smooth fitness metric, in the sense that a slight perturbation to the 
programmer’s state of mind will probably produce code that is at most a little worse, and possibly a little better. 
Human thoughts, which are the original source of human-written code, are resilient; the code itself is fragile.
```

<a name="#miscellaneous"></a>

## Miscellaneous
([overview](#overview))

From [Marc Andreessen gives the career advice that nobody wants to hear](https://www.businessinsider.com/andreessen-whatever-you-do-dont-follow-your-passion-2014-5/?IR=T):

```markdown
"Do what you love" / "Follow your passion" is dangerous and destructive career advice. We tend to hear it 
from (a) Highly successful people who (b) Have become successful doing what they love. The problem is that 
we do NOT hear from people who have failed to become successful by doing what they love. Particularly pernicious 
problem in tournament-style fields with a few big winners lots of losers: media, athletics, startups. Better 
career advice may be "Do what contributes" -- focus on the beneficial value created for other people vs just 
one's own ego. People who contribute the most are often the most satisfied with what they do -- and in fields 
with high renumeration, make the most $. Perhaps difficult advice since requires focus on others vs oneself -- 
perhaps bad fit with endemic narcissism in modern culture? Requires delayed gratification -- may toil for many 
years to get the payoff of contributing value to the world, vs short-term happiness.
```

Razib Khan:

```markdown
But, there's another problem, and that is the fact that statistical and probabilistic thinking is a real 
damper on "intellectual" conversation. By this, I mean that there are many individuals who wish to make 
inferences about the world based on data which they observe, or offer up general typologies to frame a 
subsequent analysis. These individuals tend to be intelligent and have college degrees. Their discussion 
ranges over topics such as politics, culture and philosophy. But, introduction of questions about the 
moments about the distribution, or skepticism as to the representativeness of their sample, and so on, tends 
to have a chilling affect on the regular flow of discussion. While the average human being engages mostly 
in gossip and interpersonal conversation of some sort, the self-consciously intellectual interject a bit of 
data and abstraction (usually in the form of jargon or pithy quotations) into the mix. But the raison d'etre 
of the intellectual discussion is basically signaling and cuing; in other words, social display. No one 
really cares about the details and attempting to generate a rigorous model is really beside the point. Trying 
to push the N much beyond 2 or 3 (what you would see in a college essay format) will only elicit eye-rolling 
and irritation.
```

David Wong:

```markdown
It's not that clean energy will never happen -- it totally will. It's just that it won't come from a 
wild-haired scientist running out of his basement screaming, "Eureka! I've discovered how to get limitless 
clean energy from common seawater!" Instead, it will come from thousands of scientists publishing unreadable 
studies with titles like "Assessing Effectiveness and Costs of Asymmetrical Methods of Beryllium Containment 
in Gen 4 Liquid Fluoride Thorium Reactors When Factoring for Cromulence Decay." The world will be saved by a 
series of boring, incremental advances that chip away at those technical challenges one tedious step at a time.

But nobody wants to read about that in their morning Web browsing. We want to read that while we were sleeping, 
some unlikely hero saved the world. Or at least cured cancer.
```

William T. Powers:

```markdown
One thing I have advocated, without much success, is that children be taught social rules (when they are 
ready) in exactly the same way they are taught and teach each other games. The point is not whether the 
rules are right or wrong. Are the rules of 5-card stud poker or hopscotch right or wrong? It's that we're 
playing a certain game here, and there are rules to this game just as in any other game. If you want to 
be in the game, then you have to learn how to play it. Different groups of people play different games 
(different rules = different game), so if you want to play in different groups, you have to learn the 
games they play. When you develop the levels of understanding above the rule level, you'll be able to 
understand all games, and be able to join in anywhere. You won't be stuck knowing how to play only one game.

My problem with selling this idea is that people tend to think that their game is the only right one. 
In fact, being told that they are playing a game with arbitrary rules is insulting or frightening. They 
want to believe that the rules they know are the ones that everyone ought to play by; they even set 
up systems of punishment and reward to make sure that nobody tries to play a different game. They turn 
the game into something that is deadly serious, and so my idea simply seems frivolous instead of liberating.
```

Patrick McKenzie, "Some Perspective on the Japan Earthquake":

```markdown
The story of Japanese railways during the earthquake and tsunami is the story of an unceasing drumbeat 
of everything going right [...] The overwhelming response of Japanese engineering to the challenge posed 
by an earthquake larger than any in the last century was to function exactly as designed. Millions of 
people are alive right now because the system worked and the system worked and the system worked.

That this happened was, I say with no hint of exaggeration, one of the triumphs of human civilization. 
Every engineer in this country should be walking a little taller this week. We can’t say that too loudly, 
because it would be inappropriate with folks still missing and many families in mourning, but it doesn’t 
make it any less true.
```

Bruce Schneier:

```markdown
In our large, anonymous society, it's easy to forget moral and reputational pressures and concentrate on 
legal pressure and security systems. This is a mistake; even though our informal social pressures fade 
into the background, they're still responsible for most of the cooperation in society.
```

Razib Khan, [Reification is alright by me](http://blogs.discovermagazine.com/gnxp/2012/05/reification-is-alright-by-me/):

```markdown
The categories and classes we construct are simply the semantic sugar which makes the reality go down 
easier. They should never get confused for the reality that is, the reality which we perceive but darkly 
and with biased lenses. The hyper-relativists and subjectivists who are moderately fashionable in some 
humane studies today are correct to point out that science is a human construction and endeavor. Where 
they go wrong is that they are often ignorant of the fact that the orderliness of many facets of nature 
is such that even human ignorance and stupidity can be overcome with adherence to particular methods and 
institutional checks and balances. The predictive power of modern science, giving rise to modern 
engineering, is the proof of its validity. No talk or argumentation is needed. Boot up your computer. 
Drive your car.
```

Paul Graham, [A student's guide to startups](http://www.paulgraham.com/mit.html):

```markdown
The market doesn't give a shit how hard you worked. Users just want your software to do what they need, 
and you get a zero otherwise. That is one of the most distinctive differences between school and the 
real world: there is no reward for putting in a good effort. In fact, the whole concept of a "good effort" 
is a fake idea adults invented to encourage kids. It is not found in nature. 
```

From *Cryptonomicon*, by Neal Stephenson:

```markdown
Your younger nerd takes offense quickly when someone near him begins to utter declarative sentences, 
because he reads into it an assertion that he, the nerd, does not already know the information being 
imparted. 

But your older nerd has more self-confidence, and besides, understands that frequently people need to 
think out loud. 

And highly advanced nerds will furthermore understand that uttering declarative sentences whose contents 
are already known to all present is part of the social process of making conversation and therefore 
should not be construed as aggression under any circumstances.
```

Hastie Dawes, *Rational Choice in an Uncertain World*, pp. 67-8:

```markdown
A lot of outcomes about which we care deeply are not very predictable. For example, it is not comforting 
to members of a graduate school admissions committee to know that only 23% of the variance in later 
faculty ratings of a student can be predicted by a unit weighting of the student's undergraduate GPA, 
his or her GRE score, and a measure of the student's undergraduate institution selectivity -- but that 
is opposed to 4% based on those committee members' global ratings of the applicant. We want to predict 
outcomes important to us. It is only rational to conclude that if one method (a linear model) does not 
predict well, something else may do better. What is not rational -- in fact, it's irrational -- is to 
conclude that this "something else" necessarily exists and, in the absence of any positive supporting 
evidence, is intuitive global judgment.
```

David Friedman, *The Machinery of Freedom*:

```markdown
The person who says, as almost everyone does say, that human life is of infinite value, not to be measured 
in mere material terms, is talking palpable, if popular, nonsense. If he believed that of his own life, he 
would never cross the street, save to visit his doctor or to earn money for things necessary to physical 
survival. He would eat the cheapest, most nutritious food he could find and live in one small room, saving 
his income for frequent visits to the best possible doctors. He would take no risks, consume no luxuries, 
and live a long life. If you call it living. If a man really believed that other people's lives were 
infinitely valuable, he would live like an ascetic, earn as much money as possible, and spend everything 
not absolutely necessary for survival on CARE packets, research into presently incurable diseases, and 
similar charities.

In fact, people who talk about the infinite value of human life do not live in either of these ways. They 
consume far more than they need to support life. They may well have cigarettes in their drawer and a sports 
car in the garage. They recognize in their actions, if not in their words, that physical survival is only 
  one value, albeit a very important one, among many.
```

From [Oglaf](http://oglaf.com/bugfuck/epilogue/):

```markdown
In some species of Anglerfish, the male is much smaller than the female and incapable of feeding 
independently. To survive he must smell out a female as soon as he hatches. He bites into her releasing 
an enzime which fuses him to her permanently. He lives off her blood for the rest of his life, providing 
her with sperm whenever she needs it. Females can have multiple males attached. 

The moral is simple: males are parasites, women are sluts. 

Ha! Just kidding! The moral is don't treat actual animal behavior like a fable. Generally speaking, 
animals have no interest in teaching you anything.
```

Terry Pratchett, *Unseen Academicals*:

```markdown
The Patrician took a sip of his beer. "I have told this to few people, gentlemen, and I suspect I never 
will again, but one day when I was a young boy on holiday in Uberwald I was walking along the bank of a 
stream when I saw a mother otter with her cubs. A very endearing sight, I'm sure you will agree, and even 
as I watched, the mother otter dived into the water and came up with a plump salmon, which she subdued 
and dragged onto a half-submerged log. As she ate it, while of course it was still alive, the body split 
and I remember to its day the sweet pinkness of its roes as they spilled out, much to the delight of the 
baby otters who scrambled over themselves to feed on the delicacy. 

One of nature's wonders, gentlemen: mother and children dining upon mother and children. 

And that's when I first learned about evil. It is built in to the very nature of the universe. Every world 
spins in pain. If there is any kind of supreme being, I told myself, it is up to all of us to become his 
moral superior."
```

Megan McArdle, [Only stupid people call people stupid](https://www.bloomberg.com/opinion/articles/2014-08-12/only-stupid-people-call-people-stupid):

```markdown
I’m always fascinated by the number of people who proudly build columns, tweets, blog posts or Facebook 
posts around the same core statement: “I don’t understand how anyone could (oppose legal abortion/support 
a carbon tax/sympathize with the Palestinians over the Israelis/want to privatize Social Security/insert 
your pet issue here)." It’s such an interesting statement, because it has three layers of meaning.

The first layer is the literal meaning of the words: *I lack the knowledge and understanding to figure this 
out*. But the second, intended meaning is the opposite: *I am such a superior moral being that I cannot even 
imagine the cognitive errors or moral turpitude that could lead someone to such obviously wrong conclusions*. 
And yet, the third, true meaning is actually more like the first: *I lack the empathy, moral imagination or 
analytical skills to attempt even a basic understanding of the people who disagree with me*.

In short, “I’m stupid.” Something that few people would ever post so starkly on their Facebook feeds.
```

Steven Pinker, [The trouble with Harvard](https://newrepublic.com/article/119321/harvard-ivy-league-should-judge-students-standardized-tests): 

```markdown
A skilled professional I know had to turn down an important freelance assignment because of a recurring 
commitment to chauffeur her son to a resumé-building “social action” assignment required by his high 
school. This involved driving the boy for 45 minutes to a community center, cooling her heels while he 
sorted used clothing for charity, and driving him back—forgoing income which, judiciously donated, could 
have fed, clothed, and inoculated an African village. The dubious “lessons” of this forced labor as an 
overqualified ragpicker are that children are entitled to treat their mothers’ time as worth nothing, 
that you can make the world a better place by destroying economic value, and that the moral worth of 
an action should be measured by the conspicuousness of the sacrifice rather than the gain to the beneficiary.
```

Douglas Hofstadter on the necessary strangeness of scientific explanations:

```markdown
It is no accident, I would maintain, that quantum mechanics is so wildly counterintuitive. Part of the 
nature of explanation is that it must eventually hit some point where further probing only increases 
opacity rather than decreasing it. 

Consider the problem of understanding the nature of solids. You might wonder where solidity comes form. 
What if someone said to you, "The ultimate basis of this brick's solidity is that it is composed of a 
stupendous number of eensy weensy bricklike objects that themselves are rock-solid"? You might be 
interested to learn that bricks are composed of micro-bricks, but the initial question - "What accounts 
for solidity?" - has been thoroughly begged. What we ultimately want is for solidity to vanish, to 
dissolve, to disintegrate into some totally different kind of phenomenon with which we have no experience. 
Only then, when we have reached some completely novel, alien level will we feel that we have really made 
progress in explaining the top-level phenomenon.

...

I first saw this thought expressed in the stimulating book Patterns of Discovery by Norwood Russell 
Hanson. Hanson attributes it to a number of thinkers, such as Isaac Newton, who wrote, in his famous 
work Opticks: "The parts of all homogeneal hard Bodies which fully touch one another, stick together 
very strongly. And for explaining how this may be, some have invented hooked Atoms, which is begging 
the Question." Hanson also quotes James Clerk Maxwell (from an article entitled "Atom"): "We may 
indeed suppose the atom elastic, but this is to endow it with the very property for the explanation 
of which... the atomic constitution was originally assumed." Finally, here is a quote Hanson provides 
from Werner Heisenberg himself: "If atoms are really to explain the origin of color and smell of 
visible material bodies, then they cannot possess properties like color and smell." 

So, although it is not an original thought, it is useful to bear in mind that "greenness disintegrates".
```

<a name="#back"></a>
