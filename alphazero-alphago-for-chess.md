Been waiting for this ever since AlphaGo defeated Fan Hui :D 
Abstract from Hassabis and friends’ [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning 
Algorithm](https://arxiv.org/abs/1712.01815) (emphasis _gleefully_ mine):

```markdown
The game of chess is the most widely-studied domain in the history of artificial intelligence. 
The strongest programs are based on a combination of sophisticated search techniques, domain-specific 
adaptations, and handcrafted evaluation functions that have been refined by human experts over several 
decades.

In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by 
tabula rasa reinforcement learning from games of self-play. In this paper, **we generalise this approach 
into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many 
challenging domains**.

Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 
24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and 
convincingly defeated a world-champion program in each case.
```

Music to my ears, this:

```markdown
The AlphaZero algorithm is a more generic version of the AlphaGo Zero algorithm that was first introduced in the 
context of Go (29). It replaces the handcrafted knowledge and domain-specific augmentations used in traditional 
game-playing programs with deep neural networks and a tabula rasa reinforcement learning algorithm.
```

You _generalize_ that algo, my good man!

Before reading the rest of the paper: I’m _really_ hoping the “world champion program” is Stockfish or Komodo. 
(I guess Rybka would do too.)

```markdown
A landmark for artificial intelligence was achieved in 1997 when Deep Blue defeated the human world champion (9). 
Computer chess programs continued to progress steadily beyond human level in the following two decades. These 
programs evaluate positions using features handcrafted by human grandmasters and carefully tuned weights, 
combined with a high-performance alpha-beta search that expands a vast search tree using a large number of 
clever heuristics and domain-specific adaptations. In the Methods we describe these augmentations, focusing on 
the 2016 Top Chess Engine Championship (TCEC) world-champion Stockfish (25); other strong chess programs, 
including Deep Blue, use very similar architectures (9, 21).
```

Stockfish it is!

How does AlphaZero improve upon AlphaGo Zero, the strongest go player ever built?

```markdown
1. AlphaGo Zero estimates and optimises the probability of winning, assuming binary win/loss outcomes. AlphaZero 
instead estimates and optimises the expected outcome, taking account of draws or potentially other outcomes.

2. The rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in 
two ways. First, training data was augmented by generating 8 symmetries for each position. Second, during MCTS, 
board positions were transformed using a randomly selected rotation or reflection before being evaluated by the 
neural network, so that the MonteCarlo evaluation is averaged over different biases. The rules of chess and shogi 
are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does 
not transform the board position during MCTS.

3. In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. After each 
iteration of training, the performance of the new player was measured against the best player; if it won by a margin 
of 55% then it replaced the best player and self-play games were subsequently generated by this new player. In 
contrast, AlphaZero simply maintains a single neural network that is updated continually, rather than waiting for 
an iteration to complete. Self-play games are generated by using the latest parameters for this neural network, 
omitting the evaluation step and the selection of best player.

4. AlphaGo Zero tuned the hyper-parameter of its search by Bayesian optimisation. In AlphaZero we reuse the same 
hyper-parameters for all games without game-specific tuning. The sole exception is the noise that is added to the 
prior policy to ensure exploration (29); this is scaled in proportion to the typical number of legal moves for 
that game type.
```

Enter the monster.

```markdown
Figure 1 shows the performance of AlphaZero during self-play reinforcement learning, as a function of training 
steps, on an Elo scale (10). In chess, AlphaZero outperformed Stockfish after just 4 hours (300k steps); in shogi, 
AlphaZero outperformed Elmo after less than 2 hours (110k steps); and in Go, AlphaZero outperformed AlphaGo Lee 
(29) after 8 hours (165k steps).2

We evaluated the fully trained instances of AlphaZero against Stockfish, Elmo and the previous version of AlphaGo 
Zero (trained for 3 days) in chess, shogi and Go respectively, playing 100 game matches at tournament time controls 
of one minute per move. AlphaZero and the previous AlphaGo Zero used a single machine with 4 TPUs. Stockfish and 
Elmo played at their strongest skill level using 64 threads and a hash size of 1GB. AlphaZero convincingly defeated 
all opponents, losing zero games to Stockfish and eight games to Elmo (see Supplementary Material for several example 
games), as well as defeating the previous version of AlphaGo Zero (see Table 1).

We also analysed the relative performance of AlphaZero’s MCTS search compared to the state-of-the-art alpha-beta 
search engines used by Stockfish and Elmo. AlphaZero searches just 80 thousand positions per second in chess and 
40 thousand in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the 
lower number of evaluations by using its deep neural network to focus much more selectively on the most promising 
variations – arguably a more “human-like” approach to search, as originally proposed by Shannon (27). Figure 2 
shows the scalability of each player with respect to thinking time, measured on an Elo scale, relative to Stockfish 
or Elmo with 40ms thinking time. AlphaZero’s MCTS scaled more effectively with thinking time than either Stockfish 
or Elmo, calling into question the widely held belief (4, 11) that alpha-beta search is inherently superior in 
these domains.3

Finally, we analysed the chess knowledge discovered by AlphaZero. Table 2 analyses the most common human openings 
(those played more than 100,000 times in an online database of human chess games (1)). Each of these openings is 
independently discovered and played frequently by AlphaZero during self-play training. When starting from each 
human opening, AlphaZero convincingly defeated Stockfish, suggesting that it has indeed mastered a wide spectrum 
of chess play.
```

Seriously, this is worth summarizing (emphasis again mine):

```markdown
**The game of chess represented the pinnacle of AI research over several decades**. State-of-the-art 
programs are based on powerful engines that search many millions of positions, leveraging handcrafted 
domain expertise and sophisticated domain adaptations. **AlphaZero is a generic reinforcement learning 
algorithm** – originally devised for the game of Go – that achieved **superior results within a few hours, 
searching a thousand times fewer positions, given no domain knowledge except the rules of chess**. 
Furthermore, **the same algorithm was applied without modification to the more challenging game of shogi, 
again outperforming the state of the art within a few hours**.
```

There were a couple more interesting tidbits. 

1. Shogi is harder than chess: 
```markdown
Shogi is a significantly harder game, in terms of computational complexity, than chess (2, 14): it is played on a 
larger board, and any captured opponent piece changes sides and may subsequently be dropped anywhere on the board. 
The strongest shogi programs, such as Computer Shogi Association (CSA) world-champion Elmo, have only recently 
defeated human champions (5). These programs use a similar algorithm to computer chess programs, again based on a 
highly optimised alpha-beta search engine with many domain-specific adaptations.
```

2. Chess and shogi are less innately suited to AlphaGo’s neural network architecture, because the rules 
    
   1. are position-dependent instead of translationally invariant; 
   2. are asymmetric instead of being rotationally and reflectionally symmetric; 
   3. allow long-range interactions, etc:

```markdown
Go is well suited to the neural network architecture used in AlphaGo because the rules of the game are 
translationally invariant (matching the weight sharing structure of convolutional networks), are defined in 
terms of liberties corresponding to the adjacencies between points on the board (matching the local structure 
of convolutional networks), and are rotationally and reflectionally symmetric (allowing for data augmentation 
and ensembling). Furthermore, the action space is simple (a stone may be placed at each possible location), 
and the game outcomes are restricted to binary wins or losses, both of which may help neural network training.

Chess and shogi are, arguably, less innately suited to AlphaGo’s neural network architectures. The rules are 
position-dependent (e.g. pawns may move two steps forward from the second rank and promote on the eighth rank) 
and asymmetric (e.g. pawns only move forward, and castling is different on kingside and queenside). The rules 
include long-range interactions (e.g. the queen may traverse the board in one move, or checkmate the king from 
the far side of the board). The action space for chess includes all legal destinations for all of the players’ 
pieces on the board; shogi also allows captured pieces to be placed back on the board. Both chess and shogi may 
result in draws in addition to wins and losses; indeed it is believed that the optimal solution to chess is a 
draw (17, 20, 30)
```
