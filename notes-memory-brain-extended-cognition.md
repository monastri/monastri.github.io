*[Word count](https://wordcounter.net/): 14,500*

## What is this?

This is the "memory, the brain and extended cognition" section of [this notebook](https://github.com/monastri/monastri.github.io/blob/master/quotes.md), last updated Apr 6, 2019, which got so big (1.2 million char) that GitHub refused to render the whole page anymore, ruining my original dream of having my entire notebook in one long page for two purposes: (1) zero-latency clickthrough to make up for my working memory's sand-thru-sieve transience; (2) lower activation energy for continuous document-wide refactoring, to aid recall and cross-domain interlinking.

That said, this is a "living document", a "perpetual draft" in [the style of Gwern](https://www.gwern.net/About#long-content). I'm particulary taken by the following quote: 

```markdown
I have read blogs for many years and most blog posts are the triumph of the hare over the tortoise.
They are meant to be read by a few people on a weekday in 2004 and never again, and are quickly
abandoned—and perhaps as Assange says, not a moment too soon. (But isn’t that sad? Isn’t it a 
terrible ROI for one’s time?) On the other hand, the best blogs always seem to be building something:
they are rough drafts—works in progress. (EY's early contributions to LW is an example; Robin 
Hanson's OB blog is the *anti*-example.) 

I did not wish to write a blog. Then what? More than just “evergreen content”, what would constitute 
Long Content as opposed to the existing culture of Short Content? How does one live in a Long Now 
sort of way?

My answer is that one uses such a framework to work on projects that are too big to work on normally
or too tedious. ...Knowing your site will survive for decades to come gives you the mental wherewithal
to tackle long-term tasks like gathering information for years, and such persistence can be useful --
if one holds onto every glimmer of genius for years, then even the dullest person may look a bit like
a genius himself. Half the challenge of fighting procrastination is the pain of starting—I find when 
I actually get into the swing of working on even dull tasks, it’s not so bad. 

So this suggests a solution: never start. Merely have perpetual drafts, which one tweaks from time to
time. And the rest takes care of itself.
```

There's also this quote from Paul Graham's essay [You weren't meant to have a boss](http://www.paulgraham.com/boss.html), [paraphrased](https://meltingasphalt.com/about/) by Kevin Simler:

```markdown
An obstacle downstream propagates upstream. If you're not allowed to implement new ideas, 
you stop having them. And vice versa: when you can do whatever you want, you have more 
ideas about what to do. So [keeping a blog] makes your brain more powerful in the same way
a low-restriction exhaust system makes an engine more powerful.
```

This is my first experiment in Gwern's vein. The quotes here have been collected over more than half a decade, albeit in different pages. I intend for them to shape my worldview; doing so like this allows, or so I hope, the shaping to be more fine-grained and guided than the recency-weighted randomness of normal worldview-shaping. (Or at least that was the original intent before I had to break up the notebook. Now I'm not so sure I can do this.)

I also really, *really* hate experiences of the [Jeremy Bentham type](https://github.com/monastri/monastri.github.io/blob/master/notes-amazing-people.md#Jeremy-bentham). This document is intended to prevent them from happening again.

Besides Gwern Branwen, [Cosma Shalizi's notebooks](http://bactra.org/notebooks/) (indeed his [entire oeuvre](http://bactra.org/)) are another major inspiration behind this document. 

<a name="#overview"></a>

## Overview

I've sorted the quotes below into the following categories. This is a provisional taxonomy, subject to perpetual refactoring. The reason it has a [Borgesian flavor](https://github.com/monastri/monastri.github.io/blob/master/poetry.md#the-celestial-emporium-of-benevolent-knowledge) is that it's meant to aid recall and idea-building. The categories are ordered alphabetically; the actual quotes (the top-level categories that is) are chronologically added.

1. [Augmenting long-term memory](#augmenting-long-term-memory), e.g. Anki
3. [Cognitive science](#cognitive-science)
2. [Errors vs bugs](#errors-vs-bugs)
2. [Externalizing the brain](#external-brain), e.g. Google, writing, Cosma on collective cognition
2. [Important findings on learning](#important-findings-on-learning), ft. 25 principles by U of Missouri
3. [Names matter](#names-matter)
3. [Polymathy, or being a generalist](#polymathy)
	1. [Semicolon-shaped people](#Semicolon-shaped-people)
2. [Procedural vs declarative memory](#procedural-vs-declarative-memory)
4. [Steve Yegge on memory](#Yegge-on-memory)
5. [Wisdom](#wisdom)
	6. [The predictive uselessness of folk wisdom](#The-predictive-uselessness-of-folk-wisdom) 
	

----------------------------------

<a name="#errors-vs-bugs"></a>
## Errors vs bugs
([overview](#overview))

Sarah Constantin is one of my favorite writers -- she appears a *lot* in these notebooks. She has a blog, [Otium](https://srconstantin.wordpress.com/), and goes by celandine13 on LiveJournal. This quote on the *error vs bug model of learning* comes from her LJ essay [Errors vs Bugs and the End of Stupidity](https://celandine13.livejournal.com/33599.html).

```markdown
A common mental model for performance is what I'll call the "error model."  In the error model, a person's 
performance of a musical piece (or performance on a test) is a perfect performance plus some random error.  
You can literally think of each note, or each answer, as x + c times epsilon_i, where x is the correct note 
/ answer, and epsilon_i is a random variable, iid Gaussian or something.  Better performers have a lower 
error rate c.  Improvement is a matter of lowering your error rate.  This, or something like it, is the model 
that underlies school grades and test scores. Your grade is based on the percent you get correct.  Your 
performance is defined by a single continuous parameter, your accuracy.

But we could also consider the "bug model" of errors.  A person taking a test or playing a piece of music is 
executing a program, a deterministic procedure.  If your program has a bug, then you'll get a whole class of 
problems wrong, consistently.  Bugs, unlike error rates, can't be quantified along a single axis as less or 
more severe.  A bug gets everything that it affects wrong.  And fixing bugs doesn't improve your performance 
in a continuous fashion; you can fix a "little" bug and immediately go from getting everything wrong to 
everything right.  You can't really describe the accuracy of a buggy program by the percent of questions it 
gets right; if you ask it to do something different, it could suddenly go from 99% right to 0% right.  You 
can only define its behavior by isolating what the bug does.

Often, I think mistakes are more like bugs than errors.  My clinkers weren't random; they were in specific 
places, because I had sub-optimal fingerings in those places.  A kid who gets arithmetic questions wrong 
usually isn't getting them wrong at random; there's something missing in their understanding, like not 
getting the difference between multiplication and addition.  Working generically "harder" doesn't fix bugs 
(though fixing bugs does require work). 

Once you start to think of mistakes as deterministic rather than random, as caused by "bugs" (incorrect 
understanding or incorrect procedures) rather than random inaccuracy, a curious thing happens.

You stop thinking of people as "stupid."

Tags like "stupid," "bad at X", "sloppy," and so on, are ways of saying "You're performing badly and I 
don't know why." Once you move it to "you're performing badly because you have the wrong fingerings," or 
"you're performing badly because you don't understand what a limit is," it's no longer a vague personal 
failing but a causal necessity. Anyone who never understood limits will flunk calculus. 

It's not you, it's the bug.
```

The rest of the linked article is *fantastic*. I had trouble quoting it because I felt like quoting everything.

<a name="#important-findings-on-learning"></a>
## Important findings on learning
([overview](#overview))

From the University of Missouri's [25 Learning Principles to Guide Pedagogy and the
Design of Learning Environments](http://whaaales.com/25principlesoflearning.pdf). I only picked those I found interesting. 

Contiguity Effects:

```markdown
Ideas that need to be associated should be presented contiguously in space and time in the
multimedia learning environment. For example, the verbal label for a picture needs to be placed
spatially near the picture on the display, not on the other side of the screen. An explanation of an
event should be given when the event is depicted rather than many minutes, hours, or days later.
```

Perceptual-motor Grounding:

```markdown
Whenever a concept is first introduced, it is important to ground it in a concrete
perceptual-motor experience. The learner will ideally visualize a picture of the concept, will be
able to manipulate its parts and aspects, and will observe how it functions over time. The teacher
and learner will also gain a common ground (shared knowledge) of the learning material.

Perceptual-motor experience is particularly important when there is a need for precision, such as
getting directions to find a spatial location. For example, a course in statistics is not grounded in
perceptual-motor experience when the teacher presents symbols and formulae that have no
meaning to the student and cannot be visualized
```

Dual Code and Multimedia Effects:

```markdown
Information is encoded and remembered better when it is delivered in multiple modes
(verbal and pictorial), sensory modalities (auditory and visual), or media (computers and
lectures) than when delivered in only a singe mode, modality, or medium. Dual codes provide
richer and more varied representations that allow more memory retrieval routes. 

However, the amount of information should not overwhelm the learner because attention is 
split or cognitive capacities are overloaded.
```

Testing Effect:

```markdown
There are direct and indirect effects of taking frequent tests. One indirect benefit is that
frequent testing keeps students constantly engaged in the material. Although students will learn
from testing without receiving feedback, there is less forgetting if students receive informative
feedback about their performance. Multiple tests slow forgetting better than a single test.
Formative assessment refers to the use of testing results to guide teachers in making decisions
about what to teach. Learners also benefit if they use test results as a guide for their own
learning. 
```

The best way to do this is via spaced repetition. It's the whole idea behind Anki flashcards. See also [here](#augmenting-long-term-memory), as well as the next section -- Spaced Effects:

```markdown
Spaced schedules of testing (like spaced schedules of studying) produce better long-term
retention than a single test. When a single test is administered immediately after learning,
students obtain high scores, but long-term retention is reduced with a single immediate test
relative to spaced testing. When a test is given immediately after learning has occurred, learners
still have the newly-learned information in a primary memory system and therefore obtain high
test scores. Both teachers and learners often misjudge their high scores on a test given
immediately after learning as evidence of good retention, when, in fact, long-term retention
suffers with this practice
```

Generation Effect:

```markdown
Learning is enhanced when learners produce answers compared to having them recognize
answers. Free recall or essay tests which require the test taker to generate answers with minimal
cues produce better learning than multiple choice tests in which the learner only needs to be able
to recognize correct answers. In fact, free recall tests produce as much learning as restudying the
material
```

Organization effects (part of the reason this document exists in the first place!):

```markdown
Outlining, integrating, and synthesizing information produces better learning than rereading
materials or other more passive strategies. Students frequently report that when they study they
reread materials they already read once. Strategies that require learners to be actively engaged
with the material to-be-learned produce better long-term retention than the passive act of reading.
Learners should develop their own mini-testing situations as they review, such as stating the
information in their own words (without viewing the text) and synthesizing information from
multiple sources, such as from class and textbooks.
```

Coherence effect (Robert Frost does this well in [his answers on Quora](https://www.quora.com/profile/Robert-Frost-1/answers)):

```markdown
The learner needs to get a coherent, well connected representation of the main ideas to be
learned. It is important to remove distracting, irrelevant material, even when the added
information is artistically appealing. Seductive details that do not address the main points to be
conveyed run the risk of consuming the learner’s attention and effort at the expense of their
missing the main points. 
```

Stories and example cases (see also [John Baez on why math publications should take a page from the storytelling skillbook](#why-math-is-boring)):

```markdown
Stories and other forms of narrative are easier to read, comprehend, and remember than
other types of learning materials. For many millennia, the primary way of passing wisdom down
from generation to generation was through stories. Stories have concrete characters, objects,
locations, plot, themes, emotions, and actions that bear some similarity to everyday experiences.
Many stories also convey a point or moral that can be generalized to many situations. Example
cases in a story-like format are persuasive, easy to comprehend, and very memorable. 
```

Negative suggestion effects:

```markdown
Just as people learn correct information with frequent testing, they also can learn wrong
information this way. For example, when incorrect alternatives on multiple choice tests are
presented, the wrong answers can be learned instead of the correct answers. This effect is also
found on short answer essay questions when students do not know the answers and use their
general knowledge about the field to construct a response that seems reasonable to them. In this
situation, learners recall their incorrect, but logically consistent response as being correct. These
effects can be reduced when learners receive feedback immediately after taking a test which
allows them to revise their memory and understanding without delay.
```

Desirable Difficulties (struggle promotes long-term recall -- true more so in math):
 
```markdown
Learning is enhanced when learners have to organize the information themselves or exert
additional effort during acquisition or retrieval than in conditions in which the information to be
learned or retrieved does not require effort. One possible explanation for this effect is that
learners create multiple retrieval paths which make the information more accessible at retrieval.
These practices slow initial learning, but promote long-term recall.
```

Explanation effects:

```markdown
Explanations consist of causal analyses of events, logical justifications of claims, and
functional rationales for actions. Explanations provide coherence to the material and justify why
information is relevant and important. Students may be prompted to give self-explanations of the
material through think aloud protocols or questioning tasks that elicit explanations that connect
the material to what they already know. Self-explanations and the activity of studying good
explanations facilitate deeper comprehension, learning, memory, and transfer. 
```

Deep questions:

```markdown
Deep explanations of material and reasoning are elicited by questions such as why, how,
what-if-and what-if not, as opposed to shallow questions that require the learner to simply fill in
missing words, such as who, what, where, and when. Training students to ask deep questions
facilitates comprehension of material from text and classroom lectures. The learner gets into the
mindset of having deeper standards of comprehension and the resulting representations are more
elaborate. 
```

Cognitive disequilibrium:

```markdown
Cognitive disequilibrium stimulates inquiry, curiosity, thinking, and deep questions,
which in turn lead to deeper learning. Cognitive disequilibrium occurs when there are obstacles
to goals, contradictions, conflicts, anomalous events, breakdown scenarios, salient gaps in
knowledge, uncertainty, equally attractive alternatives, and other types of impasses. When these
impasses occur, the learner needs to engage in reasoning, thought, problem solving, and planning
in route to restoring cognitive equilibrium. There is a higher incidence of deep questions,
thought, reasoning, and study efforts when learners undergo cognitive disequilibrium.
```

Goldilocks principle: 

```markdown
Assignments should not be too hard or two easy, but at the right level of difficulty for the
student’s level of skill or prior knowledge. The definition of the zone of proximal development
(ZPD) is a bit more technical: the difference in learning that occurs with versus without a
learning scaffold (e.g., tutor, teacher, text, and computer). Researchers have identified a number
of zones that reflect how much learning, memory, mastery, or satisfaction occurs along a
continuum of task difficulty and that is sensitive to individual differences among learners. When
the material is too easy for the learner, the student is not challenged and may get bored. When it
is too difficult, the student acquires very little and gets frustrated or tunes out
```

Anchored learning:

```markdown
Anchored learning occurs when students work in teams for several hours or days trying to
solve a challenging practical problem that matters to the student. The activity is linked to
background knowledge of the learner on a topic that is interesting. The problem is challenging,
so the learner needs to engage in problem solving and recruit multiple levels of knowledge and
skills. These activities are coherently organized around solving the practical problem. Examples
of anchored learning are problem-based curricula in medical schools where students work on
genuine medical cases and communities of practice where students try to solve problems of
pollution in their city.
```

<a name="#polymathy"></a>
## Polymathy
([overview](#overview))

Here's Eric Drexler on [how to understand everything](http://metamodern.com/2009/05/17/how-to-understand-everything-and-why/) and [how to learn about everything](http://metamodern.com/2009/05/27/how-to-learn-about-everything/).

I feel a sort of kinship with what people like Drexler are trying to do here, albeit for different reasons. To oversimplify by mapping my motivations in doing the same to the ‘carrot-and-stick’ model: my carrot is this intrinsic need to ‘see the Systems of the World’ (paraphrasing Neal Stephenson, evoking the unnamed protagonist in Ted Chiang’s Understand, etc); my stick is the pain of being blindsided by unknown unknowns (so I have to at least know the outlines of everything, if not their contents, and how they all fit together). A deeper understanding than the “teacher’s password” awareness of trivia competition champions and my high school self, but not much deeper, not so deep as to sacrifice breadth.

But I digress. Eric contends that we need "knowledge of extent and structure of human knowledge on a trans-disciplinary scale":

```markdown
Formal education in science and engineering centers on teaching facts and problem-solving skills in a 
series of narrow topics. It is true that a few topics, although narrow in content, have such broad 
application that they are themselves integrative: These include (at a bare minimum) substantial chunks 
of mathematics and the basics of classical mechanics and electromagnetism, with the basics of 
thermodynamics and quantum mechanics close behind.

Most subjects in science and engineering, however, are narrower than these, and advanced education 
means deeper and narrower education. What this kind of education omits is knowledge of extent and 
structure of human knowledge on a trans-disciplinary scale. This means understanding — in a particular,
limited sense — everything.
```

How to figure out the outlines of a field, and knowledge about knowledge:

```markdown
To avoid blunders and absurdities, to recognize cross-disciplinary opportunities, and to make sense of
new ideas, requires knowledge of at least the outlines of every field that might be relevant to the 
topics of interest. By knowing the outlines of a field, I mean knowing the answers, to some reasonable
approximation, to questions like these:

What are the physical phenomena?
What causes them?
What are their magnitudes?
When might they be important?
How well are they understood?
How well can they be modeled?
What do they make possible?
What do they forbid?

And even more fundamental than these are questions of knowledge about knowledge:

What is known today?
What are the gaps in what I know?
When would I need to know more to solve a problem?
How could I find what I need?

This sort of knowledge is a kind of specialty, really — a limited slice of learning, but oriented
crosswise. Because of this orientation, though, it provides leverage in integrating knowledge from
diverse sources.
```

Why care? Problem recognition is very important:

```markdown
It takes far less knowledge to recognize a problem than to solve it, yet in key respects, that bit 
of knowledge is more important: With recognition, a problem may be avoided, or solved, or an idea 
abandoned. Without recognition, a hidden problem may invalidate the labor of an hour, or a lifetime.
Lack of a little knowledge can be a dangerous thing.
```

Eric distinguishes between learning everything (which is impossible) and learning *about* everything (which is not):

```markdown
Note that the title above isn’t “how to learn everything”, but “how to learn about everything”. The 
distinction I have in mind is between knowing the inside of a topic in deep detail — many facts and 
problem-solving skills — and knowing the structure and context of a topic: essential facts, what 
problems can be solved by the skilled, and how the topic fits with others.

This knowledge isn’t superficial in a survey-course sense: It is about both deep structure and practical
applications. Knowing about, in this sense, is crucial to understanding a new problem and what must be 
learned in more depth in order to solve it. The cross-disciplinary reach of nanotechnology almost
demands this as a condition of competence.
```

Some advice on going about it:

```markdown
To intellectually ambitious students I recommend investing a lot of time in a mode of study that may 
feel wrong. An implicit lesson of classroom education is that successful study leads to good test 
scores, but this pattern of study is radically different. It cultivates understanding of a kind that 
won’t help pass tests — the classroom kind, that is:

Read and skim journals and textbooks that (at the moment) you only half understand. Include Science 
and Nature.

Don’t halt, dig a hole, and study a particular subject as if you had to pass a test on it.

Don’t avoid a subject because it seems beyond you — instead, read other half-understandable journals 
and textbooks to absorb more vocabulary, perspective, and context, then circle back.

Notice that concepts make more sense when you revisit a topic.

Notice which topics link in all directions, and provide keys to many others. Consider taking a class.

Continue until almost everything you encounter in Science and Nature makes sense as a contribution to 
a field you know something about.
```

<a name="#Semicolon-shaped-people"></a>
### Semicolon-shaped people
([overview](#overview))

From Venkat Rao's [Breaking Smart newsletter](https://us1.campaign-archive.com/?u=78cbbb7f2882629a5157fa593&id=f280d3e632).

Venkat first starts off with T-shaped people, which he'll later contrast with semicolons like himself:

```markdown
2/  Back in grad school 15 years ago, I read a book called Tomorrow's Professor
by Newport's spiritual predecessor, Richard Reis (and clearly learned nothing 
from it).

3/ In it, I encountered the idea of a T-shaped person. Reis' model was that you
ought to have a broad understanding of adjacent fields and a deep understanding
of your own to be a "good" academic.

4/ You've probably heard the phrase. T-shaped people are what  career counsellors
and HR people have in mind when they talk about "talent." Not just depth in craft,
but breadth in vision.

5/ You have your horizontal bar representing shallow generalist skills, and your 
vertical stem representing deep specialist skills.

6/ The horizontal bar represents your socialization as a knowledge worker. It 
overlaps with others' horizontal bars, enabling you to communicate across 
specialist disciplines.

7/ You can think of the T as spanning knowledge/skill based roles you could occupy. 
It's your intellectual home. T for turf. T for territory. T for textbook. 

8/ People who have a highly socialized, institutionalized, and territorial 
understanding of knowledge and work love the T-shaped-person idea. Academics 
particularly love it.

9/ In Freudian terms, T-shapes are how your superego thinks you ought to work. You
find your T, make friends and find collaborators along the bar, and dive deep along
the stem.

10/ All the while staying harmoniously connected to your intellectual community
oriented around a shared sense of "up" and directing your work "down" in the mines.  

11/ So long as you stay in your digging lane indicated by the stem, you'll also be
a good citizen of a knowledge economy, respecting others' expertise, and quietly 
proud of your own.

12/ T's stack and overlap nicely. They can be used to build stable big structures,
with good redundancy properties and natural paths of career development.

13/ In a knowledge map covered by overlapping T's, your T can grow bigger or sink
deeper. Your stem may be in your mentor's bar. Your current stem may become your 
future bar.

14/ T's induce natural authority relationships. If two people are trying to occupy
the same turf, the one for whom the position is shallower has authority (because 
they can go deeper).

15/ As in Tetris, T's are easy to work with. They are fine, upstanding citizens. 
They chair committees, organize interdisciplinary conferences, give TED talks, and
write solid books.

16/ They are nice people. To use an academic term of art, they are "collegial" types.
They can afford to be. They know where they belong in the world, and it's a good
place.

17/ Think T for Tenure. The American system of academic tenure is the perfect example
of an institution of, by, and for T-shaped people.

18/ Don't get me wrong. Unlike empty suit types, scenesters, and pretenders, T-
shaped people are people of substance. They are sincere and they do good, sometimes
great work.

19/ I have nothing but respect for T-shaped types. The backstop the reliability of 
civilizational knowledge, and are pillars of the community in an entirely positive 
sense of the term.

20/ You should WANT your Statistics 101 professors, doctors, lawyers, accountants,
bankers to be T-shaped. Trust in institutions is about the presence of T-shaped people
within them.

21/ So to summarize. T for textbook, T for tenure, T for turf, T for territory, T for
trust, T for talent. All good things. If you are T-shaped, or want to be T-shaped, 
more power to you. The world needs T's.
```

And now a taster for semicolons, illustrated with an example:

```markdown
22/ That said, I personally find T-shaped people boring, and am personally a lousy 
T. By the norms of T-shaped people, I am a could-have-been T who betrayed the values
and virtues of T-dom.

23/ Fortunately, I don't navigate by T-shaped norms. I navigate by the belief that 
there is an entire universe of deep knowing and doing that cannot be accessed by T-
shaped means.

24/ Here's an example. My buddy Kyle just released version 1.0 of a powerful React-
based open-source website building tool called Gatsby, after a year of heavy effort.

25/ It's definitely deep work, but throughout, Kyle was active on Twitter and social 
media. More importantly, the work didn't get done in a traditional institutional 
context. It was free-agent deep work.

26/ The work itself is not "T-shaped." It is a work of tech art in the rhizomatic 
frankenstack mess that is web technology.

27/ One of the interesting things about watching the project evolve was how it was 
driven by Kyle's contrarian view that the messy explosion of Javascript frameworks
and libraries is a good thing.

28/ That was his Thiel-ian "secret" -- the belief, not shared by most programmers, 
that the Javascript jungle is a good thing for the web, not a swamp to be drained. 
He vigorously defends this view online.

29/ The world of Javascript is decidedly unfriendly to systematic T-shaped approaches
to organizing, mapping, and navigating intellectual territories. T-shaped programmers
don't do well there.

30/ It is not surprising that the evolution of Javascript, unlike that of backend
computing technologies, has been strongly driven outside of academia. 

31/ It is also not surprising that you need a contrarian view of apparent messiness, 
and a willingness to ignore disciplinary maps and categories, to navigate and build 
on such rhizomatic knowledge.

32/ Javascript for example, began life as an underpowered browser language and evolved
into a powerful language for both backend and frontend systems in unexpected ways.

33/ To do deep work in the world of Javascript, you can't afford to be a T-shaped 
person. You have to be what I call a semi-colon shaped person.
```

Semicolons:

```markdown
34/ The dot of the semi-colon represents the anchor community for your deep work. 
In Gatsby's case, the world of Javascript sprawling messily across industry and 
open-source worlds.

35/ The curvy tail is the rhizomatic structure you explore to do something deep. 
It will sprawl untidily across a map built out of T's. It will offend sensibilities
and violate sacred beliefs about what goes where.

36/ The gap between the dot and the tail is what I call the Explorer's Chasm. To 
do deep work in a rhizomatic zone, you must have a "secret" separating you from 
your community.

37/ The chasm is CRUCIAL. It represents a sort of epistemic estrangement from the
nearest socialized zone of knowledge. This is necessary for work that is not just 
deep, but has truly ORIGINAL elements.

38/ The chasm also symbolizes the possibility that a work of deep knowing and doing
does not have a necessary, pre-determined and fixed connection to a locus of 
socialization that "owns" the work. 

39/ This means the knowledge could be subversive and challenge institutional 
authority. Squint a little and the tail of the semicolon might appear attached to a
different dot.

40/ And finally, a semicolon is both smaller and lower in a line of text than a T. 
I like to think this symbolizes its capacity for a) going deeper b) being efficient
at a smaller scale.
```

Contrasting T's and semicolons:

```markdown
41/ Paradoxically, because being semicolon-shaped means being less attached to a 
default social home locus, it requires you to be more agile, nomadic, alive, and 
active in relating to social context.

42/ Tenured professors with status in a discipline can tune out the world and do 
"deep work" peers recognize as "important" before it is done (with accompanying
ivory-tower/angels-on-pinhead risks).

43/ But a free agent, with no institutional safety net, no underwriting of 
exploratory expeditions by disciplinary consensus, and no research grants, cannot 
afford this luxury.

44/ To do deep work in semicolon mode, you must be plugged in, despite being 
fundamentally alone on your path, continuously renegotiating the meaning of what 
you're doing with the social context. 

45/ If T-shaped work is like climbing a sheer rock face with all kinds of safety
equipment and ropes, semicolon-shaped work is like free climbing. Riskier, but
much more rewarding if you pull it off.

46/ Almost all the people I personally find interesting, and learn a lot from,
tend to be semicolon shaped. This may be a personal preference, but I think
there's more to it.

47/ I suspect, for T-shaped people, exploration is at best an instrumental activity
that furthers their social development. For them, belongingness trumps curiosity.
That's why it can stay in its lane.

48/ But curiosity ungoverned by belongingness motives does not stay in fixed lanes,
seek permission to stray, apologize for wandering, or express contrition for 
"moving fast and breaking things."

49/ Such curiosity can be unapologetically rude, abrasive, damaging, combative, 
difficult, and intransigent when it conflicts with belongingness. It can appear to
be spoiling for a fight for no good reason.

50/ Belongingness-governed curiosity is great for stewarding existing 
institutionalized knowledge and building systematically and cautiously upon it. 
T-shaped curiosity is the bedrock of living traditionalism.

51/ You don't need formal institutions like universities for this kind of collective
knowledge environment. Even mature and developed cultural scenes have this 
characteristic. 

58/ Most T-shaped people and semicolon shaped ones probably find their own kind
interesting and admirable, and the opposite kind offensive and boring. Voice people
and exit people don't mix well.

59/ Most of the time, most people are (and should be), T-shaped. But in times of
institutional decay, renewal, and churn, you should be asking yourself: should I 
perhaps be semicolon-shaped?
```

Why are T's dominant? Venkat draws parallels to Pournelle's law; I first thought of atheism vs organized religion actually:

```markdown
53/ In a way, the dominance of T-shaped people and their modes of knowing and doing
are a consequence of Pournelle's Iron's Law of bureaucracy generalized to any social
system. 

54/ Pournelle's Law, restated for this context: *in any community, those who 
prioritize primal belongingness will eventually wrest control from those who
prioritize primal curiosity*.

55/ By contrast, semicolon shaped people and their modes of knowing and doing reflect 
a primacy of curiosity over belongingness. A willingness to sacrifice social harmony 
and relationships to the exploratory urge.

56/ This means, of course, that semicolon shaped people can be (but need not be, unless 
challenged or obstructed) socially disruptive, destabilizing, unreliable, disloyal, and
a "threat to society."

57/ But on the other hand, they also represent creative-destructive potential, and the
possibility of societies renewing themselves through the actions of those who don't 
feel strong belonging.
```

<a name="#names-matter"></a>
## Names matter
([overview](#overview))

In Michael Nielsen's essay [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html) there's a section recounting a famous story in physics I've always taken to heart, the one by Dick Feynman dismissing the value of knowing the names of things:

```markdown
One kid (a know-it-all) says to me, “See that bird? What kind of bird is that?” 

I said, “I haven't the slightest idea what kind of a bird it is.” 

He says, “It'a brown-throated thrush. Your father doesn't teach you anything!” 

But it was the opposite. He (Feynman's father) had already taught me: “See that bird?” 
he says. “It's a Spencer's warbler.” (I knew he didn't know the real name.) “Well, in 
Italian, it's a Chutto Lapittida. In Portuguese, it's a Bom da Peida… You can know the name 
of that bird in all the languages of the world, but when you're finished, you'll know absolutely
nothing whatever about the bird! You'll only know about humans in different places, and what they
call the bird. So let's look at the bird and see what it's *doing* — that's what counts.” (I
learned very early the difference between knowing the name of something and knowing something.)
```

Dick might've also been influenced by the following sentiment, expressed in *Computers From The Inside Out*:

```markdown
One of the miseries of life is that everybody names things a little bit wrong.
```

Roger Zelazny puts it far more poetically in his novel *Lords of Light* in one of the most powerful passages I've ever read:

```markdown
Sam sat with his eyes closed for several minutes, then said softly:

"I have many names, and none of them matter." He opened his eyes slightly then, but he did not 
move his head. He looked upon nothing in particular.

"Names are not important," he said. "To speak is to name names, but to speak is not important.

"A thing happens once that has never happened before. Seeing it, a man looks on reality. He cannot
tell others what he has seen. Others wish to know, however, so they question him saying, 'What is 
it like, this thing you have seen?'

"So he tries to tell them. Perhaps he has seen the very first fire in the world. He tells them, 
'It is red, like a poppy, but through it dance other colors. It has no form, like water, flowing 
everywhere. It is warm, like the sun of summer, only warmer. It exists for a time on a piece of wood,
and then the wood is gone, as though it were eaten, leaving behind that which is black and can be 
sifted like sand. When the wood is gone, it too is gone.'

"Therefore, the hearers must think reality is like a poppy, like water, like the sun, like that which
eats and excretes. They think it is like to anything that they are told it is like by the man who has 
known it. But they have not looked upon fire. They cannot really *know* it. They can only know *of* it.

"But fire comes again into the world, many times. More men look upon fire. After a time, fire is 
as common as grass and clouds and the air they breathe. They see that, while it is like a poppy, 
it is not a poppy, while it is like water, it is not water, while it is like the sun, it is not 
the sun, and while it is like that which eats and passes wastes, it is not that which eats and
passes wastes, but something different from each of these apart or all of these together. So they
look upon this new thing and they make a new word to call it. They call it 'fire.'

"If they come upon one who still has not seen it and they speak to him of fire, he does not know 
what they mean. So they, in turn, fall back upon telling him what fire is like. As they do, they 
know from their own experience that what they are telling him is not the truth, but only a part of it.
They know that this man will never know reality from their words, though all the words in the world 
are theirs to use. He must look upon the fire, smell of it, warm his hands by it, stare into its heart,
or remain forever ignorant.

"Therefore, 'fire' does not matter, 'earth' and 'air' and 'water' do not matter. 'I' do not matter.
No word matters.

"But man forgets reality and remembers words. The more words he remembers, the cleverer do his fellows
esteem him. He looks upon the great transformations of the world, but he does not see them as they were
seen when man looked upon reality for the first time. Their names come to his lips and he smiles as he 
tastes them, thinking he knows them in the naming. The thing that has never happened before is still 
happening. It is still a miracle. The great burning blossom squats, flowing, upon the limb of the world,
excreting the ash of the world, and being none of these things I have named and at the same time 
all of them, and *this* is reality—the Nameless.”
```

Nielsen pushes back against this attitude insofar as it goes too far:

```markdown
It's a good story. But it goes too far: names do matter. Maybe not as much as the know-it-all kid
thought, and they're not usually a deep kind of knowledge. But they're the foundation that allows 
you to build up a network of knowledge.

This trope that names don't matter was repeatedly drilled into me during my scientific training.
When I began using Anki, at first I felt somewhat silly putting questions about names for things 
into the system. But now I do it enthusiastically, knowing that it's an early step along the way 
to understanding.

Anki is useful for names of all kinds of things, but I find it particularly helpful for non-verbal
things. For instance, I put in questions about artworks, like: “What does the artist Emily Hare's 
painting Howl look like?”.

I put that question in for two reasons. The main reason is that I like to remember the experience of
the painting from time to time. And the other is to put a name to the painting.( Actually, a better
question for that is to be shown the painting and asked what its name is.) If I wanted to think more 
analytically about the painting – say, about the clever use of color gradients – I could add more
detailed questions. But I'm pretty happy just committing the experience of the image to memory.

Friends sometimes complain that many books are over-padded essays. Perhaps a benefit of such padding
is that it enforces an Anki-like spaced repetition, since readers take weeks to read the book. This
may be an inefficient way to memorize the main points, but is better than having no memory of the 
book at all.
```

Names are hard to get right too. Carlos Bueno, [The Mature Optimization Handbook](http://carlos.bueno.org/optimization/mature-optimization.pdf):

```markdown
Naming things has been half-jokingly called the second-hardest
problem in computer science. Anyone can name the things they
build anything they want, and they do. That’s the problem. The
computer doesn’t care about names. They’re for the benefit of
humans so there are no technical arguments to fall back on.
Excess jargon is the sawdust of new technology, and the mental
friction it imposes is scandalous. Whoever figures out how to
sweep it up does a service to mankind.

Take the word we’ve been using for intervals of real time,
“walltime”. Perhaps it’s more properly called “duration”. Time
spent on the CPU could be called “cpu_duration”; time spent
waiting for the database “db_duration” and so on. And why
not be explicit about the units, eg “duration_cpu_usec”? If you
have a strong preference either way, I humbly suggest that it’s a
matter of taste and not universal truth. Walltime sounds more
natural to me because that was the jargon I was first exposed
to. But who actually has clocks on their walls any more? The
term is as dated as “dialing” a phone number.

For that matter, take instructions. Now that we’ve decided
to round to the nearest thousand, is the name “instructions”
misleading? Is “kilo_instructions” too cumbersome to type? Is
“kinst” too obscure to remember?

This all might sound mincing and pedantic, but a) you have
to pick something and b) you’ll have to deal with your choices
for a long time. So will the people who come after you. You
can’t clean up the world but you can mind your own patch.

Even an ugly scheme, if it’s consistently ugly, is better than
having to remember that walltime here is duration over there
and response_time_usec somewhere else. Whatever ontology
you build, *write it down* somewhere it will be noticed. Explain
what the words mean, the units they describe, and be firm about
consistency
```

<a name="#cognitive-science"></a>
## Cognitive science
([overview](#overview))

From Cosma Shalizi's [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*:

```markdown
Cognition, whether human, animal or artificial, is a kind of information-processing, taking place, in our
case, in the brain. The information takes the form of representations (of sensory stimuli, of states of 
parts of the world, of facts, of relations, of possible states of parts of the world, of courses of action,
or what-not). The processing consists of the transformation of these representations according to definite,
though perhaps stochastic, rules. (So far, we have not excluded the connectionist heretics.) An immense amount
of information-processing takes place subconsciously, particularly that which turns raw irritation of the 
afferent nerves into useful perceptions of the world about us, and turns volitions into raw stimulations of
the efferent nerves. To recognize a dagger you see before you involves a lot of computational work; some people,
having been wounded in the parts of the brain which do the computations, cannot. At least at some level of 
abstraction, the representations and transformations are usefully, conveniently and/or accurately thought of as
structures of symbols and as algorithms, respectively. (This does rule out the connectionists.) The algorithms 
may be (or, if you like, instantiate) rules of inference, or rules for producing new representations from old 
ones more generally ("production systems"). 

One particularly well-studied kind of cognition, sometimes taken as the paradigm of all cognition, is
problem-solving, conceived of as turning a representation of the problem, step by step, into a representation of
a solution, or something close enough to a solution to satisfy the problem-solver. (Expertise in solving a kind 
of problem consists in knowing good algorithms to apply to it, being able to represent a problem in a way which
makes it easy to solve, and being able to recognize a solution when you have one.) In principle, all this takes 
place in the brain; in practice, we can fake a larger and more accurate memory than we possess by either using 
external symbols, or by taking advantage of regular and persistent parts of our environment.
```

<a name="#external-brain"></a>
## External brain 
([overview](#overview))

Alfred North Whitehead, *Symbolism: Its Meaning And Effect*:

```markdown
“Civilization advances by extending the number of
important operations which we can perform without thinking about them.”
```

Aids to memory have been opposed for millennia. Here's Socrates, in Plato's Phaedrus, circa 370 BCE, bemoaning the deleterious effects of the new technology of "writing":

```markdown
If men learn this, it will implant forgetfulness in their souls; they will cease to exercise
memory because they rely on that which is written, calling things to remembrance no longer 
from within themselves, but by means of external marks. What you have discovered is a recipe 
not for memory, but for reminder. And it is no true wisdom that you offer your disciples, but
only its semblance, for by telling them of many things without teaching them you will make 
them seem to know much, while for the most part they know nothing, and as men filled, not with
wisdom, but with the conceit of wisdom, they will be a burden to their fellows.
```

Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally) and the comments are great. It's from 2009; a decade hence, they're more applicable than ever. Here's some neat quotes.

```markdown
How many of the things you "know" do you have memorized?

Do you remember how to spell all of those words you let the spellcheck catch?  Do you remember
what fraction of a teaspoon of salt goes into that one recipe, or would you look at the list of
ingredients to be sure?  Do you remember what kinds of plastic they recycle in your neighborhood,
or do you delegate that task to a list attached with a magnet to the fridge?

If I asked you what day of the month it is today, would you know, or would you look at your
watch/computer clock/the posting date of this post?

Before I lost my Palm Pilot, I called it my "external brain".  It didn't really fit the description; 
with no Internet access, it mostly held my contact list, class schedule, and grocery list.  And a 
knockoff of Minesweeper.  Still, in a real enough sense, it remembered things for me.The vast arena 
of knowledge at our fingertips in the era of constant computing has, ironically, brought it farther
away.  It seems nearer: after all, now, if you are curious about Zanzibar, Wikipedia is a few 
keystrokes away.  Before the Internet, you'd probably have been looking at a trip to the library and
a while wrestling with the card catalog; and that would be if you lived in an affluent, literate society. 
If you didn't, good luck knowing Zanzibar exists in the first place!

But if you were an illiterate random peasant farmer in some historical venue, and you needed to know
the growing season of taro or barley or insert-your-favorite-staple-crop-here, Wikipedia would have
been superfluous: you would already know it.  It would be unlikely that you would find a song lyrics
website of any use, because all of the songs you'd care about would be ones you really knew, in the
sense of having heard them sung by real people who could clarify the words on request, as opposed to
the "I think I heard half of this on the radio at the dentist's office last month" sense.
```

Per Kaj Sotala, the distributed cognition paradigm of research is all about exploring the idea behind "externalizing" or "outsourcing" our brains to the environment. An excerpt from [this primer](http://www.isr.uci.edu/~jpd/classes/ics234bs03/13-HollanEtAl-TOCHI.pdf):

```markdown
In several environments we found subjects using space to simplify choice by creating arrangements 
that served as heuristic cues. For instance, we saw them covering things, such as garbage disposal 
units or hot handles, thereby hiding certain affordances or signaling a warning and so constraining
what would be seen as feasible. At other times they would highlight affordances by putting items 
needing immediate attention near to them, or creating piles that had to be dealt with. We saw them lay
down items for assembly in a way that was unambiguously encoding the order in which they were to be 
put together or handed off. That is, they were using space to encode ordering information and so were 
off-loading memory. These are just a few of the techniques we saw them use to make their dedecision
problems combinatorially less complex.

We also found subjects reorganizing their workspace to facilitate perception: to make it possible to 
notice properties or categories that were not noticed before, to make it easier to find relevant items,
to make it easier for the visual system to track items. One subject explained how his father taught him 
to place the various pieces of his dismantled bicycle, many of which were small, on a sheet of newspaper.
This made the small pieces easier to locate and less likely to be kicked about. In videos of cooking we 
found chefs distinguishing otherwise identical spoons by placing them beside key ingredients or on the 
lids of their respective saucepans, thereby using their positions to differentiate or mark them. We found
jigsaw puzzlers grouping similar pieces together, thereby exploiting the capacity of the visual system to
note finer differences between pieces when surrounded by similar pieces than when surrounded by different
pieces.

Finally, we found a host of ways that embodied agents enlist the world to perform computation for them. 
Familiar examples of such off-loading show up in analog computations. When the tallest spaghetti noodle
is singled out from its neighbors by striking the bundle on a table, a sort computation is performed by 
using the material and spatial properties of the world. But more prosaically we have found in laboratory 
studies of the computer game Tetris that players physically manipulate forms to save themselves 
computational effort [Kirsh 2001; Kirsh and Maglio 1995]. They modify the environment to cue recall, to 
speed up identification, and to generate mental images faster than they could if unaided. In short, they 
make changes to the world to save themselves costly and potentially error-prone computations.
```

Sotala takes the distributed cognition idea-seed and runs away with it:

```markdown
Information processing doesn't only happen inside brains and computers. The paradigm of distributed 
cognition studies human societies as information-processing systems, with individuals being parts of
the larger system. For instance, the operation of an airliner cockpit's crew has been studied from 
this perpective [1]. For a flight to proceed without trouble, the different crew members need to be 
aware of information relating to their areas of responsibility at any given moment. If the crew is 
experienced and well trained, they'll constantly stay up to date by e.g. simply listening to other 
crew members converse with flight control. As flight control informs the captain of a new flight 
altitude, the rest of the pilots begin to adjust the altitude even while the captain is still
finishing up the communication. The cockpit functions as a unified system, and relevant information 
is propagated to wherever needed. Several crew members hearing the same information also allows for
error correction. If the message is unclear and the captain can't make out flight control's words, 
he can ask the others for clarification. The co-pilot answers the captain's query: even though one 
part of the system has failed to absorb the information received from outside the system, the same 
information has been stored in another part, which may then attempt to re-send it where needed.

Several other fields have been studied in the same manner, ranging from a child's language learning 
[2] to creativity [3]. A child doesn't learn language by itself and in a vacuum, but via interaction
with adults and older children. Creativity, on the other hand, requires common, shared "idea resources"
which individuals may use to come up with their own inventions and then give them back for others to 
refine further. Another theory of innovation considers inventions to be responses to problems encountered
by the community. Things such as bad laws or ineffective ways of doing things show up in community, and 
are considered problems by its members. This leads the community - the system - into a need state,
mobilizing its members to seek solutions until they're found.

One central idea is that social communities are cognitive architectures the same way that individual
minds are [4]. The argument is as follows. Cognitive processes involve trajectories of information 
(transmission and transformation), so the patterns of these information trajectories, if stable,
reflect some underlying cognitive architecture. Since social organization - plus the structure added
by the context of activity - largely determines the way information flows through a group, social 
organization may itself be viewed as a form of cognitive architecture.

[1] Hutchins, E. & Klausen, T. (1995) Distributed Cognition in an Airline Cockpit.

[2] Spurrett, D. & Cowley, S.J. (2004) How to do things without words: infants, utterance-activity and
distributed cognition. Language Sciences, 6, 443-466.

[3] Miettinen, R. (2006) The Sources of Novelty: A Cultural and Systemic View of Distributed Creativity.
Creativity and Innovation Management. Vol. 15, no. 2.

[4] Hollan, J. & Hutchins, E. & Kirsh, D. (2000) Distributed Cognition: Toward a New Foundation for 
Human-Computer Interaction Research. ACM Transactions on Computer-Human Interaction. Vol 7, no. 2.
```

In other words, says Sotala, "probably nothing to be worried about. Just normal human use of the environment."

From Andy Clark's book *Supersizing the Mind*, a [comment by trent](https://nforum.ncatlab.org/discussion/1927/unpopularity-of-category-theory/) I found on the nForum showing how Feynman understood the notion of the external brain:

```markdown
When historian Charles Weiner found pages of Nobel Prize-winning physicist Richard Feynman’s
notes, he saw it as a “record” of Feynman’s work. Feynman himself, however, insisted that the
notes were not a record but the work itself. In Supersizing the Mind, Andy Clark argues that 
our thinking doesn’t happen only in our heads but that “certain forms of human cognizing include
inextricable tangles of feedback, feed-forward and feed-around loops: loops that promiscuously 
criss-cross the boundaries of brain, body and world.” The pen and paper of Feynman’s thought
are just such feedback loops, physical machinery that shape the flow of thought and enlarge the
boundaries of mind.
```

For context, this whole thread was about trying to figure out why category theory was unpopular. Trent adds:

```markdown
I see theory as helping one adopt elegant solutions like that, and, more generally I think that
the more physicists understand the role things seemingly outside of the mind such as notation
play in cognition, the more they will see the importance of work in mathematical physics which 
places physics in the most elegant possible notation. It’s not just theory addicts trying to 
justify their work when they say that it aids in problem solving, it’s how cognition works.
```

Cosma Shalizi is a great read for collective cognition. Here's the introduction he wrote to the [Collective Cognition:
Mathematical Foundations of Distributed Intelligence](http://csc.ucdavis.edu/~dynlearn/colcog/description.htm) workshop he co-organized awhile back, giving modern science and markets as examples:

```markdown
Many forms of individual cognition are enhanced by communication and collaboration with other 
intelligent agents. We propose to call this collective cognition, by analogy with the well known 
concept of collective action. People (and other intelligent agents) often "think better" in groups
and sometimes think in ways which would be simply impossible for isolated individuals. Perhaps the most
spectacular and important instance of collective cognition is modern science. An array of formal 
organizations and informal social institutions also can be considered means of collective cognition. For
instance, Hayek famously argued that competitive markets effectively calculate an adaptive allocation of
resources that could not be calculated by any individual market-participant. 

Hitherto the study of collective cognition has been qualitative, philosophical, even at times anecdotal.
Only recently, we believe, have the tools fallen into place to initiate a rigorous, quantitative science of
collective cognition. Moreover, it appears that soon there will be a real practical need for such a science.
```

A bit more on collective cognition:

```markdown
Collective cognition involves an interaction among three elements-the individual abilities of the agents,
their shared knowledge, and their communication structure. Cognitive collectives therefore resemble many
other complex systems which are collectives of goal-directed processes. Typically, the individual processes
know little of the detailed dynamics and the state of the overall system and, therefore, must use adaptive
techniques to achieve their goals. There are many naturally occurring examples, including human economies, 
human organizations, ecosystems, and even spin glasses. In addition, it has recently become clear that many
of the engineered systems of the future must be of this type, with massively distributed computational 
elements. There is optimism in the multi-agent system (MAS) field that widely applicable solutions to large,
distributed problems are close at hand. Some experts now believe that, in the information and
telecommunications networks of today, we have nascent examples of artificial cognitive collectives.
```

Collective cognition touches on a dazzling variety of fields: 

```markdown
Cognitive science
Situated agents
Emergent computation
Bounded rationality
Institutional economics
Economies of information
Evolutionary game theory
Cognitive ethology
Collective phenomena in physics
Neural computation and distributed representations
Distributed computation
Mechanism design
General equilibrium theory
Population biology
Robustness
Swarm intelligences
Reinforcement learning
Adaptive control
Cultural evolution
Cognitive sociology and the sociology of science
Telecommunications data routing
```

Some basic (i.e. foundational) questions of interest, at least for Cosma's workshop:

```markdown
Interaction between communication structure and cognitive performance.

How are knowledge dynamics and communication structure related? How are computational structure and 
communication structure related?

Why do some collectives not support much cognition and others support substandard or even pathological
forms?

When, and to what extent, can we attribute cognition, or at least computation, to the collective as 
such, rather than its individual members?

How much do we need to know about individual cognition to do adequate models of collective intelligence?

How can the capacity for collective cognition evolve?

Collective-action problems: Why should an agent contribute to the collective? Conversely, does the 
collective-cognition perspective shed any light on collective action in the conventional sense?

Incentive design: When we cannot directly control the goals individual agents, how can we still configure 
the system so that each agent has incentives to pursue a goal that is both readily achievable and good
for the overall collective?

How useful is fiat money in collectives whose agents are not human beings?

When and how should agents be induced to form teams?

When and how does agent heterogeneity enhance collective cognitive performance? Are "disagreement"
and "controversy" among agents always bad, or sometimes desirable for the collective?

How is robustness of behavior against external perturbation related to quality of behavior?
```

Cosma writes about collective cognition in a more fun-to-read way in his [review](http://bactra.org/reviews/cognition-in-the-wild/) of Edwin Hutchins' book *Cognition in the Wild*. It begins like so:

```markdown
Human beings coordinate their actions to do things which would be hard or impossible for them 
individually. This is not a particularly recondite fact, and the recognition of it is ancient; 
it is in the fifth book of Lucretius's De Rerum Natura, for instance. It was a commonplace of 
the Enlightenment, that most sociable age, and the philosophes were even, it seems, the first to 
realize that thinking, too, can be a collective activity, one conducted and amplified by social
groups --- which is not to say that societies have thoughts. ...

The nineteenth century, and to a lesser degree this one, have witnessed a dramatic expansion in 
the numbers of us engaged in administration, bureaucracy, management, oversight --- that is to say,
in formally-organized tasks of collective cognition and control. We did not invent bureaucracy, 
the mainstay of the ancient empires, but we're much, much better at it than they were. A random 
American town of 200,000 --- Piffleburg, WI, let us say --- will have police, a rescue squad, a fire
department, a hospital, universal schooling, several large factories, insurance offices, banks, a 
community college, a public library with several thousand volumes at least, a post office, public 
utilities, political parties, garbage collection, paved and usable roads everywhere, mercantile 
connections stretching across the country, and, with some luck, unions. These are corrupt, 
inefficient institutions which work poorly; every election, Piffleburg's citizens mutter something 
like "what do we pay taxes for anyway?" Yet to run any one of these institutions at the level of 
honesty, efficiency and efficacy which makes Piffleburg grumble would have demanded the full powers
and attention of even the ablest Roman propraetor or T'ang magistrate. That all of those institutions,
plus the ones not restricted to a single city, could be run at once, and while governed by a very 
ordinary slice of common humanity, would have seemed to such officials flatly impossible.
```

So why are we, as Cosma puts it, "so much better at collective endeavors than the ancients"? This is fairly easy to address:

```markdown
To a first approximation, the answer is: brute force and massive literacy. We teach nearly everyone to
read and write, and to do it, by historical standards, at a high level. This lets us staff large
bureaucracies (by some estimates, over 40% of the US workforce does data-handling), which lets us run an
industrial economy (the trains run on time), which makes us rich enough to afford to educate everyone 
and keep them in bureaucratic employment, with some surplus left over to expand the system. 

 This would do us no good if our ideas of administration were as shabby as those of our ancestors in the 
 dark ages, but they're not: we inherited those of the ancient empires, and have had quite a while to 
 improve upon them (and improvements are made easier and faster by the large number of administrators and
 the high standard of literacy). Among the improvements are many techniques (standardized procedures,
 standardized parts, standardized credentials and jobs, explicit qualifications for jobs and goods, files,
 standardized categories) and devices (forms, punch cards, punch card tabulators, adding machines, card
 catalogs, and, recently, computers) for making the administration of people and things easier.
```

Now while this is all splendid, it's "in the realm of technique"; when it comes to theory, nobody has any real idea how to explain what's going on:

```markdown
We don't really have a good theory about how collective action and cognition work, when and why they do, 
how they can be made to work better, why they fail, what they can and cannot accomplish, and so forth. 

Intellectually, these are large, tempting problems; technologically, they have obvious relevance to the 
design of parallel and distributed computers; economically, they could mean real money, not just billions;
and, in general, it'd be nice to know what it is we've gotten ourselves into.
```

Enter Edwin Hutchins, who conducted fieldwork studying navigation on a US navy ship based in San Diego with the problem of theoretically grounding collective cognition in mind. His fieldwork is interesting:

```markdown
Hutchins's field evidence consists of very detailed records, taken in the early 1980s, on the performance 
of the navigation crew of a helicopter carrier ship he calls the Palau, principally as they fix their 
location and plot their course near shore. The way it worked, in those pre-GPS days, was, roughly, this: 
three land-marks on shore, of known location on the navigation charts, would be selected by the main
person in charge, the "quartermaster of the watch." Then they'd "take bearings" on these, i.e. find the
orientation of the line from the landmark to the ship. These lines would be drawn on the chart. Now, it's 
an elementary result in Euclidean geometry that any two lines meet at a single point (unless they're parallel);
three lines form a triangle (unless they all meet at the same point). Somewhere within that triangle is the 
ship: this fixes the current position. The position of the ship at the next fix is estimated by "dead
reckoning," which is simply taking the current position and heading of the ship, and its planned speed, and 
extrapolating forward along the line of its heading. A single person can do this, if he's not too rushed.
Close to shore, the Navy gets worried, and demands fixes every few minutes, so the task gets broken down: 
naval flunkies take the bearings, a different flunky tells them when to take the bearings, and so on. There's 
a fairly rigid protocol for coordinating all these actions, and for communicating their results in a usable
form, and specialized instruments for making the job easier.

So, what does all this actually show? Well, that cognitive tasks can get spread over several people; that, 
in this instance, those who do tasks which require input from other people are generally superior to them in
rank; that the official job descriptions do not quite correspond to what people do; that people have a hard 
time believing things which are strange to them, and tend to ask those who report them questions along the 
lines of "Are you sure?"; that, if you don't know what something looks like, a verbal description can be very
unhelpful; that the right tools can make the job simpler; and that building computation into tools can make 
the job simpler for people, since it's easier to use a slide-rule than take a sine or a logarithm in your head;
that, if you can't talk about something, it's hard to make plans with someone else about it. There's more, but 
they're along the same lines.

These are not exactly earth-shaking results; in fact, they're about what common sense says to us. This doesn't 
make it useless to check them, since common sense is so often wrong; but even then, Hutchins has checked them
against the performance of one task (navigation; more particularly, location fixing), in one set of social 
groups (a couple of ships of the US Navy) --- ones where the social system is designed, and has received several
centuries of re-design from people whose common sense more or less agrees with the above. (One wonders if they
did things differently aboard the Potemkin.)
```

Unfortunately his conclusions are rubbish.

<a name="#procedural-vs-declarative-memory"></a>
## Procedural vs declarative memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), talking about procedural vs declarative memory in the context of using Anki flashcards:

```markdown
There's a big difference between remembering a fact and mastering a process. For instance, while 
you might remember a Unix command when cued by an Anki question, that doesn't mean you'll recognize
an opportunity to use the command in the context of the command line, and be comfortable typing it 
out. And it's still another thing to find novel, creative ways of combining the commands you know, 
in order to solve challenging problems.

Put another way: to really internalize a process, it's not enough just to review Anki cards. You need
to carry out the process, in context. And you need to solve real problems with it.
```

From Alicorn's LW post [The Great Brain is Located Externally](https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally):

```markdown
Propositional knowledge is being gradually supplanted by the procedural.  You need only know *how to find*
information, to be able to use it after a trivial delay.  This requires some snippet of propositional data
- to find a song lyric, you need a long enough string that you won't turn up 99% noise when you try to 
Google it! - but mostly, it's a skill, not a fact, that you need to act like you knew the fact.

It's not clear to me whether this means that we should be alarmed and seek to hone our factual memories...
or whether we should devote our attention to honing our Google-fu, as our minds gradually become
server-side operations.
```

<a name="#augmenting-long-term-memory"></a>
## Augmenting long-term memory
([overview](#overview))

From Michael Nielsen's [Augmenting Long-term Memory](http://augmentingcognition.com/ltm.html), which is an all-around great essay you should read in its entirety. One of its theses:

```markdown
Many people treat memory ambivalently or even disparagingly as a cognitive skill: for instance,
people often talk of “rote memory” as though it's inferior to more advanced kinds of understanding.
I'll argue against this point of view, and make a case that memory is central to problem solving 
and creativity.
```

To expand on his point:

```markdown
It's a mistake to underestimate the importance of memory. I used to believe such tropes about the 
low importance of memory. But I now believe memory is at the foundation of our cognition.

There are two main reasons for this change, one a personal experience, the other based on evidence
from cognitive science.

Let me begin with the personal experience.

Over the years, I've often helped people learn technical subjects such as quantum mechanics. Over 
time you come to see patterns in how people get stuck. One common pattern is that people think 
they're getting stuck on esoteric, complex issues. But when you dig down it turns out they're having 
a hard time with basic notation and terminology. It's difficult to understand quantum mechanics when
you're unclear about every third word or piece of notation! Every sentence is a struggle.

It's like they're trying to compose a beautiful sonnet in French, but only know 200 words of French.
They're frustrated, and think the trouble is the difficulty of finding a good theme, striking 
sentiments and images, and so on. But really the issue is that they have only 200 words with which to
compose.

My somewhat pious belief was that if people focused more on remembering the basics, and worried less 
about the “difficult” high-level issues, they'd find the high-level issues took care of themselves.

But while I held this as a strong conviction about other people, I never realized it also applied to
me. And I had no idea at all how strongly it applied to me. Using Anki to read papers in new fields 
disabused me of this illusion. I found it almost unsettling how much easier Anki made learning such 
subjects. I now believe memory of the basics is often the single largest barrier to understanding. If
you have a system such as Anki for overcoming that barrier, then you will find it much, much easier 
to read into new fields.

This experience of how much easier Anki made learning a new technical field greatly increased my 
visceral appreciation for the importance of memory.
```

<a name="#Yegge-on-memory"></a>
## Yegge on memory
([overview](#overview))

From one of his more memorable posts, [Done and gets things smart](https://steve-yegge.blogspot.com/2008/06/done-and-gets-things-smart.html):

```markdown
So we all think we're smart for different reasons. Mine was memorization. Smart, eh? 
In reality I was just a giant, uncomprehending parrot. I got my first big nasty surprise
when I was in the Navy Nuclear Power School program in Orlando, Florida, and I was 
setting historical records for the highest scores on their exams. The courses and exams 
had been carefully designed over some 30 years to maximize and then test "literal 
retention" of the material. They gave you all the material in outline form, and made you
write it in your notebook, and your test answers were graded on edit-distance from the 
original notes. (I'm not making this up or exaggerating in the slightest.) They had set 
up the ultimate parrot game, and I happily accepted. I memorized the entire notebooks 
word-for-word, and aced their tests.

They treated me like some sort of movie star — that is, until the Radar final lab exam in
electronics school, in which we had to troubleshoot an actual working (well, technically,
not-working) radar system. I failed spectacularly: I'd arguably set another historical 
record, because I had no idea what to do. I just stood there hemming and hawing and pooing
myself for three hours. I hadn't understood a single thing I'd memorized. Hey man, I was 
just playing their game! But I lost. I mean, I still made it through just fine, but I lost
the celebrity privileges in a big way.

Having a good memory is a serious impediment to understanding. It lets you cheat your way
through life. I've never learned to read sheet music to anywhere near the level I can play
(for both guitar and piano.) I have large-ish repertoires and, at least for guitar, good 
technique from lots of lessons, but since I could memorize the sheet music in one sitting,
I never learned how to read it faster than about a measure a minute. (It's not a 
photographic memory - I have to work a little to commit it to memory. But it was a lot 
less work than learning to read the music.) And as a result, my repertoire is only a 
thousandth what it could be if I knew how to read.

My memory (and, you know, overall laziness) has made me musically illiterate.

But when you combine the Dunning-Kruger effect (which affects me just as much as it does 
you) with having one or two things I've been good at in the past, it's all too easy to 
fall into the trap of thinking of myself as "smart", even if I know better now. All you 
have to do, to be "smart", is have a little competency at something, anything at all, just
enough to be dangerous, and then the Dunning-Kruger Effect makes you think you're God's 
gift to that field, discipline, or what have you.
```

<a name="#wisdom"></a>
## Wisdom
([overview](#overview))

Scott Alexander on wisdom in [Does age bring wisdom?](https://slatestarcodex.com/2017/11/07/does-age-bring-wisdom/) resonated pretty strongly with me:

```markdown
We’ve been talking recently about the high-level frames and heuristics that organize other 
concepts. They’re hard to transmit, and you have to rediscover them on your own, sometimes
with the help of lots of different explanations and viewpoints (or one very good one). 
They’re not obviously apparent when you’re missing them; if you’re not ready for them, they
just sound like platitudes and boring things you’ve already internalized.

Wisdom seems like the accumulation of those, or changes in higher-level heuristics you get
once you’ve had enough of those. I look back on myself now vs. ten years ago and notice
I’ve become more cynical, more mellow, and more prone to believing things are complicated. 
For example:

1. Less excitement about radical utopian plans to fix everything in society at once
2. Less belief that I’m special and can change the world
3. Less trust in any specific system, more resignation to the idea that anything useful
requires a grab bag of intuitions, heuristics, and almost-unteachable skills.
4. More willingness to assume that other people are competent in aggregate in certain
ways, eg that academic fields aren’t making incredibly stupid mistakes or pointlessly
circlejerking in ways I can easily detect.
5. More willingness to believe that power (as in “power structures” or “speak truth to
power”) matters and infects everything.
6. More belief in Chesterton’s Fence.
7. More concern that I’m wrong about everything, even the things I’m right about, on 
the grounds that I’m missing important other paradigms that think about things completely 
differently.
8. Less hope that everyone would just get along if they understood each other a little
better.
9. Less hope that anybody cares about truth (even though ten years ago I would have
admitted that nobody cares about truth).

All these seem like convincing insights. But most of them are in the direction of elite 
opinion. There’s an innocent explanation for this: intellectual elites are pretty wise, 
so as I grow wiser I converge to their position. But the non-innocent explanation is that 
I’m not getting wiser, I’m just getting *better socialized*. Maybe in medieval Europe, the 
older I grew, the more I would realize that the Pope was right about everything.
```

A particular example:

```markdown
I’m pretty embarassed by Parable On Obsolete Ideologies, which I wrote eight years ago. 
It’s not just that it’s badly written, or that it uses an ill-advised Nazi analogy. It’s
that it’s an impassioned plea to jettison everything about religion immediately, because
institutions don’t matter and only raw truth-seeking is important. If I imagine myself 
entering that debate today, I’d be more likely to take the opposite side. But when I read 
Parable, there’s…nothing really wrong with it. It’s a good argument for what it argues for.
I don’t have much to say against it. Ask me what changed my mind, and I’ll shrug, tell you
that I guess my priorities shifted. But I can’t help noticing that eight years ago, New 
Atheism was really popular, and now it’s really unpopular. Or that eight years ago I was in
a place where having Richard Dawkins style hyperrationalism was a useful brand, and now I’m
(for some reason) in a place where having James C. Scott style intellectual conservativism 
is a useful brand. A lot of the “wisdom” I’ve “gained” with age is the kind of wisdom that
helps me channel James C. Scott instead of Richard Dawkins; how sure am I that this is the
right path?
```

This is the "money quote", the whole reason I started this subheading:

```markdown
Sometimes I can almost feel this happening. First I believe something is true, and say so.
Then I realize it’s considered low-status and cringeworthy. Then I make a principled decision 
to avoid saying it – or say it only in a very careful way – in order to protect my reputation 
and ability to participate in society. Then when other people say it, I start looking down on
them for being bad at public relations. Then I start looking down on them just for being low-
status or cringeworthy. Finally the idea of “low-status” and “bad and wrong” have merged so 
fully in my mind that the idea seems terrible and ridiculous to me, and I only remember it’s
true if I force myself to explicitly consider the question. And even then, it’s in a 
condescending way, where I feel like the people who say it’s true deserve low status for not
being smart enough to remember not to say it. This is endemic, and I try to quash it when I 
notice it, but I don’t know how many times it’s slipped my notice all the way to the point 
where I can no longer remember the truth of the original statement.
```

Are old people really wiser? Why do they sound so crankily conservative? A model:

```markdown
And if I accept my intellectual changes as “gaining wisdom”, shouldn’t I also believe that 
old people are wiser than I am? And old people mostly seem to go around being really 
conservative and saying that everything was better in the old days and the youth are corrupt
and Facebook is going to be the death of us. I could model this as two different processes –
a real wisdom-related process that ends exactly where I am now, plus a false rose-colored-
glasses-related process that ends with your crotchety great-uncle talking about how things 
have been going downhill since the war – but that’s a lot of special pleading. I remember 
when I was twenty, I thought the only reason adults were less utopian than I was, was
because of their hidebound rose-colored self-serving biases. Pretty big coincidence that I 
was wrong then, but I’m right about everyone older than me *now.*
```

John "Erisology" Nerst responds:

```markdown
I think there could be selection effect. Not all people get wiser as they age and many hit 
a ceiling at some time. Maybe those are the ones most likely to make their opinions heard 
(I mean, it’s hardly the case that the wisest are the loudest among the younger population
either). And the really wise ones stay silent because their wisdom has become impossible to
communicate?

It reminds me of the quote from Julian Barnes’ Staring at the Sun:

*Everything you wanted to say required a context. If you gave the full context, people
thought you a rambling old fool. If you didn’t give the context, people thought you a
laconic old fool.”*
```

Worst-case scenario -- wisdom as "NMDA reception function change with age":

```markdown
There’s one more possibility that bothers me even worse than the socialization or
traumatization theory. I’m going to use science-y sounding terms just as an example, but I 
don’t actually think it’s this in particular – we know that the genes for liberal-conservative 
differences are mostly NMDA receptors in the brain. And we know that NMDA receptor function 
changes with aging. It would be pretty awkward if everything we thought was “gaining wisdom
with age” was just “brain receptors consistently functioning differently with age”. If we
were to find that were true – and furthermore, that the young version was intact and the older 
version was just the result of some kind of decay or oxidation or something – could I trust 
those results? Intuitively, going back to earlier habits of mind would feel inherently 
regressive, like going back to drawing on the wall with crayons. But I don’t have any *proof.*
```

This is pithily captured in the quote (attribution unknown, phrasing by "Keith"):

```markdown
He who isn’t radical as a youth has no heart. 
And he who isn’t conservative as an adult has no brain.
```

What this looks like in science -- the "grand old academics" phenomenon:

```markdown
I’ve noticed a vaguely related trend in science:

you get a number of grand old academics, the kind of people who continue to hang out at the
institution long after they’re officially retired who are an absolute goldmine for various 
minutiae of their subject.

They’ve tried many approaches over the decades and can warn you about dead ends….

but they also often have an overabundance of cynicism.

Often they remember that approach X didn’t work, they may not remember the exact details as 
to why. their memory of the event gets pared down to “that’s a dead end”… and then at some 
point a new generation of grad students come along and eventually someone ignores the advice 
that X is a dead end and it turns out that in the 30 years that have passed the things that 
made X a dead end no longer apply. The sequencing methods can now read through long-repeats 
or the chemistry used for some step is improved or some background piece of knowledge has
been added to the field that now allows people to power through the former roadblock.

Is that wisdom? Knowing lots of dead ends can be useful and can save resources…but it can also
be maladaptive as the world changes around you.
```

Commenter Deej's response:

```markdown
we need to distinguish between individual people changing their views as they get older, 
and the centre grounds shifting as younger people are more liberal than their predecessors.
My feeling is that for economic issues people’s individual views probably do shift 
rightwards as they get older, but for social issues it’s seems likely that it’s the centre
ground that’s moving. Although for today’s more exterme identity politics left youths that
might change.

Third. I think it’s worth distinguising between types of people and how their views might 
change. People who are properly interested in politics, for example, are – I would exepct 
– much more likely top have big changes in their views, than those that aren’t. See ex-
trotskists now in the Tory party or at least Blairite in the UK. I expect that the people
interested in politics changes are likely to be relatively more driven by learning from 
experience and reflective thought than people just slowly change their views over time from,
for example, a bit left to a bit right of centre. Or left to a bit left less left, right to
a bit less right etc.
```

Somewhat relevant are these quotes from Robin Hanson's *Age of Em*:

```markdown
Controlling for birth cohort, individual productivity does not peak until at least age 60,
and may never peak (Cardoso et al. 2011; Göbel and Zwick 2012). […] Also, any falling
productivity after age 60 for humans today may be primarily caused by declining physical
abilities, not declining mental abilities

Today, our abilities at different kinds of tasks peak at different ages. For example, raw 
cognitive processing peaks in late teens, learning and remembering names in early 20s, 
short-term memory about age 30, face recognition in early 30s, social understanding about
age 50, and word knowledge above age 65 (Hartshorne and Germine 2015).
```

<a name="#The-predictive-uselessness-of-folk-wisdom"></a>
### The predictive uselessness of folk wisdom
([overview](#overview))

Keith E. Stanovich, [How to Think Straight About Psychology](http://www.pearsonhighered.com/assets/hip/us/hip_us_pearsonhighered/samplechapter/0205914128.pdf):

```markdown
Often a person uses some folk proverb to explain a behavioral event even though, on an earlier occasion, this 
same person used a directly contradictory folk proverb to explain the same type of event. For example, most of 
us have heard or said, “look before you leap.” Now there’s a useful, straightforward bit of behavioral advice—
except that I vaguely remember admonishing on occasion, “he who hesitates is lost.” And “absence makes the heart 
grow fonder” is a pretty clear prediction of an emotional reaction to environmental events. But then what about 
“out of sight, out of mind”? And if “haste makes waste,” why do we sometimes hear that “time waits for no man”? 
How could the saying “two heads are better than one” not be true? Except that “too many cooks spoil the broth.” 
If I think “it’s better to be safe than sorry,” why do I also believe “nothing ventured, nothing gained”? And if 
“opposites attract,” why do “birds of a feather flock together”? I have counseled many students to “never to put 
off until tomorrow what you can do today.” But I hope my last advisee has never heard me say this, because I just 
told him, “cross that bridge when you come to it.”

The enormous appeal of clichés like these is that, taken together as implicit “explanations” of behavior, they 
cannot be refuted. No matter what happens, one of these explanations will be cited to cover it. No wonder we all 
think we are such excellent judges of human behavior and personality. We have an explanation for anything and 
everything that happens. Folk wisdom is cowardly in the sense that it takes no risk that it might be refuted.

That folk wisdom is “after the fact” wisdom, and that it actually is useless in a truly predictive sense, is why 
sociologist Duncan Watts titled one of his books: Everything Is Obvious—Once You Know the Answer (2011). Watts 
discusses a classic paper by Lazarsfeld (1949) in which, over 60 years ago, he was dealing with the common 
criticism that “social science doesn’t tell us anything that we don’t already know.” Lazarsfeld listed a series 
of findings from a massive survey of 600,000 soldiers who had served during World War II; for example, that men 
from rural backgrounds were in better spirits during their time of service than soldiers from city backgrounds. 
People tend to find all of the survey results to be pretty obvious. In this example, for instance, people tend 
to think it obvious that rural men would have been used to harsher physical conditions and thus would have 
adapted better to the conditions of military life. It is likewise with all of the other findings—people find them 
pretty obvious. Lazarsfeld then reveals his punchline: All of the findings were the opposite of what was originally 
stated. For example, it was actually the case that men from city backgrounds were in better spirits during their 
time of service than soldiers from rural backgrounds. The last part of the learning exercise is for people to 
realize how easily they would have explained just the opposite finding. In the case of the actual outcome, people 
tend to explain it (when told of it first) by saying that they expected it because city men are used to working 
in crowded conditions and under hierarchical authority. They never realize how easily they would have concocted an 
explanation for exactly the opposite finding.
```
