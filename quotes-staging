[Culture, community, and walled gardens](#Culture-community-and-walled-gardens)
	1. [Anglerfish and beacons, or why some blogs have higher comment quality](#Anglerfish-and-beacons), on geek subculture dilution by Chapman-sociopaths
[Thinking and explaining](#Thinking-and-explaining)
	
(UNDER MATH - OPINIONS)
<a name="#Thinking-and-explaining"></a>
### Thinking and explaining
([overview](#overview))

I always return to the late Bill Thurston's great MO question [Thinking and explaining](https://mathoverflow.net/questions/38639/thinking-and-explaining) every few months or years for the wealth of insightful quotes within. Here I place them within the wider context of my walled garden of ideas/document.

Bill's question:

```markdown
How big a gap is there between how you think about mathematics and what you say to others?
Do you say what you're thinking? Please give either personal examples of how your thoughts
and words differ, or describe how they are connected for you.


I've been fascinated by the phenomenon the question addresses for a long time. We have 
complex minds evolved over many millions of years, with many modules always at work. A 
lot we don't habitually verbalize, and some of it is very challenging to verbalize or to
communicate in any medium. Whether for this or other reasons, I'm under the impression 
that mathematicians often have unspoken thought processes guiding their work which may 
be difficult to explain, or they feel too inhibited to try. One prototypical situation 
is this: there's a mathematical object that's obviously (to you) invariant under a 
certain transformation. For instant, a linear map might conserve volume for an 'obvious'
reason. But you don't have good language to explain your reason---so instead of 
explaining, or perhaps after trying to explain and failing, you fall back on computation. 
You turn the crank and without undue effort, demonstrate that the object is indeed
invariant.

Here's a specific example. Once I mentioned this phenomenon to Andy Gleason; he immediately
responded that when he taught algebra courses, if he was discussing cyclic subgroups of a
group, he had a mental image of group elements breaking into a formation organized into 
circular groups. He said that 'we' never would say anything like that to the students. His
words made a vivid picture in my head, because it fit with how I thought about groups. I 
was reminded of my long struggle as a student, trying to attach meaning to 'group', rather
than just a collection of symbols, words, definitions, theorems and proofs that I read in 
a textbook.
```

The following are MO's responses I liked. 

Here's one by Bill that comes to mind every once in a while:

```markdown
When listening to a lecture, I can't possibly attend to every word: so many words blank 
out my thoughts. My attention repeatedly dives inward to my own thoughts and my own mental
models, asking 'what are they really saying?' or 'where is this going?'. I try to shortcut
through my own understanding, then emerge to see if I'm still with the lecture.
```

Terry Tao is -- well, he's as close as you'll get to a universal problem-solver this generation:

```markdown
I find there is a world of difference between explaining things to a colleague, and explaining
things to a close collaborator. With the latter, one really can communicate at the intuitive
level, because one already has a reasonable idea of what the other person's mental model of the
problem is. In some ways, I find that throwing out things to a collaborator is closer to the 
mathematical thought process than just thinking about maths on one's own, if that makes any 
sense.

One specific mental image that I can communicate easily with collaborators, but not always to 
more general audiences, is to think of quantifiers in game theoretic terms. Do we need to show
that for every epsilon there exists a delta? Then imagine that you have a bag of deltas in your
hand, but you can wait until your opponent (or some malicious force of nature) produces an 
epsilon to bother you, at which point you can reach into your bag and find the right delta to 
deal with the problem. Somehow, anthropomorphising the "enemy" (as well as one's "allies") can 
focus one's thoughts quite well. This intuition also combines well with probabilistic methods, 
in which case in addition to you and the adversary, there is also a Random player who spits out
mathematical quantities in a way that is neither maximally helpful nor maximally adverse to your 
cause, but just some randomly chosen quantity in between. The trick is then to harness this
randomness to let you evade and confuse your adversary.

Is there a quantity in one's PDE or dynamical system that one can bound, but not otherwise 
estimate very well? Then imagine that it is controlled by an adversary or by Murphy's law, and 
will always push things in the most unfavorable direction for whatever you are trying to
accomplish. Sometimes this will make that term "win" the game, in which case one either gives up
(or starts hunting for negative results), or looks for additional ways to "tame" or "constrain" 
that troublesome term, for instance by exploiting some conservation law structure of the PDE.

For evolutionary PDEs in particular, I find there is a rich zoo of colourful physical analogies
that one can use to get a grip on a problem. I've used the metaphor of an egg yolk frying in a
pool of oil, or a jetski riding ocean waves, to understand the behaviour of a fine-scaled or 
high-frequency component of a wave when under the influence of a lower frequency field, and how
it exchanges mass, energy, or momentum with its environment. In one extreme case, I ended up 
rolling around on the floor with my eyes closed in order to understand the effect of a gauge 
transformation that was based on this type of interaction between different frequencies. 
(Incidentally, that particular gauge transformation won me a Bocher prize, once I understood how 
it worked.) I guess this last example is one that I would have difficulty communicating to even 
my closest collaborators. Needless to say, none of these analogies show up in my published papers,
although I did try to convey some of them in my PDE book eventually.

ADDED LATER: I think one reason why one cannot communicate most of one's internal mathematical
thoughts is that one's internal mathematical model is very much a function of one's mathematical
upbringing. For instance, my background is in harmonic analysis, and so I try to visualise as
much as possible in terms of things like interactions between frequencies, or contests between 
different quantitative bounds. This is probably quite a different perspective from someone 
brought up from, say, an algebraic, geometric, or logical background. I can appreciate these other
perspectives, but still tend to revert to the ones I am most personally comfortable with when I am
thinking about these things on my own.

ADDED (MUCH) LATER: Another mode of thought that I and many others use routinely, but which I 
realised only recently was not as ubiquitious as I believed, is to use an "economic" mindset to 
prove inequalities such as X≤Y or X≤CY for various positive quantities X,Y, interpreting them in the
form "If I can afford Y, can I therefore afford X?" or "If I can afford lots of Y, can I therefore
afford X?" respectively. This frame of reference starts one thinking about what types of quantities
are "cheap" and what are "expensive", and whether the use of various standard inequalities 
constitutes a "good deal" or not. It also helps one understand the role of weights, which make things
more expensive when the weight is large, and cheaper when the weight is small.

ADDED (MUCH, MUCH) LATER: One visualisation technique that I have found very helpful is to
incorporate the ambient symmetries of the problem (a la Klein) as little "wobbles" to the objects 
being visualised. This is most familiarly done in topology ("rubber sheet mathematics"), where every
object considered is a bit "rubbery" and thus deforming all the time by infinitesimal homeomorphisms.
But geometric objects in a scale-invariant problem could be thought of as being viewed through a 
camera with a slightly wobbly zoom lens, so that one's mental image of these objects is always 
varying a little in size. Similarly, if one is in a translation-invariant setting, one's mental
camera should be sliding back and forth just a little to remind you of this, if one is working in a 
Euclidean space then the camera might be jiggling through all the rigid motions, and so forth. A more
advanced example: if the problem is invariant under tensor products, as per the tensor product trick,
then one's low dimensional objects should have a tiny bit of shadowing (or perhaps look like one of 
these 3D images when one doesn't have the polarised glasses, with the slightly separated red and blue
components) that suggest that they are projections of a higher dimensional Cartesian product.

One reason why one wants to do this is that it helps suggest useful normalisations. If one is
viewing a situation with a wobbly zoom lens and there is some length that appears all over one's
analysis, one is reminded that one can spend the scale invariance of the problem to zoom up or 
down as appropriate to normalise this scale to equal 1. Similarly for other ambient symmetries.

This sort of wobbling of symmetries is also available in less geometric settings. When viewing,
say, a graph on n vertices, perhaps the labels 1,…,n on the vertices have a tendency to swap with 
each other every so often, to emphasise the symmetry of relabeling in graph theory. Similarly, 
when dealing with a set {a,b,c,d,…}, perhaps the positions of the elements a,b,c,d in one's
enumeration of the set are volatile and swap places every so often. In analysis, one often only 
cares about the order of magnitude of some very large or very small quantity X, rather than its 
exact value; so one should view this quantity as being a bit squishy in size, growing or shrinking
by a factor of two or so every time one looks at the problem. If there is some probability theory
in one's problem, and some of your objects are random variables rather than deterministic 
variables, then you can imagine that every so often the "game resets", with the random variables
jumping around to different values in their range (and any quantities depending on these variables 
changing accordingly), whereas the deterministic variables stay fixed. Similarly if one has generic
points in a variety, or nonstandard objects in a space (with the point being that if something bad
happens if, say, your generic point is trapped in a subvariety, you can "reset the game" in which 
the generic point is now outside the subvariety; similarly one can "reset" an unbounded nonstandard 
number to be larger than any given standard number, etc.).
```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown

```

Cam MacLeman:

```markdown
One of my favorites from undergrad was describing a linear transformation as a commander-
in-chief, who told the generals (a basis) where to go, who in turn tells all the soldiers
(the rest of the vectors) where to go. The chain of command in action in a linear algebra
class.
```


<a name="#Culture-community-and-walled-gardens"></a>
## Culture, community, and walled gardens
([overview](#overview))

<a name="#Anglerfish-and-beacons"></a>
### Anglerfish and beacons
([overview](#overview))

Ben Hoffman talks about avoiding anglerfish, or Chapman's sociopaths, in a great essay I often come back to called [On the construction of beacons](http://benjaminrosshoffman.com/construction-beacons/). I tried not to just copy-paste everything, but Ben doesn't waste words, he's a precision writer, so it's hard to leave things out because "everything is in its place" so to speak.

First of all, why "anglerfish"?

```markdown
The anglerfish lives in waters too far beneath the surface of the sea for sunlight to reach.
It dangles a luminescent lure in front of itself. This resembles a fishing angle, whence 
comes its name. This lure attracts animals of the deep sea, which approach the anglerfish, 
and are consequently eaten by it.

Why - in the deep sea where no sunlight can reach - would evolution favor animals that are
attracted to light?

The secondary uses of such a strategy are clear enough. Once some deep-sea-dwellers emit
light, larger animals that predate on them might do better if attracted to light sources. 
But that presupposes the existence of other animals that already emit light, for other
reasons.

What are the primary uses of light? In a region where no other creatures emit light, here 
are some reasons why would might begin to do so:

1. To illuminate potential prey.
2. As a ward, to warn potential competitors that one is prepared to defend territory.
3. To attract complementary animals, either as symbiotes, or as mates.

In all these cases, the purpose of the light is to *reveal information*. In all but the
first case, it is to *share information with others, in order to enable cooperation*. 
Perhaps the purest version of this is the mating display. We can see this in the firefly, 
which uses its distinctive patterns of luminescent flashes to find mates.

The firefly has some information. It activates a beacon, in order to find someone with 
*complementary information*, in order to engage in *productive exchange*. Likewise for 
deep-sea fish who mate or find symbiotes by means of a light display.

The predation strategy of the anglerfish, properly generalized is a strategy that predates
on all information-seeking behavior, whether competitive or cooperative. The anglerfish
does not need to know that the animal that just swam in front of it is evaluating its 
mating display and finds it wanting, or is looking for a very different creature as a 
symbiote. So long as there are animals seeking illumination, the anglerfish only cares that
some calories and raw materials have been brought within reach of a single burst of 
swimming and the clamping shut of its great maw.

Typically, a predator has to be more sophisticated than the creatures on which it preys.
But the anglerfish follows a simple, information-poor strategy, that preys on sophisticated,
information-rich ones. It doesn’t have to be a particularly skilled mimic - it simply preys
on the fact that creatures seeking information will move towards beacons.
```

How does this analogize to subculture dilution?

```markdown
In David Chapman’s geeks, MOPs, and sociopaths, “geeks” are the originators of subcultures. 
They are persons of refined taste and discernment. They found subcultures by discovering or
creating something they believe to be of intrinsic value. The originators of this information
share it with others, and the first to respond enthusiastically will be other geeks, who can 
tell that the content of the message is valuable.

Eventually, enough geeks congregate together, and the thing they are creating together becomes
valuable enough, that people without the power to independently discern the source of value 
can tell that value is being created. These Chapman calls “Members Of the Public”, or “MOP”s. 
Geeks map roughly onto Aellagirl's possums, MOPs onto otters.

In the right ratios, MOPs and Geeks are symbiotes. The MOPs enjoy the benefits of the thing the
geeks created, and are generally happy to share their social capital, including money, with the
geeks.

But from another perspective MOPs are an exploitable resource, which the geeks have gathered in
one place but are neither efficiently exploiting, nor effectively defending. This attracts
people following a strategy of predating on such clusters of MOPs. These predators, whom Chapman
names “sociopaths,” do not care about the idiosyncratic value the geeks are busy creating. What 
they do care about, is the generic resources - attention, money, volunteer hours, social proof -
that the MOPs provide.

To summarize the above: Geeks build beacons. Initially these beacons are not very bright, but
they are sending out high-information signals which attract other geeks looking for that 
information. Eventually, enough geeks are contributing to the beacons that they become bright
enough to attract MOPs.

Chapman’s sociopaths can’t just waltz in and propose that everyone give them things for nothing.
After all, everyone in their feeding ground was attracted to it by something about it, something
that distinguishes it from other places in the culture. They need to look like a part of the 
scene. So they start by imitating, or proposing refinements to, the beacons the geeks have erected.

The geeks are only putting up a very particular kind of beacon. There are a lot of constraints 
on exactly what sort of signal they are willing to send. This is the same as saying that their 
beacons have a lot of information content. From the geeks’ perspective, the exchange of this 
information is the whole point of setting up beacons, and the presence of friendly MOPs is just a
happy side effect.

But from the sociopaths’ perspective, these information-bearing constraints are mere shibboleths.
Chapman’s sociopaths will follow whatever rules they have to in order to pass as contributors to
the subculture, but they won’t put independent effort into understanding why these rules are the 
ones they have to follow. Instead, their contribution is to **iteratively improve the beacons’ 
ability to attract prey.**

As sociopaths test out variations in their beacons, they will learn which variants are best at 
attracting people, by means of trial and error. Three things about this will reduce the relative 
proportion of geeks in the subculture, and therefore the geeks’ influence.

1. First, since MOPs are less sensitive to fine variations in signal than geeks are, random
mutations in beacon design are more likely to attract more MOPs than more geeks.

2. Second, as the overall process becomes better at attracting MOPs, more sociopaths will notice
that it is a promising feeding ground.

3. Finally, many changes that are neutral or beneficial for attracting MOPs, will, from the geeks’ 
perspective, seem like the introduction of errors. This will make the signal less attractive to
geeks who have not already invested in the subculture.

What does this process look like from the geeks’ perspective?

At first - people are coming into the geeks’ subculture, and trying to contribute to it. These
newcomers are putting a lot of energy into creating new content, but from time to time 
introduce perplexing errors. But, they are getting a lot of people interested in this wonderful
information you’ve created, so the geeks are not inclined to complain. The MOPs basically trust
the geeks’ implied endorsement, and accept the new contributors on the same footing as the old 
ones.

But now there are two forces at play affecting the content of the signals being sent. One is a
force correcting errors - the geeks’ desire to preserve, transmit, and develop the original 
information-content of the signal. The other force introduces errors: the sociopaths’ desire to
attract more MOPs. When the second force becomes stronger than the first, the sociopaths are now 
the dominant faction, and able to coordinate to suppress geek attempts to correct errors that 
make the message more popular.

At first, the MOPs’ acceptance of the sociopaths depended in part on the geeks’ tacit endorsement.
But once a sufficiently powerful faction of sociopaths has been given social proof, they can wield
the force of disendorsement against the geeks. The only meaningful constraint is that MOPs don’t
like conflict, so the sociopaths will want to avoid escalating to a point where the conflict
becomes overt.

From the sociopaths’ perspective, the geeks were inexplicably donating their time and energy to 
discovering a new signal to broadcast, that would attract a pool of MOPs to feed on. But the geeks
were - again incomprehensibly - neither exploiting nor defending that resource. The sociopath 
strategy invests in general understanding of social dynamics, but does not need to understand the 
specific content of what the geeks are trying to do. The sociopath need only know that some
attention, money, volunteer hours, and social proof have been brought within reach of a competent
marketing and sales effort.

From the sociopaths' perspective, they are not introducing errors - they are correcting them.

The paradigmatic predator is sufficiently smarter than its targets to anticipate and manipulate
their behavior. But Chapman’s sociopaths follow a simple, information-poor strategy, that preys on
sophisticated, information-rich ones. This strategy doesn’t have to understand the signal as well 
as the geeks do - the geeks will help it pass their tests (because geeks are usually guess culture,
and guess culture screens for trying to cooperate). It simply iterates empirically towards shining 
the most attractive beacon it can, of a kind that has already been selected to attract its prey.

The predation strategy of Chapman’s sociopath is a strategy that predates on all information-
seeking behavior, whether competitive or cooperative.
```

Note that sociopaths aren't "bad":

```markdown
Sociopaths are not necessarily universally bad or mean people. They just *don't care about your 
project*. This is fine. You don't care about most people's projects. Likewise, most people don't 
care about yours. The problem is when you let those people run your project.

As far as Chapman's sociopaths know, they are just doing what one does to beacons - trying to make
them more pleasing to more people. They are cooperating with the geeks as sincerely as they know 
how - as sincerely as the believe to be possible. In many cases they simply don’t understand that
the original signal had value. There's little point in being indignant about this.
```

It's the "geeks" who're most responsible for maintaining that subculture, and for the creation of community standards:

```markdown
The people who need to do something about the corruption of a message are the people who *care
the most about that message*: the geeks. In subcultures following this lifecycle, geeks have 
committed a key sin: trying to get something for nothing, by pretending to be more popular than
we are.

People playing sociopath strategies gain a foothold in subcultures, because they *bring in more
resources*, get more people involved, get attention from respectable people, raise money - since 
they are paying attention to how attractive their beacons are, not whether they are correct 
(from a geek perspective).

The obvious strategy to counter this is to speak up early and often when errors are being 
introduced. It is not a sin to be error-tolerant, in the sense of not immediately expelling people 
for making errors. But it is *always* a sin, in an otherwise-cooperative community, to *suppress the
calling-out of errors*, in order to avoid making a scene, scaring off the MOPs, harming morale and
momentum. If you are a geek in that sort of subculture, the MOPs are relying on your implied 
endorsement of the other content-creators. If you remain silent in the face of error, then you are
*betraying this trust*. There is no additional error-correction system that will save you - you were
supposed to be the error-correction system.

If you and your collaborators diligently follow this practice, then this will enable the creation 
of common knowledge when someone is reliably introducing errors, and either failing to correct them
or making the minimum possible correction. You will have shared knowledge of track records - who is
introducing information, and who is destroying it with noise. It is only with this knowledge that 
you can begin to have actual community standards.
```
